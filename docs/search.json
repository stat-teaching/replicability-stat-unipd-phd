[
  {
    "objectID": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#multiverse-analysis-steegen2016-lz",
    "href": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#multiverse-analysis-steegen2016-lz",
    "title": "Multiverse",
    "section": "Multiverse analysis Steegen et al. (2016)\n",
    "text": "Multiverse analysis Steegen et al. (2016)\n\n\nEven a simple data analysis is characterized by several (often arbitrary) choices\nThe impact of these choices is not always clear, known or easy to predict\nResearchers (with or without awareness) usually report a single set of choices"
  },
  {
    "objectID": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#some-examples",
    "href": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#some-examples",
    "title": "Multiverse",
    "section": "Some examples",
    "text": "Some examples\n\nusing a predictor as continous or choosing some thresholds and transforming to categorical\nincluding or excluding an observation (e.g., outlier)\nincluding or excluding a covariate (e.g., controlling for age or not)\nusing a linear model or an ordinal regression for discrete ordered data\n‚Ä¶"
  },
  {
    "objectID": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#the-garden-of-forking-paths",
    "href": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#the-garden-of-forking-paths",
    "title": "Multiverse",
    "section": "The garden of forking paths",
    "text": "The garden of forking paths\nA tree of possibilities‚Ä¶"
  },
  {
    "objectID": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#the-garden-of-forking-paths-1",
    "href": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#the-garden-of-forking-paths-1",
    "title": "Multiverse",
    "section": "The garden of forking paths",
    "text": "The garden of forking paths\n‚Ä¶ Where only some of them produce a certain result."
  },
  {
    "objectID": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#only-plausible-vs-all-scenarios",
    "href": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#only-plausible-vs-all-scenarios",
    "title": "Multiverse",
    "section": "Only plausible vs all scenarios",
    "text": "Only plausible vs all scenarios\nNot all scenarios are equally plausible or reasonable. Thus the multiverse is not the entire set but a non-random sample of plausibile choices.\n\nCodex &lt;- seq(0, 1, 0.0001)\ny &lt;- dexp(x, 2)\ny &lt;- (y-min(y))/(max(y)-min(y))\n\nplot(x*100, y, type = \"l\",\n     xlab = \"All possible scenarios\",\n     ylab = \"Plausibility\",\n     lwd = 2)"
  },
  {
    "objectID": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#meta-analysis-many-choices",
    "href": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#meta-analysis-many-choices",
    "title": "Multiverse",
    "section": "Meta-analysis many choices",
    "text": "Meta-analysis many choices\nDespite useful and very powerful, meta-analysis is characterized by several (arbitrary) choices. For example:\n\nShould the study \\(x\\) be excluded for theoretical or statistical (e.g., outliers) reasons?\nShould we use an equal or random-effects model?\nWhich value should take the pre-post missing correlation?\n‚Ä¶"
  },
  {
    "objectID": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#an-example-pre-post-designs",
    "href": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#an-example-pre-post-designs",
    "title": "Multiverse",
    "section": "An example: Pre-post designs",
    "text": "An example: Pre-post designs\nLet‚Äôs imagine that we have \\(k\\) studies about the efficacy of a certain treatment. They collected a sample of participants measuring a certain variable \\(y\\) before and after the treatment.\nWith this design we can summarise the effect computing the difference between the average of the two time points.\nAn effect size (e.g., Cohen‚Äôs \\(d\\)) can be calculated as:\n\\[\nd = \\frac{\\overline y_{post} - \\overline y_{pre}}{\\sigma_p}\n\\]\nAnd \\(\\sigma_p\\) is the pooled pre-post standard deviation."
  },
  {
    "objectID": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#an-example-pre-post-cohens-d",
    "href": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#an-example-pre-post-cohens-d",
    "title": "Multiverse",
    "section": "An example: Pre-post Cohen‚Äôs \\(d\\)\n",
    "text": "An example: Pre-post Cohen‚Äôs \\(d\\)\n\nWith a pre-post Cohen‚Äôs \\(d\\) we need the pre-post correlation \\(\\rho\\) to calculate the sampling variance:\n\\[\n\\sigma^2_{\\epsilon_{pp}} = \\frac{2(1 - \\rho)}{n} + \\frac{d^2}{2n}\n\\]\n\\(\\rho\\) is usually non reported and need to be chosen from previous literature or a plausible guess."
  },
  {
    "objectID": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#pre-post-cohens-d",
    "href": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#pre-post-cohens-d",
    "title": "Multiverse",
    "section": "Pre-post Cohen‚Äôs \\(d\\)\n",
    "text": "Pre-post Cohen‚Äôs \\(d\\)\n\n\nCodev_dm &lt;- function(r, d, n){\n    2*(1-r)/n + d^2/(2*n)\n}\n\nvdd &lt;- data.frame(x = seq(-1, 1, 0.001))\nvdd$y &lt;- v_dm(vdd$x, 0, 40)\n\nggplot(vdd, aes(x = x, y = y)) +\n    geom_line() +\n    xlab(expression(rho)) +\n    ylab(expression(sigma^2)) +\n    theme_minimal(25)"
  },
  {
    "objectID": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#a-simulated-example",
    "href": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#a-simulated-example",
    "title": "Multiverse",
    "section": "A simulated example",
    "text": "A simulated example\nThe data structure: multiple measures of the same outcome within each paper\n\n\n\n\n\nstudy\noutcome\nni\nyi\nvi\n\n\n\n1\n1\n25\n0.65\n0.09\n\n\n1\n2\n25\n0.42\n0.08\n\n\n1\n3\n25\n-0.71\n0.09\n\n\n2\n1\n88\n-0.07\n0.02\n\n\n2\n2\n88\n0.15\n0.02\n\n\n...\n...\n...\n...\n...\n\n\n9\n3\n72\n-0.03\n0.03\n\n\n9\n4\n72\n-0.82\n0.03\n\n\n9\n5\n72\n0.13\n0.03\n\n\n10\n1\n164\n0.24\n0.01\n\n\n10\n2\n164\n0.58\n0.01"
  },
  {
    "objectID": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#simulation-approach",
    "href": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#simulation-approach",
    "title": "Multiverse",
    "section": "Simulation approach",
    "text": "Simulation approach\nLet‚Äôs make an example for a paper with \\(j = 3\\) measures of the outcome:\n\\[\n\\begin{bmatrix}\n        y_{11} \\\\\n        y_{12} \\\\\n        y_{13}\n    \\end{bmatrix}\n    =\n    \\begin{bmatrix}\n        \\mu_{\\theta_1} \\\\\n        \\mu_{\\theta_2} \\\\\n        \\mu_{\\theta_3}\n    \\end{bmatrix}\n    +\n    \\begin{bmatrix}\n        \\delta_{\\theta_1} \\\\\n        \\delta_{\\theta_2} \\\\\n        \\delta_{\\theta_3}\n    \\end{bmatrix}\n    +\n    \\begin{bmatrix}\n        \\epsilon_{\\theta_{11}} \\\\\n        \\epsilon_{\\theta_{12}} \\\\\n        \\epsilon_{\\theta_{13}}\n\\end{bmatrix}\n\\]\n\\[\n\\boldsymbol{\\delta} \\sim \\text{MVN}\\left(\n    \\begin{matrix}\n    0 \\\\\n    0 \\\\\n    0\n    \\end{matrix}\n    \\\n    \\begin{matrix}\n        \\tau^2_1 &        & \\\\\n        \\rho_{21}\\tau_2\\tau_1        & \\tau^2_1 & \\\\\n         \\rho_{31}\\tau_2\\tau_1        &  \\rho_{32}\\tau_2\\tau_1        & \\tau^2_1 \\\\\n    \\end{matrix}\n    \\right)\n\\]\n\\[\n\\boldsymbol{\\epsilon} \\sim \\text{MVN}\\left(\n    \\begin{matrix}\n    0 \\\\\n    0 \\\\\n    0\n    \\end{matrix}\n    \\\n    \\begin{matrix}\n        \\sigma^2_{\\epsilon_1} &        & \\\\\n        \\rho_{21}\\sigma^2_{\\epsilon_2}\\sigma^2_{\\epsilon_1}        & \\sigma^2_{\\epsilon_2} & \\\\\n        \\rho_{31}\\sigma^2_{\\epsilon_3}\\sigma^2_{\\epsilon_1}         &  \\rho_{32}\\sigma^2_{\\epsilon_3}\\sigma^2_{\\epsilon_2}        & \\sigma^2_{\\epsilon_3} \\\\\n    \\end{matrix}\n    \\right)\n\\]"
  },
  {
    "objectID": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#simulation-approach-1",
    "href": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#simulation-approach-1",
    "title": "Multiverse",
    "section": "Simulation approach",
    "text": "Simulation approach\nWe simulated individual participant data, thus:\n\nSampling the true values \\(\\theta_{ij}\\) for each study \\(i\\) and outcome \\(j\\) from the multivariate distribution\nGenerating \\(n_i\\) pre and post data with correlation \\(\\rho\\)\n\nCalculating the effect size (imputing the pre-post correlation)\nAggregating multiple outcomes within the same paper (imputing the correlation)\nFitting the meta-analyis model\nCalculating the scores\nRepeating 3-4 for each scenario\nUsing PIMA"
  },
  {
    "objectID": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#simulation-approach-2",
    "href": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#simulation-approach-2",
    "title": "Multiverse",
    "section": "Simulation approach",
    "text": "Simulation approach\nWe simulated a relatively simple but plausible multiverse with:\n\n4 pre-post correlations\n4 correlations between multiple measures of the same outcome\n2 meta-analysis models (fixed and random-effects)\n\nFor a total of 32 multiverse scenarios."
  },
  {
    "objectID": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#results",
    "href": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#results",
    "title": "Multiverse",
    "section": "Results",
    "text": "Results\n\nCodep_beta &lt;- ggplot(multi, aes(y = b)) +\n    xlim(c(-0.5,0.5)) +\n    geom_boxplot(width = 0.5,\n                 fill = \"dodgerblue\",\n                 alpha = 0.6) + \n    ylab(latex2exp::TeX(\"$\\\\mu_{\\\\,\\\\, \\\\theta}$\")) +\n    theme_minimal(30) +\n    theme(axis.text.x = element_blank())\n\np_pval &lt;- ggplot(multi, aes(y = -log10(p.value))) +\n    xlim(c(-0.5,0.5)) +\n    geom_hline(yintercept = -log10(0.05)) +\n    geom_boxplot(width = 0.5,\n                 fill = \"dodgerblue\",\n                 alpha = 0.6) + \n    ylab(latex2exp::TeX(\"Raw $-log_{10}(p)$\")) +\n    theme_minimal(30) +\n    theme(axis.text.x = element_blank())\n\nrdata &lt;- cor(zi)\nrdata &lt;- data.frame(r = rdata[upper.tri(rdata, diag = FALSE)])\n\np_cor &lt;- ggplot(rdata, aes(y = r)) +\n    xlim(c(-0.5,0.5)) +\n    geom_boxplot(width = 0.5,\n                 fill = \"dodgerblue\",\n                 alpha = 0.6) + \n    ylab(latex2exp::TeX(\"$\\\\rho$\")) +\n    theme_minimal(30) +\n    theme(axis.text.x = element_blank()) +\n    ylim(c(-1,1))\n\ncowplot::plot_grid(p_beta, p_cor, p_pval, nrow = 1)"
  },
  {
    "objectID": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#an-example-from-plessen2023-ex",
    "href": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#an-example-from-plessen2023-ex",
    "title": "Multiverse",
    "section": "An example from Plessen et al. (2023)\n",
    "text": "An example from Plessen et al. (2023)"
  },
  {
    "objectID": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#an-example-from-plessen2023-ex-1",
    "href": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#an-example-from-plessen2023-ex-1",
    "title": "Multiverse",
    "section": "An example from Plessen et al. (2023)\n",
    "text": "An example from Plessen et al. (2023)\n\n\nOver the last four decades, more than 80 meta-analyses have examined the efficacy of psychotherapies for depression\nMore than 700 randomised controlled trials (RCTs)\nNot all these studies goes in the same direction"
  },
  {
    "objectID": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#an-example-from-plessen2023-ex-2",
    "href": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#an-example-from-plessen2023-ex-2",
    "title": "Multiverse",
    "section": "An example from Plessen et al. (2023)\n",
    "text": "An example from Plessen et al. (2023)\n\nDiscrepancies in results could be due to:\nWhich factors (which data to meta-analyze)\n\ninclusion/esclusion of a subset of studies (e.g., low quality studies)\ntype of control group or control therapy\n‚Ä¶\n\nHow factors (how to meta-analyze)\n\ntype of model (e.g., equal vs random)\nmodel complexity (two-level, three level, robust, etc.)\ncorrecting for publication bias"
  },
  {
    "objectID": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#describing-the-multiverse",
    "href": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#describing-the-multiverse",
    "title": "Multiverse",
    "section": "Describing the multiverse",
    "text": "Describing the multiverse\nHall et al. (2022) and Liu et al. (2021) proposed several tools to describe the results of a multiverse analysis.\nThe increase in complexity NEED to be managed using appropriate tools to summarise and visualize the results."
  },
  {
    "objectID": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#estimated-effect-of-interest",
    "href": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#estimated-effect-of-interest",
    "title": "Multiverse",
    "section": "Estimated effect of interest",
    "text": "Estimated effect of interest\n\nCodespecif_res |&gt; \n  ggplot(aes(x = b)) +\n  geom_histogram(bins = 50,\n                 fill = \"dodgerblue\",\n                 col = \"black\") +\n  ylab(\"Frequency\") +\n  xlab(\"Estimated Effect\") +\n  geom_vline(xintercept = 0, col = \"red\") +\n  geom_vline(xintercept = 0.25, col = \"green\")"
  },
  {
    "objectID": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#estimated-effect-as-a-function-of-scenarios",
    "href": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#estimated-effect-as-a-function-of-scenarios",
    "title": "Multiverse",
    "section": "Estimated effect as a function of scenarios1\n",
    "text": "Estimated effect as a function of scenarios1\n\n\nCodeabove &lt;- specif_res |&gt; \n  arrange(b) |&gt; \n  mutate(id = 1:n()) |&gt; \n  ggplot(aes(x = id, y = b)) +\n  geom_segment(aes(xend = id, y = ci.lb, yend = ci.ub)) +\n  geom_hline(yintercept = 0, col = \"red\") +\n  geom_hline(yintercept = 0.25, col = \"green\") +\n  theme(axis.title.y = element_blank(),\n        axis.text.x = element_blank(),\n        axis.title.x = element_blank())\n  \nbelow &lt;- specif_res |&gt; \n  arrange(b) |&gt; \n  mutate(id = 1:n()) |&gt; \n  pivot_longer(c(\n    target_group,\n    format,\n    diagnosis,\n    risk_of_bias,\n    type,\n    method\n  )) |&gt; \n  ggplot(aes(x = id, y = value)) +\n  geom_tile() +\n  theme(axis.title.y = element_blank(),\n        axis.text.x = element_blank()) +\n  xlab(\"Specification\")\n\nplot_grid(above, below, nrow = 2, align = \"hv\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n  \n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n  \n  \n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n  \n  \n  \n  \n  \n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n  \n  \n  \n  \n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n  \n  \n  \n  \n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n\n\nThe red line is the 0 effect and the green line is a minimum effect of interest in the research field"
  },
  {
    "objectID": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#fixing-a-specific-parameter",
    "href": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#fixing-a-specific-parameter",
    "title": "Multiverse",
    "section": "Fixing a specific parameter",
    "text": "Fixing a specific parameter\nWe can the effect of a set of scenarios having a certain parameter:\n\nCodespecif_res |&gt; \n  arrange(b) |&gt; \n  mutate(id = 1:n()) |&gt; \n  ggplot(aes(x = b, fill = diagnosis)) +\n  geom_density(alpha = 0.5)"
  },
  {
    "objectID": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#specification-curve-simonsohn2020-sr",
    "href": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#specification-curve-simonsohn2020-sr",
    "title": "Multiverse",
    "section": "Specification Curve Simonsohn, Simmons, and Nelson (2020)\n",
    "text": "Specification Curve Simonsohn, Simmons, and Nelson (2020)\n\nThe idea of the specification curve is both descriptive (see previous plots) and inferential.\nInferentially, the approach requires to generate a new dataset under the null hypothesis and compare with the observed value."
  },
  {
    "objectID": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#specification-curve-in-meta-analysis-plessen2023-ex",
    "href": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#specification-curve-in-meta-analysis-plessen2023-ex",
    "title": "Multiverse",
    "section": "Specification Curve in meta-analysis Plessen et al. (2023)\n",
    "text": "Specification Curve in meta-analysis Plessen et al. (2023)\n\nPlessen et al. (2023) describe how to implement the method for meta-analysis.\n\nFor each dataset/model generate \\(k_s\\) data (where \\(k\\) is the number of effect in the specification \\(s\\)) sampling from \\(\\mathcal{N}(0,\\sigma^2_{\\epsilon_i} + \\tau^2)\\).\nFit the model (FE or RE)\nGet the specification curve\nRepeat 1-3 for a large number of times (e.g., 1000)\nCompute the 2.5% and 95.75% quantiles\nCheck if the observed specification is outside the 95% CI"
  },
  {
    "objectID": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#specification-curve-in-meta-analysis-plessen2023-ex-1",
    "href": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#specification-curve-in-meta-analysis-plessen2023-ex-1",
    "title": "Multiverse",
    "section": "Specification Curve in meta-analysis Plessen et al. (2023)\n",
    "text": "Specification Curve in meta-analysis Plessen et al. (2023)\n\n\nCodespecif$SD |&gt; \n  filter(spec %in% sample(1:1000, 1)) |&gt; \n  ggplot(aes(x = id, y = b)) +\n  geom_line(aes(group = spec))"
  },
  {
    "objectID": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#specification-curve-in-meta-analysis-plessen2023-ex-2",
    "href": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#specification-curve-in-meta-analysis-plessen2023-ex-2",
    "title": "Multiverse",
    "section": "Specification Curve in meta-analysis Plessen et al. (2023)\n",
    "text": "Specification Curve in meta-analysis Plessen et al. (2023)\n\n\nCodespecif$SD |&gt; \n  filter(spec %in% sample(1:1000, 1)) |&gt; \n  ggplot(aes(x = id, y = b)) +\n  geom_line(aes(group = spec))"
  },
  {
    "objectID": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#specification-curve-in-meta-analysis-plessen2023-ex-3",
    "href": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#specification-curve-in-meta-analysis-plessen2023-ex-3",
    "title": "Multiverse",
    "section": "Specification Curve in meta-analysis Plessen et al. (2023)\n",
    "text": "Specification Curve in meta-analysis Plessen et al. (2023)\n\n\nCodespecif$SD |&gt; \n  filter(spec %in% sample(1:1000, 1)) |&gt; \n  ggplot(aes(x = id, y = b)) +\n  geom_line(aes(group = spec))"
  },
  {
    "objectID": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#specification-curve-in-meta-analysis-plessen2023-ex-4",
    "href": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#specification-curve-in-meta-analysis-plessen2023-ex-4",
    "title": "Multiverse",
    "section": "Specification Curve in meta-analysis Plessen et al. (2023)\n",
    "text": "Specification Curve in meta-analysis Plessen et al. (2023)\n\n\nCodespecif_res &lt;- specif_res |&gt; \n  arrange(b) |&gt; \n  mutate(id = 1:n())\n\nggplot(data = specif$SD,\n       aes(x = id, y = b)) +\n  geom_line(aes(group = spec)) +\n  geom_line(data = specif_res,\n            aes(x = id, y = b),\n            col = \"firebrick\",\n            lwd = 2)"
  },
  {
    "objectID": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#pima",
    "href": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#pima",
    "title": "Multiverse",
    "section": "PIMA",
    "text": "PIMA\nP-selection Inference in Multiverse Meta-analysis (PIMA) is an inferential approach to multiverse analysis is an inferential framework for:\n\nobtaining an overall p-value across the multiverse\nobtaining corrected p-values based on resampling methods"
  },
  {
    "objectID": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#pima-girardi2024-ip",
    "href": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#pima-girardi2024-ip",
    "title": "Multiverse",
    "section": "PIMA (Girardi et al. 2024)\n",
    "text": "PIMA (Girardi et al. 2024)"
  },
  {
    "objectID": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#pima-on-meta-analysis",
    "href": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#pima-on-meta-analysis",
    "title": "Multiverse",
    "section": "PIMA on meta-analysis",
    "text": "PIMA on meta-analysis\nWe implemented PIMA also on meta-analysis but (for the moment) limited to two-level models without moderators."
  },
  {
    "objectID": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#pima-results",
    "href": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#pima-results",
    "title": "Multiverse",
    "section": "PIMA Results",
    "text": "PIMA Results\n\nCodemulti_flip_overall &lt;- flip::npc(multi_flip, comb.funct = \"maxT\")\nmulti_p &lt;- multi_flip_overall@res$`p-value`\nfp &lt;- function(p){\n    ifelse(p &lt;= 0.001, \"p &lt; 0.001\", as.character(round(p, 3)))\n}\n\n\nThe multiverse is associated with an overall p value of 0.016 1. Then we can describe the overall results:\n\nCodep_beta &lt;- ggplot(multi, aes(y = b)) +\n    xlim(c(-0.5,0.5)) +\n    geom_boxplot(width = 0.5,\n                 fill = \"dodgerblue\",\n                 alpha = 0.6) + \n    ylab(latex2exp::TeX(\"$\\\\mu_{\\\\,\\\\, \\\\theta}$\")) +\n    theme_minimal(30) +\n    theme(axis.text.x = element_blank())\n\np_pval &lt;- ggplot(multi, aes(y = -log10(p.value))) +\n    xlim(c(-0.5,0.5)) +\n    geom_hline(yintercept = -log10(0.05)) +\n    geom_boxplot(width = 0.5,\n                 fill = \"dodgerblue\",\n                 alpha = 0.6) + \n    ylab(latex2exp::TeX(\"Raw $-log_{10}(p)$\")) +\n    theme_minimal(30) +\n    theme(axis.text.x = element_blank())\n\nrdata &lt;- cor(zi)\nrdata &lt;- data.frame(r = rdata[upper.tri(rdata, diag = FALSE)])\n\np_cor &lt;- ggplot(rdata, aes(y = r)) +\n    xlim(c(-0.5,0.5)) +\n    geom_boxplot(width = 0.5,\n                 fill = \"dodgerblue\",\n                 alpha = 0.6) + \n    ylab(latex2exp::TeX(\"$\\\\rho$\")) +\n    theme_minimal(30) +\n    theme(axis.text.x = element_blank()) +\n    ylim(c(-1,1))\n\ncowplot::plot_grid(p_beta, p_cor, p_pval, nrow = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  \n  \n\n\n  \n  \n  \n  \n\n\n  \n  \n  \n  \n\n\n  \n  \n  \n  \n\n\n  \n  \n  \n  \n\n\n  \n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  \n  \n\n\n  \n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n  \n  \n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n  \n\n\n  \n\n\n  \n\n\n  \n\n\n\n\n\n\n\ncombined using the maxT method by Westfall and Stanley Young (1993)"
  },
  {
    "objectID": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#impact-of-multiplicity-correction",
    "href": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#impact-of-multiplicity-correction",
    "title": "Multiverse",
    "section": "Impact of multiplicity correction",
    "text": "Impact of multiplicity correction\n\nCodetp &lt;- function(p){\n    -log10(p)\n}\n\nlbl &lt;- c(\"Never $p \\\\leq 0.05$\", \n         \"Before correction $p \\\\leq 0.05$\", \n         \"After correction $p \\\\leq 0.05$\")\n\n\nmulti$sign &lt;- factor(multi$sign, \n                     labels = latex2exp::TeX(lbl))\n\nggplot(multi, aes(x = tp(p.value), tp(adjust.maxt), color = sign)) +\n    geom_hline(yintercept = tp(0.05), alpha = 0.4) +\n    geom_vline(xintercept = tp(0.05), alpha = 0.4) +\n    geom_abline(linetype = \"dashed\", lwd = 0.5) +\n    geom_point(size = 5,\n               position = position_jitter(width = 0.1),\n               alpha = 0.5) +\n    xlim(c(0, 3)) +\n    ylim(c(0, 3)) +\n    xlab(TeX(\"Raw p value ($-log_{10}$)\")) +\n    ylab(TeX(\"maxT p value ($-log_{10}$)\")) +\n    theme_minimal(base_size = 20) +\n    theme(legend.title = element_blank(),\n          legend.position = \"bottom\") +\n    scale_color_manual(labels = scales::parse_format(), values = c(\"#F8766D\", \"#7CAE00\", \"#00BFC4\"))"
  },
  {
    "objectID": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#important-aspects-of-multiverse-analysis-1",
    "href": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#important-aspects-of-multiverse-analysis-1",
    "title": "Multiverse",
    "section": "Important aspects of multiverse analysis",
    "text": "Important aspects of multiverse analysis\n\nInclude only plausible scenarios. Regardless the aim (description or inference) of the multiverse, the results are meaningful if and only if scenarios are plausible.\nScenarios are assumed to be equally plausible but we could imagine a plausibility weight (future direction)\nMultiverse analysis is also useful to estimate the degree of variability according to plausible choices"
  },
  {
    "objectID": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#r-packages",
    "href": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#r-packages",
    "title": "Multiverse",
    "section": "R Packages",
    "text": "R Packages\n\nhttps://mucollective.github.io/multiverse/\nhttps://mverseanalysis.github.io/mverse/\nhttps://github.com/uwdata/boba"
  },
  {
    "objectID": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#multiverse-projects",
    "href": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#multiverse-projects",
    "title": "Multiverse",
    "section": "Multiverse projects",
    "text": "Multiverse projects\n\n\nDafflon et al. (2022) implemented a multiverse analysis for neuroimaging data\n\nHoogeveen et al. (2023) implemented a Bayesian multiverse analysis within the Many Labs 4 project (a large scale replication project in Psychology)\n\nOlsson-Collentine et al. (2023) re-analyzed some replication projects using a multiverse approach"
  },
  {
    "objectID": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#multiverse-is-catchy",
    "href": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#multiverse-is-catchy",
    "title": "Multiverse",
    "section": "Multiverse is catchy!",
    "text": "Multiverse is catchy!\n\nCodemulti_cit &lt;- data.frame(\n        year = c(2024L,2023L,2022L,2021L,2020L,2019L,\n                 2018L,2017L,2016L,2015L,2014L,2013L),\n      papers = c(30L, 46L, 33L, 19L, 10L, 11L, 4L, 2L, 3L, 0L, 1L, 1L),\n       total = c(652674L,1295092L,1317981L,1299609L,\n                 1161910L,1044346L,955091L,907118L,874112L,840256L,804004L,\n                 777242L)\n)\n\nmulti_cit |&gt; \n    mutate(year = as.integer(year)) |&gt; \n    ggplot(aes(x = year, y = papers/total)) +\n    geom_line() +\n    geom_label(aes(label = papers)) +\n    scale_x_continuous(breaks = seq(2012, 2024, 1)) +\n    theme(axis.title.x = element_blank(),\n          axis.text.y = element_blank(),\n          axis.ticks.y = element_blank()) +\n    ylab(\"# papers on Multiverse\")"
  },
  {
    "objectID": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#references",
    "href": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#references",
    "title": "Multiverse",
    "section": "References",
    "text": "References\n\n\nDafflon, Jessica, Pedro F Da Costa, Franti≈°ek V√°≈°a, Ricardo Pio Monti, Danilo Bzdok, Peter J Hellyer, Federico Turkheimer, Jonathan Smallwood, Emily Jones, and Robert Leech. 2022. ‚ÄúA Guided Multiverse Study of Neuroimaging Analyses.‚Äù Nature Communications 13 (June): 3758. https://doi.org/10.1038/s41467-022-31347-8.\n\n\nFollmann, D A, and M A Proschan. 1999. ‚ÄúValid Inference in Random Effects Meta-Analysis.‚Äù Biometrics 55 (September): 732‚Äì37. https://doi.org/10.1111/j.0006-341x.1999.00732.x.\n\n\nGirardi, Paolo, Anna Vesely, Dani√´l Lakens, Gianmarco Alto√®, Massimiliano Pastore, Antonio Calcagn√¨, and Livio Finos. 2024. ‚ÄúPost-Selection Inference in Multiverse Analysis (PIMA): An Inferential Framework Based on the Sign Flipping Score Test.‚Äù Psychometrika, April. https://doi.org/10.1007/s11336-024-09973-6.\n\n\nHall, B D, Y Liu, Y Jansen, P Dragicevic, F Chevalier, and M Kay. 2022. ‚ÄúA Survey of Tasks and Visualizations in Multiverse Analysis Reports.‚Äù Computer Graphics Forum: Journal of the European Association for Computer Graphics 41: 402‚Äì26. https://doi.org/10.1111/cgf.14443.\n\n\nHoogeveen, Suzanne, Sophie W Berkhout, Quentin F Gronau, Eric-Jan Wagenmakers, and Julia M Haaf. 2023. ‚ÄúImproving Statistical Analysis in Team Science: The Case of a Bayesian Multiverse of Many Labs 4.‚Äù Advances in Methods and Practices in Psychological Science 6 (July): 25152459231182318. https://doi.org/10.1177/25152459231182318.\n\n\nLiu, Yang, Alex Kale, Tim Althoff, and Jeffrey Heer. 2021. ‚ÄúBoba: Authoring and Visualizing Multiverse Analyses.‚Äù IEEE Transactions on Visualization and Computer Graphics 27 (February): 1753‚Äì63. https://doi.org/10.1109/TVCG.2020.3028985.\n\n\nOlsson-Collentine, Anton, Robbie C M van Aert, Marjan Bakker, and Jelte Wicherts. 2023. ‚ÄúMeta-Analyzing the Multiverse: A Peek Under the Hood of Selective Reporting.‚Äù Psychological Methods, May. https://doi.org/10.1037/met0000559.\n\n\nPlessen, Constantin Yves, Eirini Karyotaki, Clara Miguel, Marketa Ciharova, and Pim Cuijpers. 2023. ‚ÄúExploring the Efficacy of Psychotherapies for Depression: A Multiverse Meta-Analysis.‚Äù BMJ Mental Health 26 (February). https://doi.org/10.1136/bmjment-2022-300626.\n\n\nSimonsohn, Uri, Joseph P Simmons, and Leif D Nelson. 2020. ‚ÄúSpecification Curve Analysis.‚Äù Nature Human Behaviour 4 (November): 1208‚Äì14. https://doi.org/10.1038/s41562-020-0912-z.\n\n\nSteegen, Sara, Francis Tuerlinckx, Andrew Gelman, and Wolf Vanpaemel. 2016. ‚ÄúIncreasing Transparency Through a Multiverse Analysis.‚Äù Perspectives on Psychological Science: A Journal of the Association for Psychological Science 11 (September): 702‚Äì12. https://doi.org/10.1177/1745691616658637.\n\n\nWestfall, Peter H, and S Stanley Young. 1993. Resampling-Based Multiple Testing: Examples and Methods for p-Value Adjustment. John Wiley & Sons. https://play.google.com/store/books/details?id=nuQXORVGI1QC."
  },
  {
    "objectID": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#general-workflow",
    "href": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#general-workflow",
    "title": "Multiverse",
    "section": "General Workflow",
    "text": "General Workflow"
  },
  {
    "objectID": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#meta-analysis-with-permutations-follmann1999-ha",
    "href": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#meta-analysis-with-permutations-follmann1999-ha",
    "title": "Multiverse",
    "section": "Meta-analysis with permutations (Follmann and Proschan 1999)\n",
    "text": "Meta-analysis with permutations (Follmann and Proschan 1999)\n\nWith \\(k\\) observed studies where \\(y_i\\) and \\(\\sigma^2_{\\epsilon_i}\\) being the observed effect sizes and sampling variances:\n\nGenerate a random vector \\(\\mathbf{s}\\) of \\(\\pm 1\\) of length \\(k\\)\n\nMultiply the \\(\\mathbf{y}\\) vector with the \\(s\\) vector\nFit the meta-analysis model and calculate \\(z^{*}_j\\) (\\(j\\) for permuted)\nRepeat 1-3 for a large number of times \\(B\\). With small \\(k\\) we can do all the permutations \\(B = 2^k \\times k\\)\n\n\nThe first permutation (\\(j = 1\\)) is the observed data. The p value can be computed as:\n\\[\np = \\frac{\\text{\\#}(|z^{*}_j| &gt; |z^{*}_1|)}{B}\n\\]"
  },
  {
    "objectID": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#fast-meta-analysis-using-permutations",
    "href": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#fast-meta-analysis-using-permutations",
    "title": "Multiverse",
    "section": "Fast meta-analysis using permutations",
    "text": "Fast meta-analysis using permutations\n\nMeta-analysis using permutations requires recomputing \\(\\tau^2\\) and \\(\\mu_\\theta\\) after each permutation.\nWe proposed to estimate \\(\\tau^2\\) under \\(H_0\\) and use the value for the permutations (without re-estimating it)\nThis is extremely fast especially for large datasets and several multiverse scenarios"
  },
  {
    "objectID": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#estimating-tau2-under-h_0",
    "href": "slides/04-multiverse-meta-analysis/04-multiverse-meta-analysis.html#estimating-tau2-under-h_0",
    "title": "Multiverse",
    "section": "Estimating \\(\\tau^2\\) under \\(H_0\\)\n",
    "text": "Estimating \\(\\tau^2\\) under \\(H_0\\)\n\nThe crucial step is the point (1). This requires maximizing the log-likelihood fixing \\(\\mu_\\theta = 0\\):\n\\[\nL(\\mu_{\\theta}, \\tau^2|\\mathbf{y}) = -\\frac{1}{2}\\sum_{i = 1}^k \\ln(\\tau^2 + \\sigma_{\\epsilon_i}^2) - \\frac{1}{2} \\sum_{i = 1}^k \\frac{(y_i - \\mu_{\\theta})^2}{\\tau^2 +  \\sigma_{\\epsilon_i}^2}\n\\]\nThis can be done in R using some optimizer function (e.g., optim) or using directly the metafor package that allows fixing some parameters that are usually estimated."
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#some-random-concepts-1",
    "href": "slides/02-replication-methods/02-replication-methods.html#some-random-concepts-1",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Some (random) concepts",
    "text": "Some (random) concepts\n\nCredibility of scientific claims is established with evidence for their replicability using new data (Nosek and Errington 2020)\n\n\nReplication is repeating a study‚Äôs procedure and observing whether the prior finding recurs (Jeffreys 1973)\n\n\nReplication is a study for which any outcome would be considered diagnostic evidence about a claim from prior research (Nosek and Errington 2020)."
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#difficulty-in-drawing-conclusions-from-replications",
    "href": "slides/02-replication-methods/02-replication-methods.html#difficulty-in-drawing-conclusions-from-replications",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Difficulty in drawing conclusions from replications",
    "text": "Difficulty in drawing conclusions from replications\nReplication is often intended as conditioned to the original result. The original result could be a false positive or a biased result. Also the replication attempt could be a false positive or a false negative (Nosek and Errington 2020).\n\n\nTo be a replication, two things must be true. Outcomes consistent with a prior claim would increase confidence in the claim, and outcomes inconsistent with a prior claim would decrease confidence in the claim (Nosek and Errington 2020).\n\n\n\nThis is somehow similar with a Bayesian reasoning where evidence about a phenomenon is updated after collecting more data."
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#exact-and-conceptual-replications",
    "href": "slides/02-replication-methods/02-replication-methods.html#exact-and-conceptual-replications",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Exact and Conceptual replications",
    "text": "Exact and Conceptual replications\nExact replications are commonly considered as the gold-standard but in practice (especially in Social Sciences, Psychology, etc.) are rare.\nLet‚Äôs imagine, an original study \\(y_{or}\\) finding a result.\n\nReplication \\(y_{rep}\\) with the exact same method find the same result. Replication or not?\n\nReplication \\(y_{rep}\\) with a similar method find the same result. Replication or not?\n\nReplication \\(y_{rep}\\) with similar method did not find the same result. Replication or not?"
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#direct-and-conceptual-replications-schmidt2009-mq",
    "href": "slides/02-replication-methods/02-replication-methods.html#direct-and-conceptual-replications-schmidt2009-mq",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Direct and Conceptual replications (S. Schmidt 2009)\n",
    "text": "Direct and Conceptual replications (S. Schmidt 2009)\n\nA direct replication is defined as the repetition of an experimental procedure.\nA conceptual replication is defined as testing the same hypothesis with different methods."
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#exact-replications-are-often-impossible-schmidt2009-mq",
    "href": "slides/02-replication-methods/02-replication-methods.html#exact-replications-are-often-impossible-schmidt2009-mq",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Exact replications are (often) impossible (S. Schmidt 2009)\n",
    "text": "Exact replications are (often) impossible (S. Schmidt 2009)\n\nLet‚Äôs imagine an extreme example: testing the physiological reaction to arousing situation:\n\nThe original study: Experiment with prehistoric reacting to an arousing stimulus\nThe actual replication: It is possible to create the exact situation? Some phenomenon changes overtime, especially people-related phenomenon\n\nExact replication is often not feasible. Even using the same experimental setup (direct replication) does not assure that we are studying the same phenomenon."
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#as-exact-as-possible",
    "href": "slides/02-replication-methods/02-replication-methods.html#as-exact-as-possible",
    "title": "Statistical Methods for Replication Assessment",
    "section": "As Exact as possible‚Ä¶",
    "text": "As Exact as possible‚Ä¶\nEven when an experiment use almost the same setup of the original study there is a source of unknown uncertainty. Which is the impact of a slightly change in the experimental setup on the actual result?\n\nA study on the human visual system: presenting stimuli on different monitors ‚Äì&gt; small change with a huge impact\nA study on consumer behavior: participant answering question using a smartphone or a computer ‚Äì&gt; small but (maybe) irrelevant change\n\nHow to evaluate the actual impact?"
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#what-we-are-not-going-to-do",
    "href": "slides/02-replication-methods/02-replication-methods.html#what-we-are-not-going-to-do",
    "title": "Statistical Methods for Replication Assessment",
    "section": "What we are (not) going to do?",
    "text": "What we are (not) going to do?\n\n\nI will not present a strictly theoretical and philosophical approach to replication (what is a replication?, what is the most appropriate definition?, etc.). But we can discuss it together üòÑ!\n\n\n\n\nAccording to the replication definitions and problems, we will explore some statistical methods to evaluate a replication success"
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#overall-model-and-notation-1",
    "href": "slides/02-replication-methods/02-replication-methods.html#overall-model-and-notation-1",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Overall model and notation",
    "text": "Overall model and notation\nFor the purpose of notation and simplicity we can define a meta-analytical-based replication model (Hedges and Schauer 2019b; J. M. Schauer and Hedges 2021; Jacob M. Schauer 2022)\n\\[\ny_i = \\mu_{\\theta} + \\delta_i + \\epsilon_i\n\\]\n\\[\n\\delta_i \\sim \\mathcal{N}(0, \\tau^2)\n\\]\n\\[\n\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2_i)\n\\]"
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#overall-model-and-notation-2",
    "href": "slides/02-replication-methods/02-replication-methods.html#overall-model-and-notation-2",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Overall model and notation",
    "text": "Overall model and notation\n\nThus each study \\(i\\) out of the number of studies \\(k\\).\n\n\\(\\mu_{\\theta}\\) is the real average effect and \\(\\theta = \\mu_{\\theta} + \\delta_i\\) is the real effect of each study\n\n\\(\\tau^2\\) is the real variance among different studies. When \\(\\tau^2 = 0\\) there is no variability among studies\n\n\\(\\epsilon_i\\) are the sampling errors that depends on \\(\\sigma^2_i\\), the sampling variability of each study\nWe define \\(\\theta_{orig}\\) (or \\(\\theta_1\\)) as the original study and \\(\\theta_{rep_i}\\) (with \\(i\\) from 2 to \\(k\\)) as the replication studies"
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#simulating-for-learning",
    "href": "slides/02-replication-methods/02-replication-methods.html#simulating-for-learning",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Simulating for learning",
    "text": "Simulating for learning\nFor the examples we are going to simulate studies. Each study comes from a two-groups comparison on a continous outcome:\n\\[\n\\Delta = \\overline{X_1} - \\overline{X_2}\n\\]\n\\[\nSE_{\\Delta} = \\sqrt{\\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2}}\n\\]\nWith \\(X_{1_j} \\sim \\mathcal{N}(0, 1)\\) and \\(X_{2_j} \\sim \\mathcal{N}(\\Delta, 1)\\)\ncontinue‚Ä¶"
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#simulating-for-learning-1",
    "href": "slides/02-replication-methods/02-replication-methods.html#simulating-for-learning-1",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Simulating for learning",
    "text": "Simulating for learning\nThus our observed effect sizes \\(y_i\\) is sampled from: \\[\ny_i \\sim \\mathcal{N}(\\mu_\\theta, \\tau^2 + \\frac{1}{n_1} + \\frac{1}{n_2})\n\\]\nWhere \\(\\frac{1}{n_1} + \\frac{1}{n_2}\\) is the sampling variability (\\(\\sigma^2_i\\)).\nThe sampling variances are sampled from:\n\\[\n\\sigma_i^2 \\sim \\frac{\\chi^2_{n_1 + n_2 - 2}}{n_1 + n_2 - 2} (\\frac{1}{n_1} + \\frac{1}{n_2})\n\\]"
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#simulating-for-learning-2",
    "href": "slides/02-replication-methods/02-replication-methods.html#simulating-for-learning-2",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Simulating for learning",
    "text": "Simulating for learning\nEverything is implemented into the sim_studies() function:\n\nsim_studies &lt;- function(k, theta, tau2, n0, n1, summary = FALSE){\n  yi &lt;- rnorm(k, theta, sqrt(tau2 + 1/n0 + 1/n1))\n  vi &lt;- (rchisq(k, n0 + n1 - 2) / (n0 + n1 - 2)) * (1/n0 + 1/n1)\n  out &lt;- data.frame(yi, vi, sei = sqrt(vi))\n  if(summary){\n    out &lt;- summary_es(out)\n  }\n  return(out)\n}"
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#simulating-for-learning-an-example",
    "href": "slides/02-replication-methods/02-replication-methods.html#simulating-for-learning-an-example",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Simulating for learning, an example",
    "text": "Simulating for learning, an example\n\nsim_studies(k = 10, theta = 0.5, tau2 = 0.1, n0 = 30, n1 = 30)\n#&gt;             yi         vi       sei\n#&gt; 1   0.55035142 0.07077634 0.2660382\n#&gt; 2   1.21756456 0.06661776 0.2581042\n#&gt; 3  -0.12770419 0.05632815 0.2373355\n#&gt; 4   1.16168348 0.04273962 0.2067356\n#&gt; 5   0.02902981 0.05876974 0.2424247\n#&gt; 6   0.31032266 0.06052546 0.2460192\n#&gt; 7   0.14723265 0.06687206 0.2585963\n#&gt; 8   0.85115266 0.06841772 0.2615678\n#&gt; 9   0.42342508 0.07322471 0.2706006\n#&gt; 10  0.43565175 0.03883565 0.1970676"
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#exact-vs-approximate-replication",
    "href": "slides/02-replication-methods/02-replication-methods.html#exact-vs-approximate-replication",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Exact vs Approximate replication",
    "text": "Exact vs Approximate replication\nThis distinction (see Brandt et al. 2014 for a different terminology) refers to parameters \\(\\theta_i\\). With exact are considering a case where:\n\\[\n\\theta_1 = \\theta_2 = \\theta_3, \\dots, \\theta_k\n\\]\nThus the true parameters of \\(k\\) replication studies are the same. Thus the variability among true effects \\(\\tau^2 = 0\\).\nSimilarly, due to (often not controllable) differences among experiments (i.e., lab, location, sample, etc.) we could expect a certain degree of variability \\(\\tau^2\\). In other terms \\(\\tau^2 &lt; \\tau^2_0\\) where \\(\\tau^2_0\\) is the maximum variability (that need to be defined). In this way studies are replicating:\n\\[\n\\theta_i \\sim \\mathcal{N}(\\mu_\\theta, \\tau^2_0)\n\\]"
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#types-of-agreement",
    "href": "slides/02-replication-methods/02-replication-methods.html#types-of-agreement",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Types of agreement",
    "text": "Types of agreement\nCoarsely, we can define a replication success when two or more studies obtain the ‚Äúsame‚Äù result. The definion of sameness it is crucial:\n\nsame sign or direction: two studies (original and replication) evaluating the efficacy of a treatment have a positive effect \\(sign(\\theta_1) = sign(\\theta_2)\\) where \\(sign\\) is the sign function.\nsame magnitude: two studies (original and replication) evaluating the efficacy of a treatment have the same effect in terms \\(|\\theta_1 - \\theta_2| = 0\\) or similar up to a tolerance factor \\(|\\theta_1 - \\theta_2| &lt; \\gamma\\) where \\(\\gamma\\) is the maximum difference considered as null.\n\nThe different methods that we are going to see are focused on a specific type of aggreement. For example, we could consider \\(\\theta_1 = 3x\\) and \\(\\theta_2 = x\\) to have the same sign but the replication study is on a completely different scale. Is this considered a successful replication?"
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#falsification-vs-consistency",
    "href": "slides/02-replication-methods/02-replication-methods.html#falsification-vs-consistency",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Falsification vs Consistency",
    "text": "Falsification vs Consistency\n\nThis refers to how the replication setup is formulated. With \\(k = 2\\) studies where \\(k_1\\) is the original study and \\(k_2\\) is the replication we have a one-to-one setup. In this setup we compare the replication with the original and according to the chosen method and expectation we conclude if \\(k_1\\) has been replicated or not.\n\n\nWhen \\(k &gt; 2\\) we could collapse the replication studies into a single value (e.g., using a meta-analysis method) and compare the results using a one-to-one or we can use a method for one-to-many designs.\n\n\nRegardless the method, falsification approaches compared the original with the replicate(s) obtaining a yes-no answer or a continuous result. On the other side consistency methods are focused on evaluating the degree of similarity (i.e., consistency) among all studies."
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#the-big-picture",
    "href": "slides/02-replication-methods/02-replication-methods.html#the-big-picture",
    "title": "Statistical Methods for Replication Assessment",
    "section": "The big picture",
    "text": "The big picture\n\nStatistical MethodsSign/DirectionVote CountingConfidence/Prediction IntervalCI/PI original/replicationSmall TelescopeMeta-analysisEqual and Random-effectsQ StatisticsBayesian MethodsReplication Bayes Factor"
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#statistical-methods-disclaimer-schauer2021-ja",
    "href": "slides/02-replication-methods/02-replication-methods.html#statistical-methods-disclaimer-schauer2021-ja",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Statistical Methods, disclaimer (J. M. Schauer and Hedges 2021)\n",
    "text": "Statistical Methods, disclaimer (J. M. Schauer and Hedges 2021)\n\n\nThere are no unique methods to assess replication from a statistical point of view\nFor available statistical methods, statistical properties (e.g., type-1 error rate, power, bias, etc.) are not always known or extensively examined\nDifferent methods answers to the same question or to different replication definitions"
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#vote-counting-based-on-significance-or-direction",
    "href": "slides/02-replication-methods/02-replication-methods.html#vote-counting-based-on-significance-or-direction",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Vote Counting based on significance or direction",
    "text": "Vote Counting based on significance or direction\nThe simplest method is called vote counting (Valentine et al. 2011; Hedges and Olkin 1980). A replication attempt \\(\\theta_{rep}\\) is considered successful if the result has the same direction of the original study \\(\\theta_{orig}\\) and it is statistically significant i.e., \\(p_{\\theta_{rep}} \\leq \\alpha\\). Similarly we can count the number of replication with the same sign as the original study.\n\n\nEasy to understand, communicate and compute\n\n\n\n\nDid not consider the size of the effect\nDepends on the power of \\(\\theta_{rep}\\)"
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#example-with-simulated-data",
    "href": "slides/02-replication-methods/02-replication-methods.html#example-with-simulated-data",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Example with simulated data",
    "text": "Example with simulated data\nLet‚Äôs simulate an exact replication:\n\nCode## original study\nn_orig &lt;- 30\ntheta_orig &lt;- theta_from_z(2, n_orig)\n\norig &lt;- data.frame(\n  yi = theta_orig,\n  vi = 4/(n_orig*2)\n)\n\norig$sei &lt;- sqrt(orig$vi)\norig &lt;- summary_es(orig)\n\norig\n#&gt;          yi         vi       sei zi       pval      ci.lb    ci.ub\n#&gt; 1 0.5163978 0.06666667 0.2581989  2 0.04550026 0.01033725 1.022458\n\nCode\n## replications\n\nk &lt;- 10\nreps &lt;- sim_studies(k = k, theta = theta_orig, tau2 = 0, n_orig, n_orig, summary = TRUE)\n\nhead(reps)\n#&gt;           yi         vi       sei        zi        pval      ci.lb     ci.ub\n#&gt; 1 0.34239273 0.07193516 0.2682073 1.2765973 0.201744465 -0.1832840 0.8680694\n#&gt; 2 0.29894480 0.06672578 0.2583133 1.1572953 0.247151728 -0.2073400 0.8052296\n#&gt; 3 0.79978857 0.06978539 0.2641693 3.0275612 0.002465358  0.2820263 1.3175508\n#&gt; 4 0.76161294 0.07254288 0.2693378 2.8277234 0.004688029  0.2337205 1.2895054\n#&gt; 5 0.03975903 0.05609166 0.2368368 0.1678752 0.866681425 -0.4244325 0.5039506\n#&gt; 6 0.24724290 0.05087508 0.2255551 1.0961532 0.273011731 -0.1948369 0.6893227"
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#example-with-simulated-data-1",
    "href": "slides/02-replication-methods/02-replication-methods.html#example-with-simulated-data-1",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Example with simulated data",
    "text": "Example with simulated data\nLet‚Äôs compute the proportions of replication studies are statistically significant:\n\nmean(reps$pval &lt;= 0.05)\n#&gt; [1] 0.5\n\n\nLet‚Äôs compute the proportions of replication studies with the same sign as the original:\n\nmean(sign(orig$yi) == sign(reps$yi))\n#&gt; [1] 1\n\n\nWe could also perform some statistical tests. See Bushman and Wang (2009) and Hedges and Olkin (1980) for vote-counting methods in meta-analysis."
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#vote-counting-extreme-example",
    "href": "slides/02-replication-methods/02-replication-methods.html#vote-counting-extreme-example",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Vote Counting, extreme example",
    "text": "Vote Counting, extreme example\nLet‚Äôs imagine an original experiment with \\(n_{orig} = 30\\) and \\(\\hat \\theta_{orig} = 0.5\\) that is statistically significant \\(p \\approx 0.045\\). Now a direct replication (thus assuming \\(\\tau^2 = 0\\)) study with \\(n_{rep} = 350\\) found \\(\\hat \\theta_{rep_1} = 0.15\\), that is statistically significant \\(p\\approx 0.047\\)."
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#confidence-interval-replication-within-original",
    "href": "slides/02-replication-methods/02-replication-methods.html#confidence-interval-replication-within-original",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Confidence Interval, replication within original",
    "text": "Confidence Interval, replication within original\n\n\nTheory\nPlot\n\n\n\nAnother approach check if the replication attempt \\(\\theta_{rep}\\) is contained in the % confidence interval of the original study \\(\\theta_{orig}\\). Formally:\n\\[\n\\theta_{orig} - \\Phi(\\alpha/2) \\sqrt{\\sigma^2_{orig}} &lt; \\theta_{rep} &lt; \\theta_{orig} + \\Phi(\\alpha/2) \\sqrt{\\sigma^2_{orig}}\n\\]\nWhere \\(\\Phi\\) is the cumulative standard normal distribution, \\(\\alpha\\) is the type-1 error rate.\n\n\nTake into account the size of the effect and the precision of \\(\\theta_{orig}\\)\n\n\n\n\n\nThe original study is assumed to be a reliable estimation\nNo extension for many-to-one designs\nLow precise original studies lead to higher success rate\n\n\n\n\n\nCodese_orig &lt;- sqrt(4 / (2 * n_orig))\nci_orig &lt;- theta_orig + qnorm(c(0.025, 0.975)) * se_orig\ncurve(dnorm(x, theta_orig, se_orig), \n      -1, 2, \n      ylab = \"Density\", \n      xlab = latex2exp::TeX(\"$\\\\theta$\"))\nabline(v = ci_orig, lty = \"dashed\")\npoints(theta_orig, 0, pch = 19, cex = 2)\npoints(theta_rep, 0, pch = 19, cex = 2, col = \"firebrick\")\nlegend(\"topleft\", \n       legend = c(\"Original\", \"Replication\"), \n       fill = c(\"black\", \"firebrick\"))"
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#confidence-interval-replication-within-original-1",
    "href": "slides/02-replication-methods/02-replication-methods.html#confidence-interval-replication-within-original-1",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Confidence Interval, replication within original",
    "text": "Confidence Interval, replication within original\nOne potential problem of this method regards that low precise original studies are ‚Äúeasier‚Äù to replicate due to larger confidence intervals."
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#confidence-interval-original-within-replication",
    "href": "slides/02-replication-methods/02-replication-methods.html#confidence-interval-original-within-replication",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Confidence Interval, original within replication",
    "text": "Confidence Interval, original within replication\n\n\nTheory\nPlot\n\n\n\nThe same approach can be applied checking if the original effect size is contained within the replication confidence interval. Clearly these methods depends on the precision of studies. Formally:\n\\[\n\\theta_{rep} - \\Phi(\\alpha/2) \\sqrt{\\sigma_{rep}^2} &lt; \\theta_{orig} &lt; \\theta_{rep} + \\Phi(\\alpha/2) \\sqrt{\\sigma_{rep}^2}\n\\]\nThe method has the same pros and cons of the previous approach. One advantage is that usually replication studies are more precise (higher sample size) thus the parameter and the % CI is more reliable."
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#prediction-interval-pi-what-to-expect-from-a-replication",
    "href": "slides/02-replication-methods/02-replication-methods.html#prediction-interval-pi-what-to-expect-from-a-replication",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Prediction interval (PI), what to expect from a replication",
    "text": "Prediction interval (PI), what to expect from a replication\nOne problem of the previous approaches is taking into account only the uncertainty of the original or the replication study. Patil, Peng, and Leek (2016) and Spence and Stanley (2016) proposed a method to take into account both sources of uncertainty.\nIf the original and replication studies comes from the same population, the sampling distribution of the difference is centered on 0 with a certain standard error \\(\\theta_{orig} - \\theta_{rep_0} \\sim \\mathcal{N}\\left( 0, \\sqrt{\\sigma^2_{\\hat \\theta_{orig} - \\hat \\theta_{rep}}} \\right)\\) (subscript \\(0\\) to indicate that is expected to be sampled from the same population as \\(\\theta_{orig}\\))\n\\[\n\\hat \\theta_{orig} \\pm z_{95\\%} \\sqrt{\\sigma^2_{\\theta_{orig} - \\theta_{rep}}}\n\\]\nIf factors other than standard error influence the replication result, \\(\\theta_{rep_0}\\) is not expected to be contained within the 95% prediction interval."
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#prediction-interval-pi-what-to-expect-from-a-replication-1",
    "href": "slides/02-replication-methods/02-replication-methods.html#prediction-interval-pi-what-to-expect-from-a-replication-1",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Prediction interval (PI), what to expect from a replication",
    "text": "Prediction interval (PI), what to expect from a replication\nIn the case of a (un)standardized mean difference we can compute the prediction interval as:\n\\[\n\\sqrt{\\sigma^2_{\\epsilon_{\\hat \\theta_{orig} - \\hat \\theta_{rep_0}}}} = \\sqrt{\\left( \\frac{\\hat \\sigma^2_{o1}}{n_{o1}} +\\frac{\\hat \\sigma^2_{o2}}{n_{o2}}\\right) + \\left(\\frac{\\hat \\sigma^2_{o1}}{n_{r1}} + \\frac{\\hat\\sigma^2_{o2}}{n_{r2}}\\right)}\n\\]\nThe first term is just the standard error of the difference between the two groups in the original study and the second term is the standard error of the hypothetical replication study assuming the same standard deviation of the original but a different \\(n\\).\nIn this way we estimate an interval where, combining sampling variance from both studies and assuming that they comes from the same population, the replication should fall."
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#prediction-interval-pi-what-to-expect-from-a-replication-2",
    "href": "slides/02-replication-methods/02-replication-methods.html#prediction-interval-pi-what-to-expect-from-a-replication-2",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Prediction interval (PI), what to expect from a replication",
    "text": "Prediction interval (PI), what to expect from a replication\n\n\nR Code\nPlot\nPros/Cons\n\n\n\n\nset.seed(2025)\n\no1 &lt;- rnorm(50, 0.5, 1) # group 1\no2 &lt;- rnorm(50, 0, 1) # group 2\nod &lt;- mean(o1) - mean(o2) # effect size\nse_o &lt;- sqrt(var(o1)/50 + var(o2)/50) # standard error of the difference\n\nn_r &lt;- 100 # sample size replication\n\nse_o_r &lt;- sqrt(se_o^2 + (var(o1)/100 + var(o2)/100))\n\nod + qnorm(c(0.025, 0.975)) * se_o_r\n#&gt; [1] 0.325520 1.298378\n\n\n\n\n\nCodepar(mar = c(4, 4, 0.1, 0.1))  \ncurve(dnorm(x, od, se_o_r), od - se_o_r*4, od + se_o_r*4, lwd = 2, xlab = latex2exp::TeX(\"$\\\\theta$\"), ylab = \"Density\")\nabline(v = od + qnorm(c(0.025, 0.975)) * se_o_r, lty = \"dashed\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n\n\n\n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n\n  \n\n\n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTake into account uncertainty of both studies\nWe can plan a replication using the standard deviation of the original study and the expected sample size\n\n\n\n\nLow precise original studies lead to wide PI. For a replication study is difficult to fall outside the PI\nMainly for one-to-one replications design"
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#mathur-vanderweele--mathur2020-nw-p_orig",
    "href": "slides/02-replication-methods/02-replication-methods.html#mathur-vanderweele--mathur2020-nw-p_orig",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Mathur & VanderWeele (2020) \\(p_{orig}\\)\n",
    "text": "Mathur & VanderWeele (2020) \\(p_{orig}\\)\n\n\n\nTheory\nPros-cons\nR Code\nSimulation\n\n\n\nMathur & VanderWeele (2020) proposed a new method based on the prediction interval to calculate a p value \\(p_{orig}\\) representing the probability that \\(\\theta_{orig}\\) is consistent with the replications. This method is suited for many-to-one replication designs. Formally:\n\\[\nP_{orig} = 2 \\left[ 1 - \\Phi \\left( \\frac{|\\hat \\theta_{orig} - \\hat \\mu_{\\theta_{rep}}|}{\\sqrt{\\hat \\tau^2 + \\sigma^2_{orig} + \\hat{SE}^2_{\\hat \\mu_{\\theta_{rep}}}}} \\right) \\right]\n\\]\n\n\n\\(\\mu_{\\theta_{rep}}\\) is the pooled (i.e., meta-analytic) estimation of the \\(k\\) replications\n\n\\(\\tau^2\\) is the variance among replications\n\n\n\nIt is interpreted as the probability that \\(\\theta_{orig}\\) is equal or more extreme that what observed. A very low \\(p_{orig}\\) suggest that the original study is inconsistent with replications.\n\n\nSuited for many-to-one designs\nWe take into account all sources of uncertainty\nWe have a p-value\n\n\n\n\nThe code is implemented in the Replicate and MetaUtility R packages:\n\ntau2 &lt;- 0.05\ntheta_rep &lt;- 0.2\ntheta_orig &lt;- 0.7\n\nn_orig &lt;- 30\nn_rep &lt;- 100\nk &lt;- 20\n\nreplications &lt;- sim_studies(k, theta_rep, tau2, n_rep, n_rep)\noriginal &lt;- sim_studies(1, theta_orig, 0, n_orig, n_orig)\n\nfit_rep &lt;- metafor::rma(yi, vi, data = replications) # random-effects meta-analysis\n\nReplicate::p_orig(original$yi, original$vi, fit_rep$b[[1]], fit_rep$tau2, fit_rep$se^2)\n#&gt; [1] 0.5563241\n\n\n\n\n\nCode# standard errors assuming same n and variance 1\nse_orig &lt;- sqrt(4/(n_orig * 2))\nse_rep &lt;- sqrt(4/(n_rep * 2))\nse_theta_rep &lt;- sqrt(1/((1/(se_rep^2 + tau2)) * k)) # standard error of the random-effects estimate\n\nsep &lt;- sqrt(tau2 + se_orig^2 + se_theta_rep^2) # z of p-orig denominator\n\ncurve(dnorm(x, theta_rep, sep), theta_rep - 4*sep, theta_rep + 4*sep, ylab = \"Density\", xlab = latex2exp::TeX(\"\\\\theta\"))\npoints(theta_orig, 0.02, pch = 19, cex = 2)\nabline(v = qnorm(c(0.025, 0.975), theta_rep, sep), lty = \"dashed\", col = \"firebrick\")"
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#mathur-vanderweele--mathur2020-nw-hat-p_-0",
    "href": "slides/02-replication-methods/02-replication-methods.html#mathur-vanderweele--mathur2020-nw-hat-p_-0",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Mathur & VanderWeele (2020) \\(\\hat P_{> 0}\\)\n",
    "text": "Mathur & VanderWeele (2020) \\(\\hat P_{&gt; 0}\\)\n\n\n\nTheory\nR Code\nBootstrap Code\nBootstrap Results\n\n\n\nAnother related metric is the \\(\\hat P_{&gt; 0}\\), representing the proportion of replications following the same direction as the original effect. Before simply computing the proportions we need to adjust the estimated \\(\\theta_{rep_i}\\) with a shrinkage factor:\n\\[\n\\tilde{\\theta}_{rep_i} = (\\theta_{rep_i} - \\mu_{\\theta_{rep_i}}) \\sqrt{\\frac{\\hat \\tau^2}{\\hat \\tau^2 + v_{rep_i}}}\n\\]\nThis method is somehow similar to the vote counting but we are adjusting the effects taking into account \\(\\tau^2\\).\n\n\n\n# compute calibrated estimation for the replications\n# use restricted maximum likelihood to estimate tau2 under the hood\ntheta_sh &lt;- MetaUtility::calib_ests(replications$yi, replications$sei, method = \"REML\")\nmean(theta_sh &gt; 0)\n#&gt; [1] 0.75\n\n\n\n\nThe authors suggest a bootstrapping approach for making inference on \\(\\hat P_{&gt; 0}\\)\n\nnboot &lt;- 1e4\ntheta_boot &lt;- matrix(0, nrow = nboot, ncol = k)\n\nfor(i in 1:nboot){\n  idx &lt;- sample(1:nrow(replications), nrow(replications), replace = TRUE)\n  replications_boot &lt;- replications[idx, ]\n  theta_cal &lt;- MetaUtility::calib_ests(replications_boot$yi, \n                                       replications_boot$sei, \n                                       method = \"REML\")\n  theta_boot[i, ] &lt;- theta_cal\n}\n\n# calculate\np_greater_boot &lt;- apply(theta_boot, 1, function(x) mean(x &gt; 0))"
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#mathur-vanderweele--mathur2020-nw-hat-p_gtrless-q",
    "href": "slides/02-replication-methods/02-replication-methods.html#mathur-vanderweele--mathur2020-nw-hat-p_gtrless-q",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Mathur & VanderWeele (2020) \\(\\hat P_{\\gtrless q*}\\)\n",
    "text": "Mathur & VanderWeele (2020) \\(\\hat P_{\\gtrless q*}\\)\n\nInstead of using 0 as threshold, we can use meaningful effect size to be considered as low but different from 0. \\(\\hat P_{\\gtrless q*}\\) is the proportion of (calibrated) replications greater or lower than the \\(q*\\) value. This framework is similar to equivalence and minimum effect size testing (Lakens, Scheel, and Isager 2018).\n\nq &lt;- 0.2 # minimum non zero effect\n\nfit &lt;- metafor::rma(yi, vi, data = replications)\n\n# see ?MetaUtility::prop_stronger\nMetaUtility::prop_stronger(q = q,\n                           M = fit$b[[1]],\n                           t2 = fit$tau2,\n                           tail = \"above\",\n                           estimate.method = \"calibrated\",\n                           ci.method = \"calibrated\",\n                           dat = replications,\n                           yi.name = \"yi\",\n                           vi.name = \"vi\")\n#&gt;    est        se  lo   hi  bt.mn shapiro.pval\n#&gt; 1 0.35 0.1137901 0.1 0.55 0.3573    0.5224829"
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#combining-original-and-replications",
    "href": "slides/02-replication-methods/02-replication-methods.html#combining-original-and-replications",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Combining original and replications",
    "text": "Combining original and replications\n\n\nTheory\nFixed-effects Model\nRandom-Effects model\nPooling replications\n\n\n\nAnother approach is to combine the original and replication results (both one-to-one and many-to-one) using a meta-analysis model. Then we can test if the pooled estimate is different from 0 or another meaningful value.\n\n\nUse all the available information, especially when fitting a random-effects model\nTake into account the precision by inverse-variance weighting\n\n\n\n\nDid not consider the publication bias\nFor one-to-one designs only a fixed-effects model can be used\n\n\n\n\n\nCode# fixed-effects\nfit_fixed &lt;- rma(yi, vi, method = \"FE\")\nsummary(fit_fixed)\n#&gt; \n#&gt; Fixed-Effects Model (k = 20)\n#&gt; \n#&gt;   logLik  deviance       AIC       BIC      AICc   \n#&gt;  20.7415   -0.0000  -39.4829  -38.4872  -39.2607   \n#&gt; \n#&gt; I^2 (total heterogeneity / total variability):   0.00%\n#&gt; H^2 (total variability / sampling variability):  0.00\n#&gt; \n#&gt; Test for Heterogeneity:\n#&gt; Q(df = 19) = 0.0000, p-val = 1.0000\n#&gt; \n#&gt; Model Results:\n#&gt; \n#&gt; estimate      se    zval    pval   ci.lb   ci.ub      \n#&gt;   0.2000  0.0316  6.3246  &lt;.0001&gt; \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nCode# fixed-effects\nfit_random &lt;- rma(yi, vi, method = \"REML\")\nsummary(fit_random)\n#&gt; \n#&gt; Random-Effects Model (k = 20; tau^2 estimator: REML)\n#&gt; \n#&gt;   logLik  deviance       AIC       BIC      AICc   \n#&gt;  19.7044  -39.4088  -35.4088  -33.5199  -34.6588   \n#&gt; \n#&gt; tau^2 (estimated amount of total heterogeneity): 0 (SE = 0.0065)\n#&gt; tau (square root of estimated tau^2 value):      0\n#&gt; I^2 (total heterogeneity / total variability):   0.00%\n#&gt; H^2 (total variability / sampling variability):  1.00\n#&gt; \n#&gt; Test for Heterogeneity:\n#&gt; Q(df = 19) = 0.0000, p-val = 1.0000\n#&gt; \n#&gt; Model Results:\n#&gt; \n#&gt; estimate      se    zval    pval   ci.lb   ci.ub      \n#&gt;   0.2000  0.0316  6.3246  &lt;.0001&gt; \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nThe previous approach can be also implemented combining replications into a single effect and then compare the original with the combined replication study.\nThis is similar to using the CI or PI approaches but the replication effect will probably by very precise due to pooling multiple studies."
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#q-statistics",
    "href": "slides/02-replication-methods/02-replication-methods.html#q-statistics",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Q Statistics",
    "text": "Q Statistics\nAn interesting proposal is using the Q statistics (Hedges and Schauer 2019a, 2019c, 2021, 2019b; Jacob M. Schauer 2022; J. M. Schauer and Hedges 2021; Jacob M. Schauer and Hedges 2020), commonly used in meta-analysis to assess the presence of heterogeneity. Formally:\n\\[\nQ = \\sum_{i = 1}^{k} \\frac{(\\theta_i - \\bar \\theta_w)^2}{\\sigma^2_i}\n\\]\nWhere \\(\\bar \\theta_w\\) is the inverse-variance weighted average (i.g., fixed-effect model). The Q statistics is essentially a weighted sum of squares. Under the null hypothesis where all studies are equal \\(\\theta_1 = \\theta_2, ... = \\theta_i\\) the Q statistics has a \\(\\chi^2\\) distribution with \\(k - 1\\) degrees of freedom. Under the alternative hypothesis the distribution is a non-central \\(\\chi^2\\) with non centrality parameter \\(\\lambda\\). The expected value of the \\(Q\\) is \\(E(Q) = v + \\lambda\\), where \\(v\\) are the degrees of freedom."
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#q-statistics-1",
    "href": "slides/02-replication-methods/02-replication-methods.html#q-statistics-1",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Q Statistics",
    "text": "Q Statistics\nHedges & Schauer proposed to use the Q statistics to evaluate the consistency of a series of replications:\n\nIn case of exact replication, \\(\\lambda = 0\\) because \\(\\theta_1 = \\theta_2, ... = \\theta_k\\).\nIn case of approximate replication, \\(\\lambda &lt; \\lambda_0\\) where \\(\\lambda_0\\) is the maximum value considered as equal to null (i.e., 0).\n\nThis approach is testing the consistency (i.e., homogeneity) of replications. A successful replication should minimize the heterogeneity and the presence of a significant Q statistics should bring evidence for not replicating the effect1.\nThe approach has been debated by a series of opinion papers (see Hedges and Schauer 2019a; Mathur and VanderWeele 2019)"
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#q-statistics-2",
    "href": "slides/02-replication-methods/02-replication-methods.html#q-statistics-2",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Q Statistics",
    "text": "Q Statistics\nThe method has been expanded and formalized in several papers with different objectives:\n\n\nto cover different replications setup (burden of proof on replicating vs non-replicating, many-to-one and one-to-one, etc.)\n\n\n\n\ninterpret and choose the \\(\\lambda\\) parameter given that is the core of the approach\n\n\n\n\nevaluating the power and statistical properties under different replication scenarios\n\n\n\n\nthe standard implementation put the burden of proof on non-replication. Thus \\(H_0\\) is that studies replicates. They provided also a series of tests with the opposite formulation."
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#q-statistics-3",
    "href": "slides/02-replication-methods/02-replication-methods.html#q-statistics-3",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Q Statistics",
    "text": "Q Statistics\nIn the case of evaluating an exact replication we can use the Qrep() function that simply calculate the p-value based on the Q sampling distribution.\n\n\nFunction\nCode\nPlot\n\n\n\n\nQrep &lt;- function(yi, vi, lambda0 = 0, alpha = 0.05){\n  fit &lt;- metafor::rma(yi, vi)\n  k &lt;- fit$k\n  Q &lt;- fit$QE\n  df &lt;- k - 1\n  Qp &lt;- pchisq(Q, df = df, ncp = lambda0, lower.tail = FALSE)\n  pval &lt;- ifelse(Qp &lt; 0.001, \"p &lt; 0.001\", sprintf(\"p = %.3f\", Qp))\n  lambda &lt;- ifelse((Q - df) &lt; 0, 0, (Q - df))\n  res &lt;- list(Q = Q, lambda = lambda, pval = Qp, df = df, k = k, alpha = alpha, lambda0 = lambda0)\n  H0 &lt;- ifelse(lambda0 != 0, paste(\"H0: lambda &lt;\", lambda0), \"H0: lambda = 0\")\n  title &lt;- ifelse(lambda0 != 0, \"Q test for Approximate Replication\", \"Q test for Exact Replication\")\n  cli::cli_rule()\n  cat(cli::col_blue(cli::style_bold(title)), \"\\n\\n\")\n  cat(sprintf(\"Q = %.3f (df = %s), lambda = %.3f, %s\", res$Q, res$df, lambda, pval), \"\\n\")\n  cat(H0, \"\\n\")\n  cli::cli_rule()\n  class(res) &lt;- \"Qrep\"\n  invisible(res)\n}\n\n\n\n\n#&gt; Q test for Exact Replication \n#&gt; \n#&gt; Q = 367.321 (df = 99), lambda = 268.321, p  H0: lambda = 0\n\n\n\nQres &lt;- Qrep(dat$yi, dat$vi)\n#&gt; Q test for Exact Replication \n#&gt; \n#&gt; Q = 367.321 (df = 99), lambda = 268.321, p  H0: lambda = 0\n\n\n\n\n\nCodeplot.Qrep(Qres)"
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#q-statistics-for-approximate-replication",
    "href": "slides/02-replication-methods/02-replication-methods.html#q-statistics-for-approximate-replication",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Q Statistics for approximate replication",
    "text": "Q Statistics for approximate replication\n\nIn case of approximate replication we need to set \\(\\lambda_0\\) to a meaningful value but the overall test is the same. The critical \\(Q\\) is no longer evaluated with a central \\(\\chi^2\\) but a non-central \\(\\chi^2\\) with \\(\\lambda_0\\) as non-centrality parameter.\nHedges and Schauer (2019b) provide different strategies to choose \\(\\lambda_0\\). They found that under some assumptions, \\(\\lambda = (k - 1) \\frac{\\tau^2}{\\tilde{v}}\\)\nGiven that we introduced the \\(I^2\\) statistics we can derive a \\(\\lambda_0\\) based in \\(I^2\\). F. L. Schmidt and Hunter (2014) proposed that when \\(\\tilde{v}\\) is at least 75% of total variance \\(\\tilde{v} + \\tau^2\\) thus \\(\\tau^2\\) could be considered neglegible. This corresponds to a \\(I^2 = 25%\\) and a ratio \\(\\frac{\\tau^2}{\\tilde{v}} = 1/3\\) thus \\(\\lambda_0 = \\frac{(k - 1)}{3}\\) can be considered a neglegible heterogeneity"
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#q-statistics-for-approximate-replication-1",
    "href": "slides/02-replication-methods/02-replication-methods.html#q-statistics-for-approximate-replication-1",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Q Statistics for approximate replication",
    "text": "Q Statistics for approximate replication\n\nk &lt;- 100\ndat &lt;- sim_studies(k, 0.5, 0, 50, 50)\nQrep(dat$yi, dat$vi, lambda0 = (k - 1)/3)\n#&gt; Q test for Approximate Replication \n#&gt; \n#&gt; Q = 98.121 (df = 99), lambda = 0.000, p = 0.977 \n#&gt; H0: lambda"
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#small-telescopes-simonsohn2015-kg",
    "href": "slides/02-replication-methods/02-replication-methods.html#small-telescopes-simonsohn2015-kg",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Small Telescopes (Simonsohn 2015)\n",
    "text": "Small Telescopes (Simonsohn 2015)\n\nSimonsohn (2015) introduced 3 main questions when evaluating replicability:\n\n\nWhen we combine data from the original and replication study, what is our best guess of the overall effect?\n\n\n\nmeta-analysis\n\n\n\nIs the effect of the replication study different from the original study?\n\n\n\nmeta-analysis and standard tests, but problematic in terms of statistical power\n\n\n\nDoes the replication study suggest that the effect of interest is undetectable different from zero?\n\n\n\nsmall telescopes"
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#small-telescopes-simonsohn2015-kg-1",
    "href": "slides/02-replication-methods/02-replication-methods.html#small-telescopes-simonsohn2015-kg-1",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Small Telescopes (Simonsohn 2015)\n",
    "text": "Small Telescopes (Simonsohn 2015)\n\nThe idea is simple but quite powerful and insightful. Let‚Äôs assume that an original study found an effect of \\(y_{orig} = 0.7\\) on a two-sample design with \\(n = 20\\) per group.\n\n\nwe define a threshold as the effect size that is associated with a certain low power level e.g., \\(33\\%\\) given the sample size i.e.¬†\\(\\theta_{small} = 0.5\\)\n\nthe replication study found an effect of \\(y_{rep} = 0.2\\) with \\(n = 100\\) subjects\n\n\n\nIf the \\(y_{rep}\\) is lower (i.e., the upper bound of the confidence interval) than the small effect (\\(\\theta_{small} = 0.5\\)) we conclude that the effect is probably so tiny that could not have been detected by the original study. Thus there is no evidence for a replication."
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#small-telescopes-simonsohn2015-kg-2",
    "href": "slides/02-replication-methods/02-replication-methods.html#small-telescopes-simonsohn2015-kg-2",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Small Telescopes (Simonsohn 2015)\n",
    "text": "Small Telescopes (Simonsohn 2015)\n\nWe can use the custom small_telescope() function on simulated data:\n\nsmall_telescope &lt;- function(or_d,\n                            or_se,\n                            rep_d,\n                            rep_se,\n                            small,\n                            ci = 0.95){\n  # quantile for the ci\n  qs &lt;- c((1 - ci)/2, 1 - (1 - ci)/2)\n  \n  # original confidence interval\n  or_ci &lt;- or_d + qnorm(qs) * or_se\n  \n  # replication confidence interval\n  rep_ci &lt;- rep_d + qnorm(qs) * rep_se\n  \n  # small power\n  is_replicated &lt;- rep_ci[2] &gt; small\n  \n  msg_original &lt;- sprintf(\"Original Study: d = %.3f %s CI = [%.3f, %.3f]\",\n                          or_d, ci, or_ci[1], or_ci[2])\n  \n  msg_replicated &lt;- sprintf(\"Replication Study: d = %.3f %s CI = [%.3f, %.3f]\",\n                            rep_d, ci, rep_ci[1], rep_ci[2])\n  \n  \n  if(is_replicated){\n    msg_res &lt;- sprintf(\"The replicated effect is not smaller than the small effect (%.3f), (probably) replication!\", small)\n    msg_res &lt;- cli::col_green(msg_res)\n  }else{\n    msg_res &lt;- sprintf(\"The replicated effect is smaller than the small effect (%.3f), no replication!\", small)\n    msg_res &lt;- cli::col_red(msg_res)\n  }\n  \n  out &lt;- data.frame(id = c(\"original\", \"replication\"),\n                    d = c(or_d, rep_d),\n                    lower = c(or_ci[1], rep_ci[1]),\n                    upper = c(or_ci[2], rep_ci[2]),\n                    small = small\n  )\n  \n  # nice message\n  cat(\n    msg_original,\n    msg_replicated,\n    cli::rule(),\n    msg_res,\n    sep = \"\\n\"\n  )\n  \n  invisible(out)\n  \n}"
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#small-telescopes-simonsohn2015-kg-3",
    "href": "slides/02-replication-methods/02-replication-methods.html#small-telescopes-simonsohn2015-kg-3",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Small Telescopes (Simonsohn 2015)\n",
    "text": "Small Telescopes (Simonsohn 2015)\n\n\nset.seed(2025)\n\nd &lt;- 0.2 # real effect\n\n# original study\nor_n &lt;- 20\nor_d &lt;- 0.7\nor_se &lt;- sqrt(1/20 + 1/20)\nd_small &lt;- pwr::pwr.t.test(or_n, power = 0.33)$d\n\n# replication\nrep_n &lt;- 100 # sample size of replication study\ng0 &lt;- rnorm(rep_n, 0, 1)\ng1 &lt;- rnorm(rep_n, d, 1)\n\nrep_d &lt;- mean(g1) - mean(g0)\nrep_se &lt;- sqrt(var(g1)/rep_n + var(g0)/rep_n)\n\nHere we are using the pwr::pwr.t.test() to compute the effect size \\(\\theta_{small}\\) (in code d) associated with 33% power."
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#small-telescopes-simonsohn2015-kg-4",
    "href": "slides/02-replication-methods/02-replication-methods.html#small-telescopes-simonsohn2015-kg-4",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Small Telescopes (Simonsohn 2015)\n",
    "text": "Small Telescopes (Simonsohn 2015)\n\n\nCodesmall_telescope(or_d, or_se, rep_d, rep_se, d_small, ci = 0.95)\n#&gt; Original Study: d = 0.700 0.95 CI = [0.080, 1.320]\n#&gt; Replication Study: d = 0.214 0.95 CI = [-0.061, 0.490]\n#&gt; ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#&gt; The replicated effect is smaller than the small effect (0.493), no replication!\n\n\nAnd a (quite over-killed) plot:"
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#bayes-factor",
    "href": "slides/02-replication-methods/02-replication-methods.html#bayes-factor",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Bayes Factor",
    "text": "Bayes Factor\nVerhagen and Wagenmakers (2014) proposed a method to estimate the evidence of a replication study. The core topics to understand the method are:\n\nBayesian hypothesis testing using the Bayes Factor (see, Rouder et al. 2009)\n\nBayes Factor using the Savage-Dickey density ratio (SDR, Wagenmakers et al. 2010)"
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#bayesian-inference",
    "href": "slides/02-replication-methods/02-replication-methods.html#bayesian-inference",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Bayesian inference",
    "text": "Bayesian inference\nBayesian inference is the statistical procedure where prior beliefs about a phenomenon are combined, using the Bayes theorem, with evidence from data to obtain the posterior beliefs.\n\nThe interesting part is that the researcher express the prior beliefs in probabilistic terms. Then after collecting data, evidence from the experiment is combined increasing or decreasing the plausibility of prior beliefs.\n\n\nLet‚Äôs make an (not a very innovative üòÑ) example. We need to evaluate the fairness of a coin. The crucial parameter is \\(\\theta\\) that is the probability of success (e.g., head). We have our prior belief about the coin (e.g., fair but with some uncertainty). We toss the coin \\(k\\) times and we observe \\(x\\) heads. What are my conclusions?"
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#bayesian-inference-1",
    "href": "slides/02-replication-methods/02-replication-methods.html#bayesian-inference-1",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Bayesian inference",
    "text": "Bayesian inference\n\\[\np(\\theta|D) = \\frac{p(D|\\theta) \\; p(\\theta)}{p(D)}\n\\] Where \\(\\theta\\) is our parameter and \\(D\\) the data. \\(p(\\theta|D)\\) is the posterior distribution that is the product between the likelihood \\(p(D|\\theta)\\) and the prior \\(p(\\theta)\\). \\(p(D)\\) is the probability of the data (aka marginal likelihood) and is necessary only for the posterior to be a proper probability distribution.\nWe can ‚Äúread‚Äù the formula as: The probability of the parameter given the data is the product between the likelihood of the data given the parameter and the prior probability of the parameter."
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#bayesian-inference-2",
    "href": "slides/02-replication-methods/02-replication-methods.html#bayesian-inference-2",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Bayesian inference",
    "text": "Bayesian inference\nLet‚Äôs express our prior belief in probabilistic terms:"
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#bayesian-inference-3",
    "href": "slides/02-replication-methods/02-replication-methods.html#bayesian-inference-3",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Bayesian inference",
    "text": "Bayesian inference\nNow we collect data and we observe \\(x = 40\\) tails out of \\(k = 50\\) trials thus \\(\\hat{\\theta} = 0.8\\) and compute the likelihood:"
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#bayesian-inference-4",
    "href": "slides/02-replication-methods/02-replication-methods.html#bayesian-inference-4",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Bayesian inference",
    "text": "Bayesian inference\nFinally we combine, using the Bayes rule, prior and likelihood to obtain the posterior distribution:"
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#bayes-factor-1",
    "href": "slides/02-replication-methods/02-replication-methods.html#bayes-factor-1",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Bayes Factor",
    "text": "Bayes Factor\nThe idea of the Bayes Factor is computing the evidence of the data under two competing hypotheses, \\(H_0\\) and \\(H_1\\) (~ \\(\\theta\\) in our previous example):\n\\[\n\\frac{p(H_0|D)}{p(H_1|D)} = \\frac{f(D|H_0)}{f(D|H_1)} \\times \\frac{p(H_0)}{p(H_1)}\n\\]\nWhere \\(f\\) is the likelihood function, \\(y\\) are the data. The \\(\\frac{p(H_0)}{p(H_1)}\\) is the prior odds of the two hypothesis. The Bayes Factor is the ratio between the likelihood of the data under the two hypotheses."
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#bayes-factor-using-the-sdr",
    "href": "slides/02-replication-methods/02-replication-methods.html#bayes-factor-using-the-sdr",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Bayes Factor using the SDR",
    "text": "Bayes Factor using the SDR\nCalculating the BF can be problematic in some condition. The SDR is a convenient shortcut to calculate the Bayes Factor (Wagenmakers et al. 2010). The idea is that the ratio between the prior and posterior density distribution for the \\(H_1\\) is an estimate of the Bayes factor calculated in the standard way.\n\\[\nBF_{01} = \\frac{p(D|H_0)}{p(D|H_1)} = \\frac{p(\\theta = x|D, H_1)}{p(\\theta = x, H_1)}\n\\]\nWhere \\(\\theta\\) is the parameter of interest and \\(x\\) is the null value under \\(H_0\\) e.g., 0. and \\(D\\) are the data."
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#bayes-factor-using-the-sdr-example",
    "href": "slides/02-replication-methods/02-replication-methods.html#bayes-factor-using-the-sdr-example",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Bayes Factor using the SDR, Example:",
    "text": "Bayes Factor using the SDR, Example:\nFollowing the previous example \\(H_0: \\theta = 0.5\\). Under \\(H_1\\) we use a completely uninformative prior by setting \\(\\theta \\sim Beta(1, 1)\\).\nWe flip again the coin 20 times and we found that \\(\\hat \\theta = 0.75\\)."
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#bayes-factor-using-the-sdr-example-1",
    "href": "slides/02-replication-methods/02-replication-methods.html#bayes-factor-using-the-sdr-example-1",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Bayes Factor using the SDR, Example:",
    "text": "Bayes Factor using the SDR, Example:\nThe ratio between the two black dots is the Bayes Factor.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n\n\n\n\n\n\n\n  \n  \n  \n  \n\n\n  \n  \n  \n  \n\n\n  \n  \n  \n  \n\n\n  \n  \n  \n  \n\n\n  \n  \n  \n  \n\n\n  \n  \n  \n  \n\n\n\n  \n\n\n  \n\n\n  \n  \n\n\n  \n  \n\n\n  \n\n\n  \n\n\n  \n  \n\n\n  \n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n  \n  \n  \n\n\n  \n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n  \n  \n\n\n  \n\n\n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n  \n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n\n\n\n\n\n\n\n  \n  \n  \n  \n\n\n  \n  \n  \n  \n\n\n  \n  \n  \n  \n\n\n  \n  \n  \n  \n\n\n  \n  \n  \n  \n\n\n  \n  \n  \n  \n\n\n\n  \n\n\n  \n\n\n  \n  \n\n\n  \n  \n\n\n  \n\n\n  \n\n\n  \n  \n\n\n  \n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n  \n  \n\n\n  \n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n  \n  \n\n\n  \n\n\n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n  \n\n\n  \n\n\n\n\n\nIf the probability density of the null value decrease after seeing data (from prior to posterior) this means that the Bayes factor should favor the alternative hypothesis. On the left, the density of 0.5 is lower after seeing the data ‚Äì&gt; evidence for H1 On the right the density of 0.5 is higher after seeing the data ‚Äì&gt; evidence for H0"
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#verhagen2014-tx-model",
    "href": "slides/02-replication-methods/02-replication-methods.html#verhagen2014-tx-model",
    "title": "Statistical Methods for Replication Assessment",
    "section": "\nVerhagen and Wagenmakers (2014) model1\n",
    "text": "Verhagen and Wagenmakers (2014) model1\n\nThe idea is using the posterior distribution of the original study as prior for a Bayesian hypothesis testing where:\n\n\n\\(H_0: \\theta_{rep} = 0\\) thus there is no effect in the replication study\n\n\\(H_1: \\theta_{rep} \\neq 0\\) and in particular is distributed as \\(\\delta \\sim \\mathcal{N}(\\theta_{orig}, \\sigma^2_{orig})\\) where \\(\\theta_{orig}\\) and \\(\\sigma^2_{orig}\\) are the mean and standard error of the original study\n\nIf \\(H_0\\) is more likely after seeing the data, there is evidence against the replication (i.e., \\(BF_{r0} &gt; 1\\)) otherwise there is evidence for a successful replication (\\(BF_{r1} &gt; 1\\)).\nsee also Ly et al. (2019) for an improvement"
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#verhagen2014-tx-model-1",
    "href": "slides/02-replication-methods/02-replication-methods.html#verhagen2014-tx-model-1",
    "title": "Statistical Methods for Replication Assessment",
    "section": "\nVerhagen and Wagenmakers (2014) model",
    "text": "Verhagen and Wagenmakers (2014) model\n\n\n\n\n\n\nWarning\n\n\nDisclaimer: The actual implementation of Verhagen and Wagenmakers (2014) is different (they use the \\(t\\) statistics). The proposed implementation for the current workshop use a standard linear model."
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#example",
    "href": "slides/02-replication-methods/02-replication-methods.html#example",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Example",
    "text": "Example\nLet‚Äôs assume that the original study (\\(n = 30\\)) estimate a \\(y_{orig} = 0.4\\) and a standard error of \\(\\sigma^2/n\\).\n\n# original study\nn &lt;- 30\nyorig &lt;- 0.4\nse &lt;- sqrt(1/30)\n\n\n\n\n\n\n\nNote\n\n\nThe assumption of Verhagen & Wagenmakers (2014) is that the original study performed a Bayesian analysis with a completely flat prior. Thus the confidence interval is the same as the Bayesian credible interval."
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#example-1",
    "href": "slides/02-replication-methods/02-replication-methods.html#example-1",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Example",
    "text": "Example\nFor this reason, the posterior distribution of the original study can be approximated as:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n\n\n\n\n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n  \n\n\n  \n  \n  \n  \n  \n  \n  \n\n\nWith an uninformative prior the credible interval is the same as the confidence interval"
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#example-2",
    "href": "slides/02-replication-methods/02-replication-methods.html#example-2",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Example",
    "text": "Example\nLet‚Äôs imagine that a new study tried to replicate the original one. They collected \\(n = 100\\) participants with the same protocol and found and effect of \\(y_{rep} = 0.1\\).\n\nCodenrep &lt;- 100\nyrep &lt;- MASS::mvrnorm(nrep, mu = 0.1, Sigma = 1, empirical = TRUE)[, 1]\ndat &lt;- data.frame(y = yrep)\nhist(yrep, main = \"Replication Study (n1 = 100)\", xlab = latex2exp::TeX(\"$y_{rep}$\"))\nabline(v = mean(yrep), lwd = 2, col = \"firebrick\")"
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#example-3",
    "href": "slides/02-replication-methods/02-replication-methods.html#example-3",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Example",
    "text": "Example\nWe can analyze these data with an intercept-only regression model setting as prior the posterior distribution of the original study:\n\nCode# setting the prior on the intercept parameter\nprior &lt;- rstanarm::normal(location = yorig,\n                          scale = se)\n\n# fitting the bayesian linear regression\nfit &lt;- stan_glm(y ~ 1, \n                data = dat, \n                prior_intercept = prior,\n                refresh = FALSE)\n\nsummary(fit)\n#&gt; \n#&gt; Model Info:\n#&gt;  function:     stan_glm\n#&gt;  family:       gaussian [identity]\n#&gt;  formula:      y ~ 1\n#&gt;  algorithm:    sampling\n#&gt;  sample:       4000 (posterior sample size)\n#&gt;  priors:       see help('prior_summary')\n#&gt;  observations: 100\n#&gt;  predictors:   1\n#&gt; \n#&gt; Estimates:\n#&gt;               mean   sd   10%   50%   90%\n#&gt; (Intercept) 0.2    0.1  0.1   0.2   0.3  \n#&gt; sigma       1.0    0.1  0.9   1.0   1.1  \n#&gt; \n#&gt; Fit Diagnostics:\n#&gt;            mean   sd   10%   50%   90%\n#&gt; mean_PPD 0.2    0.1  0.0   0.2   0.3  \n#&gt; \n#&gt; The mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n#&gt; \n#&gt; MCMC diagnostics\n#&gt;               mcse Rhat n_eff\n#&gt; (Intercept)   0.0  1.0  2609 \n#&gt; sigma         0.0  1.0  2608 \n#&gt; mean_PPD      0.0  1.0  3185 \n#&gt; log-posterior 0.0  1.0  1693 \n#&gt; \n#&gt; For each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1)."
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#example-4",
    "href": "slides/02-replication-methods/02-replication-methods.html#example-4",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Example",
    "text": "Example\n\n\nResults\nPlot\n\n\n\nWe can use the bayestestR::bayesfactor_pointnull() to calculate the BF using the Savage-Dickey density ratio.\n\nCodebf &lt;- bayestestR::bayesfactor_pointnull(fit, null = 0)\nprint(bf)\n#&gt; Bayes Factor (Savage-Dickey density ratio) \n#&gt; \n#&gt; Parameter   |    BF\n#&gt; -------------------\n#&gt; (Intercept) | 0.274\n#&gt; \n#&gt; * Evidence Against The Null: 0\n#&gt; \n\n\n\n\n\nCodeplot(bf)"
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#example-5",
    "href": "slides/02-replication-methods/02-replication-methods.html#example-5",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Example",
    "text": "Example\nYou can also use the bf_replication() function:\n\nbf_replication &lt;- function(mu_original,\n                           se_original,\n                           replication){\n  \n  # prior based on the original study\n  prior &lt;- rstanarm::normal(location = mu_original, scale = se_original)\n  \n  # to dataframe\n  replication &lt;- data.frame(y = replication)\n  \n  fit &lt;- rstanarm::stan_glm(y ~ 1,\n                            data = replication,\n                            prior_intercept = prior, \n                            refresh = 0) # avoid printing\n  \n  bf &lt;- bayestestR::bayesfactor_pointnull(fit, null = 0, verbose = FALSE)\n  \n  title &lt;- \"Bayes Factor Replication Rate\"\n  posterior &lt;- \"Posterior Distribution ~ Mean: %.3f, SE: %.3f\"\n  replication &lt;- \"Evidence for replication: %3f (log %.3f)\"\n  non_replication &lt;- \"Evidence for non replication: %3f (log %.3f)\"\n  \n  if(bf$log_BF &gt; 0){\n    replication &lt;- cli::col_green(sprintf(replication, exp(bf$log_BF), bf$log_BF))\n    non_replication &lt;- sprintf(non_replication, 1/exp(bf$log_BF), -bf$log_BF)\n  }else{\n    replication &lt;- sprintf(replication, exp(bf$log_BF), bf$log_BF)\n    non_replication &lt;- cli::col_red(sprintf(non_replication, 1/exp(bf$log_BF), -bf$log_BF))\n  }\n  \n  outlist &lt;- list(\n    fit = fit,\n    bf = bf\n  )\n  \n  cat(\n    cli::col_blue(title),\n    cli::rule(),\n    sprintf(posterior, fit$coefficients, fit$ses),\n    \"\\n\",\n    replication,\n    non_replication,\n    sep = \"\\n\"\n  )\n  \n  invisible(outlist)\n  \n}"
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#example-6",
    "href": "slides/02-replication-methods/02-replication-methods.html#example-6",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Example",
    "text": "Example\n\nbf_replication(mu_original = yorig, se_original = se, replication = yrep)\n#&gt; Bayes Factor Replication Rate\n#&gt; ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#&gt; Posterior Distribution ~ Mean: 0.169, SE: 0.088\n#&gt; \n#&gt; \n#&gt; Evidence for replication: 0.278539 (log -1.278)\n#&gt; Evidence for non replication: 3.590164 (log 1.278)"
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#example-7",
    "href": "slides/02-replication-methods/02-replication-methods.html#example-7",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Example",
    "text": "Example\nA better custom plot:\n\nCodebfplot &lt;- data.frame(\n  prior = rnorm(1e5, yorig, se),\n  posterior = rnorm(1e5, fit$coefficients, fit$ses)\n)\n \nplt &lt;- ggplot() +\n  stat_function(geom = \"line\", \n                aes(color = \"Original Study (Prior)\"),\n                linewidth = 1,\n                alpha = 0.3,\n                fun = dnorm, args = list(mean = yorig, sd = se)) +\n  stat_function(geom = \"line\",\n                linewidth = 1,\n                aes(color = \"Replication Study (Posterior)\"),\n                fun = dnorm, args = list(mean = fit$coefficients, sd = fit$ses)) +\n  xlim(c(-0.5, 1.2)) +\n  geom_point(aes(x = c(0, 0), y = c(dnorm(0, yorig, sd = se),\n                                    dnorm(0, fit$coefficients, sd = fit$ses))),\n             size = 3) +\n  xlab(latex2exp::TeX(\"\\\\delta\")) +\n  ylab(\"Density\") +\n  theme(legend.position = \"bottom\",\n        legend.title = element_blank())"
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#example-8",
    "href": "slides/02-replication-methods/02-replication-methods.html#example-8",
    "title": "Statistical Methods for Replication Assessment",
    "section": "Example",
    "text": "Example\nA better custom plot:"
  },
  {
    "objectID": "slides/02-replication-methods/02-replication-methods.html#references",
    "href": "slides/02-replication-methods/02-replication-methods.html#references",
    "title": "Statistical Methods for Replication Assessment",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\nBrandt, Mark J, Hans IJzerman, Ap Dijksterhuis, Frank J Farach, Jason Geller, Roger Giner-Sorolla, James A Grange, Marco Perugini, Jeffrey R Spies, and Anna van ‚Äôt Veer. 2014. ‚ÄúThe Replication Recipe: What Makes for a Convincing Replication?‚Äù Journal of Experimental Social Psychology 50 (January): 217‚Äì24. https://doi.org/10.1016/j.jesp.2013.10.005.\n\n\nBushman, B J, and Morgan C Wang. 2009. ‚ÄúVote-Counting Procedures in Meta-Analysis.‚Äù In The Handbook of Research Synthesis and Meta-Analysis, 207‚Äì20. New York, NY: Russell Sage Foundation.\n\n\nHedges, Larry V, and Ingram Olkin. 1980. ‚ÄúVote-Counting Methods in Research Synthesis.‚Äù Psychological Bulletin 88 (September): 359‚Äì69. https://doi.org/10.1037/0033-2909.88.2.359.\n\n\nHedges, Larry V, and Jacob M Schauer. 2019a. ‚ÄúConsistency of Effects Is Important in Replication: Rejoinder to Mathur and VanderWeele (2019).‚Äù Psychological Methods 24 (October): 576‚Äì77. https://doi.org/10.1037/met0000237.\n\n\n‚Äî‚Äî‚Äî. 2019b. ‚ÄúStatistical Analyses for Studying Replication: Meta-Analytic Perspectives.‚Äù Psychological Methods 24 (October): 557‚Äì70. https://doi.org/10.1037/met0000189.\n\n\n‚Äî‚Äî‚Äî. 2019c. ‚ÄúMore Than One Replication Study Is Needed for Unambiguous Tests of Replication.‚Äù Journal of Educational and Behavioral Statistics: A Quarterly Publication Sponsored by the American Educational Research Association and the American Statistical Association 44 (October): 543‚Äì70. https://doi.org/10.3102/1076998619852953.\n\n\n‚Äî‚Äî‚Äî. 2021. ‚ÄúThe Design of Replication Studies.‚Äù Journal of the Royal Statistical Society. Series A, 184 (March): 868‚Äì86. https://doi.org/10.1111/rssa.12688.\n\n\nJeffreys, Harold. 1973. Scientific Inference. Cambridge, England: Cambridge University Press.\n\n\nLakens, Dani√´l, Anne M Scheel, and Peder M Isager. 2018. ‚ÄúEquivalence Testing for Psychological Research: A Tutorial.‚Äù Advances in Methods and Practices in Psychological Science 1 (June): 259‚Äì69. https://doi.org/10.1177/2515245918770963.\n\n\nLy, Alexander, Alexander Etz, Maarten Marsman, and Eric-Jan Wagenmakers. 2019. ‚ÄúReplication Bayes Factors from Evidence Updating.‚Äù Behavior Research Methods 51 (December): 2498‚Äì508. https://doi.org/10.3758/s13428-018-1092-x.\n\n\nMathur, Maya B, and Tyler J VanderWeele. 2019. ‚ÄúChallenges and Suggestions for Defining Replication \"Success\" When Effects May Be Heterogeneous: Comment on Hedges and Schauer (2019).‚Äù Psychological Methods 24 (October): 571‚Äì75. https://doi.org/10.1037/met0000223.\n\n\n‚Äî‚Äî‚Äî. 2020. ‚ÄúNew Statistical Metrics for Multisite Replication Projects.‚Äù Journal of the Royal Statistical Society. Series A, (Statistics in Society) 183 (June): 1145‚Äì66. https://doi.org/10.1111/rssa.12572.\n\n\nNosek, Brian A, and Timothy M Errington. 2020. ‚ÄúWhat Is Replication?‚Äù PLoS Biology 18 (March): e3000691. https://doi.org/10.1371/journal.pbio.3000691.\n\n\nPatil, Prasad, Roger D Peng, and Jeffrey T Leek. 2016. ‚ÄúWhat Should Researchers Expect When They Replicate Studies? A Statistical View of Replicability in Psychological Science.‚Äù Perspectives on Psychological Science: A Journal of the Association for Psychological Science 11 (July): 539‚Äì44. https://doi.org/10.1177/1745691616646366.\n\n\nRouder, Jeffrey N, Paul L Speckman, Dongchu Sun, Richard D Morey, and Geoffrey Iverson. 2009. ‚ÄúBayesian t Tests for Accepting and Rejecting the Null Hypothesis.‚Äù Psychonomic Bulletin & Review 16 (April): 225‚Äì37. https://doi.org/10.3758/PBR.16.2.225.\n\n\nSchauer, J M, and L V Hedges. 2021. ‚ÄúReconsidering Statistical Methods for Assessing Replication.‚Äù Psychological Methods 26 (February): 127‚Äì39. https://doi.org/10.1037/met0000302.\n\n\nSchauer, Jacob M. 2022. ‚ÄúReplicability and Meta-Analysis.‚Äù In Avoiding Questionable Research Practices in Applied Psychology, edited by William O‚ÄôDonohue, Akihiko Masuda, and Scott Lilienfeld, 301‚Äì42. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-031-04968-2_14.\n\n\nSchauer, Jacob M, and Larry V Hedges. 2020. ‚ÄúAssessing Heterogeneity and Power in Replications of Psychological Experiments.‚Äù Psychological Bulletin 146 (August): 701‚Äì19. https://doi.org/10.1037/bul0000232.\n\n\nSchmidt, Frank L, and John E Hunter. 2014. Methods of Meta-Analysis: Correcting Error and Bias in Research Findings. 3rd ed. Thousand Oaks, CA: SAGE Publications. https://doi.org/10.4135/9781483398105.\n\n\nSchmidt, Stefan. 2009. ‚ÄúShall We Really Do It Again? The Powerful Concept of Replication Is Neglected in the Social Sciences.‚Äù Review of General Psychology: Journal of Division 1, of the American Psychological Association 13 (June): 90‚Äì100. https://doi.org/10.1037/a0015108.\n\n\nSimonsohn, Uri. 2015. ‚ÄúSmall Telescopes: Detectability and the Evaluation of Replication Results.‚Äù Psychological Science 26 (May): 559‚Äì69. https://doi.org/10.1177/0956797614567341.\n\n\nSpence, Jeffrey R, and David J Stanley. 2016. ‚ÄúPrediction Interval: What to Expect When You‚Äôre Expecting ‚Ä¶ a Replication.‚Äù PloS One 11 (September): e0162874. https://doi.org/10.1371/journal.pone.0162874.\n\n\nValentine, Jeffrey C, Anthony Biglan, Robert F Boruch, Felipe Gonz√°lez Castro, Linda M Collins, Brian R Flay, Sheppard Kellam, Eve K Mo≈õcicki, and Steven P Schinke. 2011. ‚ÄúReplication in Prevention Science.‚Äù Prevention Science: The Official Journal of the Society for Prevention Research 12 (June): 103‚Äì17. https://doi.org/10.1007/s11121-011-0217-6.\n\n\nVerhagen, Josine, and Eric-Jan Wagenmakers. 2014. ‚ÄúBayesian Tests to Quantify the Result of a Replication Attempt.‚Äù Journal of Experimental Psychology. General 143 (August): 1457‚Äì75. https://doi.org/10.1037/a0036731.\n\n\nWagenmakers, Eric-Jan, Tom Lodewyckx, Himanshu Kuriyal, and Raoul Grasman. 2010. ‚ÄúBayesian Hypothesis Testing for Psychologists: A Tutorial on the Savage‚ÄìDickey Method.‚Äù Cognitive Psychology 60 (May): 158‚Äì89. https://doi.org/10.1016/j.cogpsych.2009.12.001."
  },
  {
    "objectID": "slides/00-intro/00-intro.html#about-me-1",
    "href": "slides/00-intro/00-intro.html#about-me-1",
    "title": "Introduction to the workshops",
    "section": "About me",
    "text": "About me\nFilippo Gambarota\n\n  \n\nPostdoctoral Researcher: Department of Developmental and Social Psychology\n\nResearch interests: meta-analysis, psychometrics, data simulation, R programming and multiverse analysis."
  },
  {
    "objectID": "slides/00-intro/00-intro.html#about-us",
    "href": "slides/00-intro/00-intro.html#about-us",
    "title": "Introduction to the workshops",
    "section": "About us",
    "text": "About us\nWe are part of the Psicostat research group. A interdisciplinary research group interested in Psychology and Statistics. psicostat.dpss.psy.unipd.it"
  },
  {
    "objectID": "slides/00-intro/00-intro.html#program-1",
    "href": "slides/00-intro/00-intro.html#program-1",
    "title": "Introduction to the workshops",
    "section": "Program",
    "text": "Program\n\n\n\n\n\nDay\nTitle\nTopics\n\n\n\n08/07/24\nTools for reproducible research\nR, Quarto, Git/Github, Open Science Framework\n\n\n09/07/24\nStatistical methods for replication assessment\nFrequentist and Bayesian statistical methods for replicability assessment\n\n\n10/07/24\nMeta-analyis and multiverse analysis\nEqual and random-effects model, heterogeneity, power-analysis, simulating data"
  },
  {
    "objectID": "slides/00-intro/00-intro.html#materials",
    "href": "slides/00-intro/00-intro.html#materials",
    "title": "Introduction to the workshops",
    "section": "Materials üìò",
    "text": "Materials üìò\n\n\nüåê All the material (code, slides, extra) are available on Github github.com/stat-teaching/replicability-stat-unipd-phd. The same material can be accessed using this link stat-teaching.github.io/replicability-stat-unipd-phd\n\n\n\n\n\nüìù Slides are created with Quarto, you can use it as standard slides (in html format) and see the source code (.qmd file)\n\n\n\n\nüíª For code debugging, exercises or general questions we can use this shared online code editor etherpad.wikimedia.org/p/replicability-stat-unipd-phd. Basically we can write code together üòÑ\n\n\n\n\n‚öôÔ∏è We are going to use mainly the slides, R Studio, the Github website and the shared editor. I suggest you to bookmark the Github website and the editor"
  },
  {
    "objectID": "slides/00-intro/00-intro.html#disclaimer",
    "href": "slides/00-intro/00-intro.html#disclaimer",
    "title": "Introduction to the workshops",
    "section": "Disclaimer",
    "text": "Disclaimer\n\n\nWe are using a lot of (R) code. When talking about code there is no a unique solution or method. My approach is not the best. If your code works, everything good üòÑ\n\n\n\n\n\nIf we have time, we can discuss about best practice in writing code in terms of efficency, organization and clarity üòé\n\n\n\n\n\nAlso for statistics related topics, there are often multiple options to solve a problem. If you know other alternatives beyond the proposed topics, we can discuss it üòâ"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Replicability Crisis in Science?",
    "section": "",
    "text": "Replicability Crisis in Science? is one of the Specialist Courses organized from the Department of Statistical Sciences (University of Padova). The course has been proposed for the AY 2023/24 to the PhD students of the XXXIX cycle."
  },
  {
    "objectID": "index.html#monday-8th-july-2024",
    "href": "index.html#monday-8th-july-2024",
    "title": "Replicability Crisis in Science?",
    "section": "Monday 8th July 2024",
    "text": "Monday 8th July 2024\n\n9:00 - 12:00\n\nBranden Fitelson - What is a Replication?\n\nPaper: Machery (2020) [pdf]\n\n\n\nGiovanni Parmigiani - Probability of Replication\n\nPaper: Miller (2009) [pdf] [slides]\n\n\n\n\n14:30 - 16:30\n\nFilippo Gambarota - Tools for Open Science"
  },
  {
    "objectID": "index.html#tuesday-9th-july-2024",
    "href": "index.html#tuesday-9th-july-2024",
    "title": "Replicability Crisis in Science?",
    "section": "Tuesday 9th July 2024",
    "text": "Tuesday 9th July 2024\n\n9:00 - 12:00\n\nBranden Fitelson - How not to measure replication\n\nPaper: Fletcher (2021) [pdf]\n\n\n\nGiovanni Parmigiani - How to measure replication\n\nPaper: Mathur & VanderWeele (2020) [pdf] and statistical supplement\n\n\n\n\n14:30 - 16:30\n\nFilippo Gambarota - R tools and examples for Measures of Replication"
  },
  {
    "objectID": "index.html#wednesday-10th-july-2024",
    "href": "index.html#wednesday-10th-july-2024",
    "title": "Replicability Crisis in Science?",
    "section": "Wednesday 10th July 2024",
    "text": "Wednesday 10th July 2024\n\n9:00 - 12:00\n\nBranden Fitelson - Replicability in Psychology\n\nPaper: Nosek et al. (2022) [pdf]\n\n\n\nGiovanni Parmigiani - Replicability in Cancer Science\n\nPaper: Errington et al. (2021) [pdf] Errington‚Äôs presentation\n\n\n\n\n14:30 - 16:30\n\nFilippo Gambarota - Meta-analysis and multiverse analysis"
  },
  {
    "objectID": "index.html#group-bayes",
    "href": "index.html#group-bayes",
    "title": "Replicability Crisis in Science?",
    "section": "Group Bayes",
    "text": "Group Bayes\n\nAlberto Petrin\nAllegra Sartore\nDenise Feurer\nIrene Di Pietro\nVanshika Keshwani"
  },
  {
    "objectID": "index.html#group-bonferroni",
    "href": "index.html#group-bonferroni",
    "title": "Replicability Crisis in Science?",
    "section": "Group Bonferroni",
    "text": "Group Bonferroni\n\nAgmas Sisay Abera\nChuchu Jia\nKenenisa Tadesse Dame\nLaura Gorla\nMatteo Licitra"
  },
  {
    "objectID": "index.html#group-gosset",
    "href": "index.html#group-gosset",
    "title": "Replicability Crisis in Science?",
    "section": "Group Gosset",
    "text": "Group Gosset\n\nAlessandro Zito\nLudovica Natali\nMaria Francesca Patalano\nMatteo Schiavone"
  },
  {
    "objectID": "index.html#group-neyman",
    "href": "index.html#group-neyman",
    "title": "Replicability Crisis in Science?",
    "section": "Group Neyman",
    "text": "Group Neyman\n\nAlex Cecchetto\nElena Baldisseri\nEvgenii Pashnin\nGiovanni Duca\nLeonardo Genesin"
  },
  {
    "objectID": "index.html#group-pearson",
    "href": "index.html#group-pearson",
    "title": "Replicability Crisis in Science?",
    "section": "Group Pearson",
    "text": "Group Pearson\n\nAmbra Perugini\nEnrico Carraro\nHillary Muhanguzi\nIsabella Valbusa\nSara Costa"
  },
  {
    "objectID": "index.html#group-tukey",
    "href": "index.html#group-tukey",
    "title": "Replicability Crisis in Science?",
    "section": "Group Tukey",
    "text": "Group Tukey\n\nDavide Forcina\nJaurel Kagho zanguim\nRunpeng Miao\nVirginia Murru\nYunfeng Sun"
  },
  {
    "objectID": "materials.html",
    "href": "materials.html",
    "title": "Replicability Crisis in Science?",
    "section": "",
    "text": "Filippo Gambarota\n\nIntroduction to the workshop\nReplication starter pack\nStatistical methods for assessing replication\nIntroduction to meta-analysis\nMultiverse analysis"
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#doing-research-is-hard-1",
    "href": "slides/01-replication-tools/01-replication-tools.html#doing-research-is-hard-1",
    "title": "Tools for reproducible research",
    "section": "Doing research is hard‚Ä¶",
    "text": "Doing research is hard‚Ä¶\n\n\nyou have to read papers, textbooks, slides and track information\n\n\n\n\nyou have to plan your experiment or research\n\n\n\n\nyou have to collect, organize and manage the research data\n\n\n\n\n\nyou have to analyze data, create figures and tables\n\n\n\n\n\nyou have to write reports, papers, slides, etc."
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#doing-research-is-hard-2",
    "href": "slides/01-replication-tools/01-replication-tools.html#doing-research-is-hard-2",
    "title": "Tools for reproducible research",
    "section": "Doing research is hard‚Ä¶",
    "text": "Doing research is hard‚Ä¶"
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#doing-reproducible-research-is-even-harder",
    "href": "slides/01-replication-tools/01-replication-tools.html#doing-reproducible-research-is-even-harder",
    "title": "Tools for reproducible research",
    "section": "Doing reproducible research is even harder‚Ä¶ üò±",
    "text": "Doing reproducible research is even harder‚Ä¶ üò±\n\n\norganize data in a sharable and comprehensible format\n\n\n\n\nchoose a future-proof place to share data along the research paper\n\n\n\n\nanalyze data using a reproducible framework: code, programming language, scripting\n\n\n\n\nreport data (papers, slides, etc.) using a reproducible framework"
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#reproducibility-starter-pack-1",
    "href": "slides/01-replication-tools/01-replication-tools.html#reproducibility-starter-pack-1",
    "title": "Tools for reproducible research",
    "section": "Reproducibility starter pack üë∑\n",
    "text": "Reproducibility starter pack üë∑\n\n\n\nA general purpose (or flexible enough) programming language such as  or \n\n\n\n\n\nA literate programming framework to integrate code and text\n\n\n\n\nA version control system to track projects\n\n\n\n\nAn online repository to store the project and sharing with others"
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#r",
    "href": "slides/01-replication-tools/01-replication-tools.html#r",
    "title": "Tools for reproducible research",
    "section": "R",
    "text": "R\n\nR is a free software environment for statistical computing and graphics.\n\n\n(TBH) Is not a proper general purpose programming language (such as C++ or Python).\nNew extensions (packages) allow R to do pretty everything (file manager, image processing, webscraping, etc.)\nIt is free and open-source\nThe community is extremely active and keep growing"
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#r---cran",
    "href": "slides/01-replication-tools/01-replication-tools.html#r---cran",
    "title": "Tools for reproducible research",
    "section": "R - CRAN",
    "text": "R - CRAN\nThe CRAN is the repository where package developers upload their packages and other users can install them.\n\n\n\n\n\n\n\nAs the saying goes: if something exist, there is for sure an R package for doing it! üòÑ"
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#r---pypl-index",
    "href": "slides/01-replication-tools/01-replication-tools.html#r---pypl-index",
    "title": "Tools for reproducible research",
    "section": "R - PYPL Index",
    "text": "R - PYPL Index\n\n\n\nSource: https://pypl.github.io/PYPL.html\n\n\nRank\nLanguage\nShare\n1-year.trend\n\n\n\n1\nPython\n28.04%\n0.30%\n\n\n2\nJava\n15.78%\n-1.30%\n\n\n3\nJavaScript\n9.27%\n-0.20%\n\n\n4\nC#\n6.77%\n-0.20%\n\n\n5\nC/C++\n6.59%\n0.40%\n\n\n6\nPHP\n5.01%\n-0.40%\n\n\n7\nR\n4.35%\n0.00%\n\n\n8\nTypeScript\n3.09%\n0.30%\n\n\n9\nSwift\n2.54%\n0.50%\n\n\n10\nObjective-C\n2.15%\n0.10%\n\n\n11\nRust\n2.14%\n0.50%"
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#r---pypl-index-1",
    "href": "slides/01-replication-tools/01-replication-tools.html#r---pypl-index-1",
    "title": "Tools for reproducible research",
    "section": "R - PYPL Index",
    "text": "R - PYPL Index\nThe popularity is on a different scale compared to Python but still increasing:\n\n\n \n  \n \n \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n \n \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n \n \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n \n \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n \n \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n \n \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n \n \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n \n \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n \n \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n \n \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n \n \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n \n \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n \n \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n \n \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n \n \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n \n \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n \n \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n \n \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n \n \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n \n \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n \n \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n \n \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n \n \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n \n \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n \n \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n \n \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n \n \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n \n \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n \n \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n \n \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n \n \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n \n\nSource: https://pypl.github.io/PYPL.html"
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#r-or-python",
    "href": "slides/01-replication-tools/01-replication-tools.html#r-or-python",
    "title": "Tools for reproducible research",
    "section": "R or Python?",
    "text": "R or Python?\n\nPython is a good alternative. Personally, I use and enjoy python but I do most of my work in R.\nPython is a very general-purpose language more powerful for general tasks.\nI find python very useful for programming cognitive experiments, image processing, automatizing tasks and interacting with the operating system\nR is still a little bit superior in terms of data manipulation and visualization. Python is faster and more powerful for complex machine learning."
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#modern-r",
    "href": "slides/01-replication-tools/01-replication-tools.html#modern-r",
    "title": "Tools for reproducible research",
    "section": "Modern R",
    "text": "Modern R\n\n\nFor purist programmers, R is somehow weird: arrays starts with 1, object-oriented programming is hidden, a lot of built-in vectorized functions, etc. See The R Inferno book for an overview.\n\n\n\n\nDespite the weirdness, R have the majority of common programming paradigms (functions, conditionals, iterations, etc.).\n\n\n\n\nThe scope of this week is not providing an introduction to R but I would put the focus on two topics for a modern usage of R:\n\nfunctional programming\nthe tidy approach"
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#functional-programming",
    "href": "slides/01-replication-tools/01-replication-tools.html#functional-programming",
    "title": "Tools for reproducible research",
    "section": "Functional Programming",
    "text": "Functional Programming\n\nIn computer science, functional programming is a programming paradigm where programs are constructed by applying and composing functions.\n\nDespite R can be used both with an imperative and object-oriented approach, the functional side is quite powerful.\nActually, functional programming is quite complex. Here we refer to breaking down our code into small functions. These functions can be function from packages, custom or anonymous functions."
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#functional-programming-example",
    "href": "slides/01-replication-tools/01-replication-tools.html#functional-programming-example",
    "title": "Tools for reproducible research",
    "section": "Functional Programming, example‚Ä¶",
    "text": "Functional Programming, example‚Ä¶\nWe have a dataset (mtcars) and we want to calculate the mean, median, standard deviation, minimum and maximum of each column and store the result in a table.\n\nhead(mtcars)\n#&gt;                    mpg cyl disp  hp drat    wt  qsec vs am gear carb\n#&gt; Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n#&gt; Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n#&gt; Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n#&gt; Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n#&gt; Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n#&gt; Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\nstr(mtcars)\n#&gt; 'data.frame':    32 obs. of  11 variables:\n#&gt;  $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n#&gt;  $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n#&gt;  $ disp: num  160 160 108 258 360 ...\n#&gt;  $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n#&gt;  $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n#&gt;  $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n#&gt;  $ qsec: num  16.5 17 18.6 19.4 17 ...\n#&gt;  $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n#&gt;  $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n#&gt;  $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n#&gt;  $ carb: num  4 4 1 1 2 1 4 2 2 4 ..."
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#functional-programming-1",
    "href": "slides/01-replication-tools/01-replication-tools.html#functional-programming-1",
    "title": "Tools for reproducible research",
    "section": "Functional Programming",
    "text": "Functional Programming\nThe standard (~imperative) option is using a for loop, iterating through columns, calculate the values and store into another data structure.\n\nncols &lt;- ncol(mtcars)\nmeans &lt;- medians &lt;- mins &lt;- maxs &lt;- rep(0, ncols)\n\nfor(i in 1:ncols){\n  means[i] &lt;- mean(mtcars[[i]])\n  medians[i] &lt;- mean(mtcars[[i]])\n  mins[i] &lt;- mean(mtcars[[i]])\n  maxs[i] &lt;- mean(mtcars[[i]])\n}\n\nresults &lt;- data.frame(means, medians, mins, maxs)\nresults$col &lt;- names(mtcars)\n\nresults\n#&gt;         means    medians       mins       maxs  col\n#&gt; 1   20.090625  20.090625  20.090625  20.090625  mpg\n#&gt; 2    6.187500   6.187500   6.187500   6.187500  cyl\n#&gt; 3  230.721875 230.721875 230.721875 230.721875 disp\n#&gt; 4  146.687500 146.687500 146.687500 146.687500   hp\n#&gt; 5    3.596563   3.596563   3.596563   3.596563 drat\n#&gt; 6    3.217250   3.217250   3.217250   3.217250   wt\n#&gt; 7   17.848750  17.848750  17.848750  17.848750 qsec\n#&gt; 8    0.437500   0.437500   0.437500   0.437500   vs\n#&gt; 9    0.406250   0.406250   0.406250   0.406250   am\n#&gt; 10   3.687500   3.687500   3.687500   3.687500 gear\n#&gt; 11   2.812500   2.812500   2.812500   2.812500 carb"
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#functional-programming-2",
    "href": "slides/01-replication-tools/01-replication-tools.html#functional-programming-2",
    "title": "Tools for reproducible research",
    "section": "Functional Programming",
    "text": "Functional Programming\nWe can decompose (and symplify the problem) by writing a function and looping through columns.\n\n\nsumm &lt;- function(x){\n  data.frame(means = mean(x), medians = median(x), mins = min(x), maxs = max(x))\n}\nncols &lt;- ncol(mtcars)\ndfs &lt;- vector(mode = \"list\", length = ncols)\n\nfor(i in 1:ncols){\n  dfs[[i]] &lt;- summ(mtcars[[i]])\n}\n\nresults &lt;- do.call(rbind, dfs)\nresults\n#&gt;         means medians   mins    maxs\n#&gt; 1   20.090625  19.200 10.400  33.900\n#&gt; 2    6.187500   6.000  4.000   8.000\n#&gt; 3  230.721875 196.300 71.100 472.000\n#&gt; 4  146.687500 123.000 52.000 335.000\n#&gt; 5    3.596563   3.695  2.760   4.930\n#&gt; 6    3.217250   3.325  1.513   5.424\n#&gt; 7   17.848750  17.710 14.500  22.900\n#&gt; 8    0.437500   0.000  0.000   1.000\n#&gt; 9    0.406250   0.000  0.000   1.000\n#&gt; 10   3.687500   4.000  3.000   5.000\n#&gt; 11   2.812500   2.000  1.000   8.000"
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#functional-programming-3",
    "href": "slides/01-replication-tools/01-replication-tools.html#functional-programming-3",
    "title": "Tools for reproducible research",
    "section": "Functional Programming",
    "text": "Functional Programming\n\nWe can be even more minimalistic by removing the for loop and using the *apply family that provide a series of compact iterative method.\n\n\n\nresults &lt;- lapply(mtcars, summ)\nresults &lt;- do.call(rbind, results)\nresults\n#&gt;           means medians   mins    maxs\n#&gt; mpg   20.090625  19.200 10.400  33.900\n#&gt; cyl    6.187500   6.000  4.000   8.000\n#&gt; disp 230.721875 196.300 71.100 472.000\n#&gt; hp   146.687500 123.000 52.000 335.000\n#&gt; drat   3.596563   3.695  2.760   4.930\n#&gt; wt     3.217250   3.325  1.513   5.424\n#&gt; qsec  17.848750  17.710 14.500  22.900\n#&gt; vs     0.437500   0.000  0.000   1.000\n#&gt; am     0.406250   0.000  0.000   1.000\n#&gt; gear   3.687500   4.000  3.000   5.000\n#&gt; carb   2.812500   2.000  1.000   8.000"
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#functional-programming-apply",
    "href": "slides/01-replication-tools/01-replication-tools.html#functional-programming-apply",
    "title": "Tools for reproducible research",
    "section": "Functional Programming, *apply\n",
    "text": "Functional Programming, *apply\n\n\nThe *apply family is one of the best tool in R. The idea is pretty simple: apply a function to each element of a list.\n\n\nThe powerful side is that in R everything can be considered as a list. A vector is a list of single elements, a dataframe is a list of columns etc.\n\n\nInternally, R is still using a for loop but the verbose part (preallocation, choosing the iterator, indexing) is encapsulated into the *apply function.\n\n\n\nmeans &lt;- rep(0, ncol(mtcars))\nfor(i in 1:length(means)){\n  means[i] &lt;- mean(mtcars[[i]])\n}\n\n# the same with sapply\nmeans &lt;- sapply(mtcars, mean)"
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#for-loops-are-bad",
    "href": "slides/01-replication-tools/01-replication-tools.html#for-loops-are-bad",
    "title": "Tools for reproducible research",
    "section": "\nfor loops are bad?",
    "text": "for loops are bad?\nfor loops are the core of each operation in R (and in every programming language). For complex operation thery are more readable and effective compared to *apply. In R we need extra care for writing efficent for loops.\nExtremely slow, no preallocation:\n\nres &lt;- c()\nfor(i in 1:1000){\n  # do something\n  res[i] &lt;- x\n}\n\nVery fast, no difference compared to *apply\n\nres &lt;- rep(0, 1000)\nfor(i in 1:length(res)){\n  # do something\n  res[i] &lt;- x\n}"
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#why-functional-programming",
    "href": "slides/01-replication-tools/01-replication-tools.html#why-functional-programming",
    "title": "Tools for reproducible research",
    "section": "Why functional programming?",
    "text": "Why functional programming?\n\n\nWe can write less and reusable code that can be shared\n\n\n\n\nThe scripts are more compact, less errors prone and more flexible (imagine that you want to improve the summ function, you only need to change it once instead of touching the for loop)\n\n\n\n\nFunctions can be easily and consistently documented (see roxygen documentation) improving the reproducibility and clarity of the code"
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#more-about-functional-programming-in-r",
    "href": "slides/01-replication-tools/01-replication-tools.html#more-about-functional-programming-in-r",
    "title": "Tools for reproducible research",
    "section": "More about functional programming in R",
    "text": "More about functional programming in R\n\nAdvanced R by Hadley Wickham, section on Functional Programming (https://adv-r.hadley.nz/fp.html)\nHands-On Programming with R by Garrett Grolemund https://rstudio-education.github.io/hopr/\n\nHadley Wickham: The Joy of Functional Programming (for Data Science)\n\nBruno Rodrigues Youtube Channel"
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#the-tidy-approach",
    "href": "slides/01-replication-tools/01-replication-tools.html#the-tidy-approach",
    "title": "Tools for reproducible research",
    "section": "The Tidy approach",
    "text": "The Tidy approach\nThe tidyverse is a series of high-quality R packages to do modern data science:\n\ndata manipulation (dplyr, tidyr)\nplotting (ggplot2)\nreporting (rmarkdown)\nstring manipulation (stringr)\nfunctionals (purrr)\n‚Ä¶"
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#the-tidy-approach---pipes",
    "href": "slides/01-replication-tools/01-replication-tools.html#the-tidy-approach---pipes",
    "title": "Tools for reproducible research",
    "section": "The Tidy approach - Pipes",
    "text": "The Tidy approach - Pipes\nOne of the great improvement from the tidyverse is the usage of the pipe %&gt;% now introduced in base R as |&gt;. You will se these symbols a lot when looking at modern R code.\n\nThe idea is very simple, the standard pattern to apply a function is function(argument). The pipe can reverse the pattern as argument |&gt; function(). Normally when we apply multiple functions progressively the pattern is this:\n\n\n\nx &lt;- rnorm(100)\nx &lt;- round(x, 3)\nx &lt;- abs(x)\nx &lt;- as.character(x)"
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#the-tidy-approach---pipes-1",
    "href": "slides/01-replication-tools/01-replication-tools.html#the-tidy-approach---pipes-1",
    "title": "Tools for reproducible research",
    "section": "The Tidy approach - Pipes",
    "text": "The Tidy approach - Pipes\nWhen using the pipe, we remove the redundand assignment &lt;- pattern:\n\nx &lt;- rnorm(100)\nx |&gt;\n  round(3) |&gt;\n  abs() |&gt;\n  as.character()\n\nThe pipe can be read as ‚Äúfrom x apply round, then abs, etc.‚Äù. The first argument of the piped function is assumed to be the result of the previus call."
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#more-about-the-tidy-approach",
    "href": "slides/01-replication-tools/01-replication-tools.html#more-about-the-tidy-approach",
    "title": "Tools for reproducible research",
    "section": "More about the Tidy approach",
    "text": "More about the Tidy approach\nThe tidy approach contains tons of functions and packages. The overall philosopgy can be deepen in the R for Data Science book.\n\nhttps://r4ds.hadley.nz/"
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#ggplot2",
    "href": "slides/01-replication-tools/01-replication-tools.html#ggplot2",
    "title": "Tools for reproducible research",
    "section": "ggplot2",
    "text": "ggplot2\n\n\nCode\nResult\nBase R Code\nBase R Result\n\n\n\nOnly an honour mention to ggplot2 https://ggplot2-book.org/ (part of the tidyverse) that is an amazing package for data visualization following the piping and tidy approach. Is the implementation of the grammar of graphics idea.\n\nlibrary(tidyverse)\niris |&gt;\n  mutate(wi = runif(n())) |&gt;\n  ggplot(aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point(aes(size = wi)) +\n  geom_smooth(method = \"lm\", se = FALSE)\n  guides(size = \"none\") +\n  theme_minimal(15)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n\n\n\n\nMore verbose, more hard coding, more steps and intermidiate objects.\n\niris_l &lt;- split(iris, iris$Species)\nlms &lt;- lapply(iris_l, function(x) lm(Petal.Width ~ Sepal.Length, data = x))\n\nplot(iris$Sepal.Length, iris$Petal.Width, col = as.numeric(iris$Species), pch = 19)\nabline(lms[[1]], col = 1, lwd = 2)\nabline(lms[[2]], col = 2, lwd = 2)\nabline(lms[[3]], col = 3, lwd = 2)\nlegend(\"topleft\", legend = levels(iris$Species), fill = 1:3)"
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#ggplot2-1",
    "href": "slides/01-replication-tools/01-replication-tools.html#ggplot2-1",
    "title": "Tools for reproducible research",
    "section": "ggplot2",
    "text": "ggplot2\nThe ggplot2 book https://ggplot2-book.org/ is a great resource to produce high-quality, publication ready plots. Clearly, the advantage of producing the figures entirely writing code are immense in terms of reusability and reproducibility."
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#literate-programming-1",
    "href": "slides/01-replication-tools/01-replication-tools.html#literate-programming-1",
    "title": "Tools for reproducible research",
    "section": "Literate Programming1\n",
    "text": "Literate Programming1\n\n\nDonald Knuth first defined literate programming as a script, notebook, or computational document that contains an explanation of the program logic in a natural language, interspersed with snippets of macros and source code, which can be compiled and rerun\n\nFor example jupyter notebooks, R Markdown and now Quarto are literate programming frameworks to integrate code and text.\n\nHeadingLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod temporHeading...Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod temporLorem ipsum dolor sit amet,...Plot CodePlot CodeMarkupMarkupMarkupMarkup............Text is not SVG - cannot display\n\n\nhttps://en.wikipedia.org/wiki/Literate_programming"
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#literate-programming-the-markup-language",
    "href": "slides/01-replication-tools/01-replication-tools.html#literate-programming-the-markup-language",
    "title": "Tools for reproducible research",
    "section": "Literate Programming, the markup language",
    "text": "Literate Programming, the markup language\nBeyond the coding part, the markup language is the core element of a literate programming framework. The idea of a markup language is separating the result from what you actually write. Some examples are:\n\nLaTeX\nHTML\nMarkdown\nXML\n‚Ä¶"
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#latex",
    "href": "slides/01-replication-tools/01-replication-tools.html#latex",
    "title": "Tools for reproducible research",
    "section": "LaTeX 1\n",
    "text": "LaTeX 1\n\n\nhttps://latexbase.com/"
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#html",
    "href": "slides/01-replication-tools/01-replication-tools.html#html",
    "title": "Tools for reproducible research",
    "section": "HTML",
    "text": "HTML\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;body&gt;\n\n&lt;h1&gt;My First Heading&lt;/h1&gt;\n\nLorem Ipsum √® un testo segnaposto utilizzato nel settore della tipografia e della stampa. Lorem Ipsum √® considerato il testo segnaposto standard sin dal sedicesimo secolo, quando un anonimo tipografo prese una cassetta di caratteri e li assembl√≤ per preparare un testo campione. √à sopravvissuto non solo a pi√π di cinque secoli, ma anche al passaggio alla videoimpaginazione, pervenendoci sostanzialmente inalterato. Fu reso popolare, negli anni ‚Äô60, con la diffusione dei fogli di caratteri trasferibili ‚ÄúLetraset‚Äù, che contenevano passaggi del Lorem Ipsum, e pi√π recentemente da software di impaginazione come Aldus PageMaker, che includeva versioni del Lorem Ipsum.\n\n&lt;h2&gt;My Second Heading&lt;/h2&gt;\n\nLorem Ipsum √® un testo segnaposto utilizzato nel settore della tipografia e della stampa. \n\nLorem Ipsum √® considerato il testo segnaposto standard sin dal sedicesimo secolo, quando un anonimo \n\ntipografo prese una cassetta di caratteri e li assembl√≤ per preparare un testo campione. \n\n√à sopravvissuto non solo a pi√π di cinque secoli, ma anche al passaggio alla videoimpaginazione, pervenendoci sostanzialmente inalterato. \n\nFu reso popolare, negli anni ‚Äô60, con la diffusione dei \n\nfogli di caratteri trasferibili ‚ÄúLetraset‚Äù, che contenevano passaggi del Lorem Ipsum\n\npi√π recentemente da software di impaginazione come Aldus PageMaker, che includeva versioni del Lorem Ipsum.\n\n&lt;/body&gt;\n&lt;/html&gt;"
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#markdown",
    "href": "slides/01-replication-tools/01-replication-tools.html#markdown",
    "title": "Tools for reproducible research",
    "section": "Markdown1\n",
    "text": "Markdown1\n\n\n\nhttps://markdownlivepreview.com/"
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#markdown-1",
    "href": "slides/01-replication-tools/01-replication-tools.html#markdown-1",
    "title": "Tools for reproducible research",
    "section": "Markdown",
    "text": "Markdown\nMarkdown is one of the most popular markup languages for several reasons:\n\neasy to write and read compared to Latex and HTML\neasy to convert from Markdown to basically every other format using pandoc\n\neasy to implement new features"
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#markdown-source-code",
    "href": "slides/01-replication-tools/01-replication-tools.html#markdown-source-code",
    "title": "Tools for reproducible research",
    "section": "Markdown (source code)",
    "text": "Markdown (source code)\n## Markdown\n\nMarkdown is one of the most popular markup languages for several reasons:\n\n- easy to write and read compared to Latex and HTML\n- easy to convert from Markdown to basically every other format using `pandoc`\n- easy to implement new features\n\nAlso the source code can be used, compared to Latex or HTML, to take notes and read. Latex and HTML need to be compiled otherwise they are very hard to read."
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#whats-wrong-about-microsoft-word",
    "href": "slides/01-replication-tools/01-replication-tools.html#whats-wrong-about-microsoft-word",
    "title": "Tools for reproducible research",
    "section": "What‚Äôs wrong about Microsoft Word?",
    "text": "What‚Äôs wrong about Microsoft Word?\nMS Word is a WYSIWYG (what you see is what you get editor) that force users to think about formatting, numbering, etc. Markup languages receive the content (plain text) and the rules and creates the final document."
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#whats-wrong-about-microsoft-word-1",
    "href": "slides/01-replication-tools/01-replication-tools.html#whats-wrong-about-microsoft-word-1",
    "title": "Tools for reproducible research",
    "section": "What‚Äôs wrong about Microsoft Word?",
    "text": "What‚Äôs wrong about Microsoft Word?\nBeyond the pure writing process, there are other aspects related to research data.\n\nwriting math formulas\nreporting statistics in the text\nproducing tables\nproducing plots\n\nIn MS Word (or similar) we need to produce everything outside and then manually put figures and tables."
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#the-solution-quarto",
    "href": "slides/01-replication-tools/01-replication-tools.html#the-solution-quarto",
    "title": "Tools for reproducible research",
    "section": "The solution‚Ä¶ Quarto",
    "text": "The solution‚Ä¶ Quarto\nQuarto (https://quarto.org/) is the evolution of R Markdown that integrate a programming language with the Markdown markup language. It is very simple but quite powerful."
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#basic-markdown",
    "href": "slides/01-replication-tools/01-replication-tools.html#basic-markdown",
    "title": "Tools for reproducible research",
    "section": "Basic Markdown",
    "text": "Basic Markdown\nMarkdown can be learned in minutes. You can go to the following link https://quarto.org/docs/authoring/markdown-basics.html and try to understand the syntax."
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#more-about-quarto-and-r-markdown",
    "href": "slides/01-replication-tools/01-replication-tools.html#more-about-quarto-and-r-markdown",
    "title": "Tools for reproducible research",
    "section": "More about Quarto and R Markdown",
    "text": "More about Quarto and R Markdown\nThe topic is extremely vast. You can do everything in Quarto, a website, thesis, your CV, etc.\n\nYihui Xie - R Markdown Cookbook https://bookdown.org/yihui/rmarkdown-cookbook/\n\nYihui Xie - R Markdown: The Definitive Guide https://bookdown.org/yihui/rmarkdown/\n\nQuarto documentation https://quarto.org/docs/guide/"
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#git-and-github-1",
    "href": "slides/01-replication-tools/01-replication-tools.html#git-and-github-1",
    "title": "Tools for reproducible research",
    "section": "Git and Github",
    "text": "Git and Github\nThe basic idea is to track changes within a folder, assign a message and eventually a tag to a specific version obtaining a version hystory. The version history is completely navigable, you can go back to a previous version of the code.\n\nThe are advanced features like branches for creating an independent version of the project to test new features and then merge into the main streamline.\n\n\nThe entire (local) Git project can be hosted on Github to improve collaboration. Other people or collaborators can clone the repository and push their changes to the project."
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#veeeery-basic-git-workflow-1",
    "href": "slides/01-replication-tools/01-replication-tools.html#veeeery-basic-git-workflow-1",
    "title": "Tools for reproducible research",
    "section": "Veeeery basic Git workflow",
    "text": "Veeeery basic Git workflow\n\nAfter installing Git, you can start a new repository opening a terminal on a folder and typing git init. The folder is now a git project you can notice by the hidden .git folder.\n\ncd ~/some/folder\ngit init\n\nThen you can add files to the staging area. Basically these files are ready to be committed i.e.¬†‚Äúwritten‚Äù in the Git history.\n\ngit add file1.txt\n# git add . # add everyting\n\nFinally you can commit the modified version of the file using git commit -m message\n\n\ngit commit -m \"my first amazing commit\"\n\nyou can see the Git hystory with all your commits:\n\ngit log"
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#github",
    "href": "slides/01-replication-tools/01-replication-tools.html#github",
    "title": "Tools for reproducible research",
    "section": "Github",
    "text": "Github\nImagine to put everyting into a server with nice viewing options and advanced features. Github is just an hosting service for your git folder.\nYou can create an empty repository on Github named git-test. Now my repo has the path git@github.com:filippogambarota/git-test.git.\ngit remote add origin git@github.com:filippogambarota/git-test.git\ngit push\nNow our local repository is linked with the remote repository. Every time we do git push our local commits will be uploaded.\nIf you worked on the repository from another machine or a colleague add some changes, you can do git pull and your local machine will be updated.\n\nThe repository git-test is online and can be seen here filippogambarota/git-test."
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#github-1",
    "href": "slides/01-replication-tools/01-replication-tools.html#github-1",
    "title": "Tools for reproducible research",
    "section": "Github",
    "text": "Github\nAn now let‚Äôs see on Github the result:"
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#more-about-git-and-github",
    "href": "slides/01-replication-tools/01-replication-tools.html#more-about-git-and-github",
    "title": "Tools for reproducible research",
    "section": "More about Git and Github",
    "text": "More about Git and Github\nThere are a lot of resources online:\n\nThe Open Science Manual - Zandonella and Massidda - Git and Github chapters.\nhttps://agripongit.vincenttunru.com/\nhttps://git-scm.com/docs/gittutorial"
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#open-science-framework-1",
    "href": "slides/01-replication-tools/01-replication-tools.html#open-science-framework-1",
    "title": "Tools for reproducible research",
    "section": "Open Science Framework",
    "text": "Open Science Framework\n\nOSF is a free, open platform to support your research and enable collaboration.\n\nIs a great tool to upload and share materials with others and collaborate on a project. Similarly to Github you can track the changes made to a project.\nThe great addition is having a DOI thus the project is persistently online and can be cited.\nIt is now common practice to create a OSF project supporting a research paper and put the link within the paper containing supplementary materials, raw data, scripts etc."
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#open-science-framework-2",
    "href": "slides/01-replication-tools/01-replication-tools.html#open-science-framework-2",
    "title": "Tools for reproducible research",
    "section": "Open Science Framework",
    "text": "Open Science Framework\nIt‚Äôs very easy to create a new project, then you simply need to add files and share it.\n\nThe project can be accessed here (depending on the visibility) https://osf.io/yf9tg/."
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#open-science-framework-3",
    "href": "slides/01-replication-tools/01-replication-tools.html#open-science-framework-3",
    "title": "Tools for reproducible research",
    "section": "Open Science Framework",
    "text": "Open Science Framework\nOSF and Github\nAn interesting feature is linking a Github repository to OSF. Now all changes made on Github (easier to manage) are mirrored into OSF. You can easily work in Github for the coding part and use OSF to upload other data or information and to assign a DOI to the project.\nPreprints\nOSF is also linked to a popular service for preprints called PsyArXiv https://psyarxiv.com/ thus you can link a preprint to an OSF project."
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#more-on-osf",
    "href": "slides/01-replication-tools/01-replication-tools.html#more-on-osf",
    "title": "Tools for reproducible research",
    "section": "More on OSF",
    "text": "More on OSF\n\nhttps://help.osf.io/article/342-getting-started-on-the-osf\nhttps://arca-dpss.github.io/manual-open-science/osf-chapter.html"
  },
  {
    "objectID": "slides/01-replication-tools/01-replication-tools.html#more-on-reproducibility",
    "href": "slides/01-replication-tools/01-replication-tools.html#more-on-reproducibility",
    "title": "Tools for reproducible research",
    "section": "More on reproducibility",
    "text": "More on reproducibility\nIn general, I highly suggest the online book The Open Science Manual https://arca-dpss.github.io/manual-open-science/ written by my friend Claudio Zandonella and Davide Massidda where these and other topics are explained in details:"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#setup",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#setup",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Setup",
    "text": "Setup\nPackages\n\nlibrary(tidyverse) # for data manipulation\nlibrary(metafor) # for meta-analysis\ndevtools::load_all() # load all utils functions"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#meta-analysis-1",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#meta-analysis-1",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Meta-analysis",
    "text": "Meta-analysis\n\nThe meta-analysis is a statistical procedure to combine evidence from a group of studies.\nIt is usually combined with a systematic review of the literature\nIs somehow the gold-standard approach when we want to summarise and make inference on a specific research area"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#effect-size",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#effect-size",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Effect size",
    "text": "Effect size\nTo compare results from different studies, we should use a common metrics. Frequently meta-analysts use standardized effect sizes. For example the Pearson correlation or the Cohen‚Äôs \\(d\\).\n\\[\nr = \\frac{\\sum{(x_i - \\bar{x})(y_i - \\bar{y})}}{\\sqrt{\\sum{(x_i - \\bar{x})^2}\\sum{(y_i - \\bar{y})^2}}}\n\\]\n\\[\nd = \\frac{\\bar{x_1} - \\bar{x_2}}{s_p}\n\\]\n\\[\ns_p = \\sqrt{\\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}}\n\\]"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#effect-size-1",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#effect-size-1",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Effect size",
    "text": "Effect size\nThe advantage of standandardized effect size is that regardless the original variable, the intepretation and the scale is the same. For example the pearson correlation ranges between -1 and 1 and the Cohen‚Äôs \\(d\\) between \\(- \\infty\\) and \\(\\infty\\) and is intepreted as how many standard deviations the two groups differs.\n\nCodeS &lt;- matrix(c(1, 0.7, 0.7, 1), nrow = 2)\nX &lt;- MASS::mvrnorm(100, c(0, 2), S, empirical = TRUE)\n\npar(mfrow = c(1,2))\nplot(X, xlab = \"x\", ylab = \"y\", cex = 1.3, pch = 19,\n     cex.lab = 1.2, cex.axis = 1.2,\n     main = latex2exp::TeX(sprintf(\"$r = %.2f$\", cor(X[, 1], X[, 2]))))\nabline(lm(X[, 2] ~ X[, 1]), col = \"firebrick\", lwd = 2)\n\n\nplot(density(X[, 1]), xlim = c(-5, 7), ylim = c(0, 0.5), col = \"dodgerblue\", lwd = 2,\n     main = latex2exp::TeX(sprintf(\"$d = %.2f$\", lsr::cohensD(X[, 1], X[, 2]))),\n     xlab = \"\")\nlines(density(X[, 2]), col = \"firebrick\", lwd = 2)"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#sec-effsize-se",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#sec-effsize-se",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Effect size sampling variability",
    "text": "Effect size sampling variability\n\n\nFormula\nPlot\n\n\n\nFor example, there are multiple methods to estimate the Cohen‚Äôs \\(d\\) sampling variability. For example:\n\\[\nV_d = \\frac{n_1 + n_2}{n_1 n_2} + \\frac{d^2}{2(n_1 + n_2)}\n\\]\nEach effect size has a specific formula for the sampling variability. The sample size is usually the most important information. Studies with high sample size have low sampling variability.\n\n\nAs the sample size grows and tends to infinity, the sampling variability approach zero."
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#notation",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#notation",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Notation",
    "text": "Notation\nMeta-analysis notation is a little bit inconsistent in textbooks and papers. We define here some rules to simplify the work.\n\n\n\\(k\\) is the number of studies\n\n\\(n_j\\) is the sample size of the group \\(j\\) within a study\n\n\\(y_i\\) are the observed effect size included in the meta-analysis\n\n\\(\\sigma_i^2\\) are the observed sampling variance of studies and \\(\\epsilon_i\\) are the sampling errors\n\n\\(\\theta\\) is the equal-effects parameter (see Equation¬†3)\n\n\\(\\delta_i\\) is the random-effect (see Equation¬†5)\n\n\\(\\mu_\\theta\\) is the average effect of a random-effects model (see Equation¬†4)\n\n\\(w_i\\) are the meta-analysis weights (e.g., see Equation¬†1)\n\n\\(\\tau^2\\) is the heterogeneity (see Equation¬†5)\n\n\\(\\Delta\\) is the (generic) population effect size\n\n\\(s_j^2\\) is the variance of the group \\(j\\) within a study"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#extra---simulating-meta-analysis",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#extra---simulating-meta-analysis",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Extra - Simulating Meta-Analysis",
    "text": "Extra - Simulating Meta-Analysis\nFor the examples and plots I‚Äôm going to use simulated data1. We simulate unstandardized effect sizes (UMD) because the computations are easier and the estimator is unbiased (e.g., Viechtbauer 2005)\nMore specifically we simulate hypothetical studies where two independent groups are compared:\n\\[\n\\Delta = \\overline{X_1} - \\overline{X_2}\n\\]\n\\[\nSE_{\\Delta} = \\sqrt{\\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2}}\n\\]\nWith \\(X_{1_i} \\sim \\mathcal{N}(0, 1)\\) and \\(X_{2_i} \\sim \\mathcal{N}(\\Delta, 1)\\)\nThe main advantage is that, compared to standardized effect size, the sampling variability do not depends on the effect size itself, simplifying the computations.\nIf you are interested in meta-analysis simulation we wrote a preprint https://psyarxiv.com/br6vy/"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#meta-analysis-as-a-weighted-average",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#meta-analysis-as-a-weighted-average",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Meta-analysis as a (weighted) average",
    "text": "Meta-analysis as a (weighted) average\nLet‚Äôs imagine to have \\(k = 10\\) studies. The segments are the 95% confidence intervals.\n\nCodek &lt;- 10\nni &lt;- round(runif(k, 10, 100))\ndat &lt;- sim_studies(k, 0.5, 0, ni, ni)\nqf &lt;- quick_forest(dat)\nqf\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n  \n\n\n  \n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\nWhat could you say about the phenomenon?"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#meta-analysis-as-a-weighted-average-1",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#meta-analysis-as-a-weighted-average-1",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Meta-analysis as a (weighted) average",
    "text": "Meta-analysis as a (weighted) average\n\n\nPlot\nFormula\n\n\n\nWe could say that the average effect is roughly ~0.5 and there is some variability around the average.\n\nCodeavg &lt;- mean(dat$yi)\nqf + geom_vline(xintercept = avg, color = \"firebrick\") +\ngeom_segment(aes(x = yi, y = id-0.3, xend = avg, yend = id-0.3),\ncolor = \"firebrick\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n  \n\n\n  \n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n\n\n\n\n\\[\n\\overline y = \\frac{\\sum_{i = 1}^k y_i}{k}\n\\]\n\nybar &lt;- mean(dat$yi)\nybar\n#&gt; [1] 0.5600977"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#meta-analysis-as-a-weighted-average-2",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#meta-analysis-as-a-weighted-average-2",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Meta-analysis as a (weighted) average",
    "text": "Meta-analysis as a (weighted) average\n\n\nPlot\nEquation\nR Code\n\n\n\nThe simple average is a good statistics. But some studies are clearly more precise (narrower confidence intervals) than others i.e.¬†the sampling variance is lower.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n  \n\n\n  \n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n\n\n\n\nWe can compute a weighted average where each study is weighted (\\(w_i\\)) by the inverse of the variance. This is called inverse-variance weighting. Clearly, a weighted average were all weights are the same reduced to a simple unweighted average.\n\\[\n\\overline y = \\frac{\\sum_{i = 1}^k y_iw_i}{\\sum_{i = 1}^k w_i}\n\\] \\[\nw_i = \\frac{1}{v_i}\n\\tag{1}\\]\n\n\n\nwi &lt;- 1/dat$vi\nywbar &lt;- weighted.mean(dat$yi, wi) \nywbar\n#&gt; [1] 0.4558081\n\nThe value is (not so) different compared to the unweighted version (0.5600977). They are not exactly the same but given that weights are pretty homogeneous the two estimate are similar."
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#meta-analysis-as-a-weighted-average-3",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#meta-analysis-as-a-weighted-average-3",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Meta-analysis as a (weighted) average",
    "text": "Meta-analysis as a (weighted) average\nWhat we did is a very simple model but actually is a meta-analysis model. This is commonly known as equal-effects model (or fixed-effect) model.\n\nCodequick_forest(dat, weigth = TRUE) +\n  geom_vline(xintercept = ywbar, color = \"firebrick\") +\n  ggtitle(\"Weighted Average\")"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#sec-ee",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#sec-ee",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Equal-effects model (EE)",
    "text": "Equal-effects model (EE)\n\n\nAssumptions\nEquations\nPlots\n\n\n\nThe EE model is the simplest meta-analysis model. The assumptions are:\n\nthere is a unique, true effect size to estimate \\(\\theta\\)\n\neach study is a more or less precise estimate of \\(\\theta\\)\n\nthere is no TRUE variability among studies. The observed variability is due to studies that are imprecise (i.e., sampling error)\n\nassuming that each study has a very large sample size, the observed variability is close to zero.\n\n\n\n\n\nFormally, we can define the EE model as: \\[\ny_i = \\theta + \\epsilon_i\n\\tag{2}\\]\n\\[\n\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2_i)\n\\tag{3}\\]\nWhere \\(\\sigma^2_i\\) is the vector of sampling variabilities of \\(k\\) studies. This is a standard linear model but with heterogeneous sampling variances."
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#sec-ee-high-low",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#sec-ee-high-low",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Equal-effects model (EE)",
    "text": "Equal-effects model (EE)\nA crucial part of the EE model is that, assuming studies with very large sample sizes, \\(\\epsilon_i\\) will tend to 0 and each study is an almost perfect estimation of \\(\\theta\\). Let‚Äôs simulate two models, with \\(k = 10\\) studies and \\(n = 30\\) and \\(n = 500\\). The effect size is the same \\(0.5\\).\n\nCodedat_low  &lt;- sim_studies(10, 0.5, 0, 30, 30)\ndat_high &lt;- sim_studies(10, 0.5, 0, 500, 500)\n\nqf_low &lt;- quick_forest(dat_low) + \n  geom_vline(xintercept = 0.5, color = \"firebrick\") +\n  xlim(c(-2, 2)) +\n  ggtitle(latex2exp::TeX(\"$n_{1,2} = 30$\"))\n\nqf_high &lt;- quick_forest(dat_high) + \n  geom_vline(xintercept = 0.5, color = \"firebrick\") +\n  xlim(c(-2, 2)) +\n  ggtitle(latex2exp::TeX(\"$n_{1,2} = 500$\"))\n\nplt_high_low &lt;- cowplot::plot_grid(\n  qf_low,\n  qf_high\n)\n\nplt_high_low"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#equal-effects-model-ee",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#equal-effects-model-ee",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Equal-effects model (EE)",
    "text": "Equal-effects model (EE)\nClearly, as \\(n\\) increase, each study is essentially a perfect estimation of \\(\\theta\\) as depicted in the theoretical figure (see slide 1.11)."
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#are-the-ee-assumptions-realistic-1",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#are-the-ee-assumptions-realistic-1",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Are the EE assumptions realistic?",
    "text": "Are the EE assumptions realistic?\nThe EE model is appropriate if our studies are somehow exact replications of the exact same effect. We are assuming that there is no real variability.\n\nHowever, meta-analysis rarely report the results of \\(k\\) exact replicates. It is more common to include studies with the same underlying objective but a roughly similar method.\n\npeople with different ages or other participant-level differences\ndifferent methodology\n‚Ä¶"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#are-the-ee-assumptions-realistic-2",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#are-the-ee-assumptions-realistic-2",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Are the EE assumptions realistic?",
    "text": "Are the EE assumptions realistic?\nIn this case, even with extremely large studies, our effect could be larger in some conditions or smaller or absent in other conditions.\nIn other terms we are assuming that there could be some variability (i.e., heterogeneity) among studies that is independent from the sample size (or more generally the precision)"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#random-effects-model-re",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#random-effects-model-re",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Random-effects model (RE)",
    "text": "Random-effects model (RE)\n\n\nTheory\nEstimation\nPlots\n\n\n\nWe can extend the EE model including another source of variability, \\(\\tau^2\\). \\(\\tau^2\\) is the true heterogeneity among studies caused by methdological differences or intrisic variability in the phenomenon.\nFormally we can extend the equation 3 as: \\[\ny_i = \\mu_{\\theta} + \\delta_i + \\epsilon_i\n\\tag{4}\\]\n\\[\n\\delta_i \\sim \\mathcal{N}(0, \\tau^2)\n\\tag{5}\\]\n\\[\n\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2_i)\n\\]\nWhere \\(\\mu_{\\theta}\\) is the average effect size and \\(\\delta_i\\) is the study-specific deviation from the average effect (regulated by \\(\\tau^2\\)).\n\n\nGiven that we extended the EE model equation. Also the estimation of the average effect need to be extended. Basically the RE is still a weighted average but weights need to include also \\(\\tau^2\\).\n\\[\n\\overline y = \\frac{\\sum_{i = 1}^k y_iw^*_i}{\\sum_{i = 1}^k w^*_i}\n\\tag{6}\\]\n\\[\nw^*_i = \\frac{1}{v_i + \\tau^2}\n\\tag{7}\\]\nThe consequence is that weights are different where extremely precise/imprecise studies will impact less the estimation of the average effect under the RE model compared to EE."
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#random-effects-model",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#random-effects-model",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Random-effects model",
    "text": "Random-effects model\nThe crucial difference with the EE model is that even with large \\(n\\), only the \\(\\mu_{\\theta} + \\delta_i\\) are estimated (almost) without error. As long \\(\\tau^2 \\neq 0\\) there will be variability in the effect sizes."
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#random-effects-model-1",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#random-effects-model-1",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Random-effects model",
    "text": "Random-effects model\nAgain, we can easily demonstrate this with a simulation. We use the same simulation as slide 1.12 but including \\(\\tau^2 = 0.2\\).\n\nCodedat_low  &lt;- sim_studies(10, 0.5, 0.2, 30, 30)\ndat_high &lt;- sim_studies(10, 0.5, 0.2, 500, 500)\n\nqf_low &lt;- quick_forest(dat_low) + \n  geom_vline(xintercept = 0.5, color = \"firebrick\") +\n  xlim(c(-3, 3)) +\n  ggtitle(latex2exp::TeX(\"$n_{1,2} = 30$\"))\n\nqf_high &lt;- quick_forest(dat_high) + \n  geom_vline(xintercept = 0.5, color = \"firebrick\") +\n  xlim(c(-3, 3)) +\n  ggtitle(latex2exp::TeX(\"$n_{1,2} = 500$\"))\n\nqf_tau_high_low &lt;- cowplot::plot_grid(\n  qf_low,\n  qf_high\n)\nqf_tau_high_low\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n  \n\n\n  \n  \n\n\n  \n\n\n  \n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n  \n\n\n  \n  \n\n\n  \n\n\n  \n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n  \n  \n\n\nEven with \\(n \\to \\infty\\) the observed variance is not reduced as long \\(\\tau^2 \\neq 0\\)."
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#extra---simulating-meta-analysis-1",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#extra---simulating-meta-analysis-1",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Extra - Simulating Meta-Analysis",
    "text": "Extra - Simulating Meta-Analysis\n\n\nEquations 1\nEquations 1\nCode\n\n\n\nFor the simulations, we can generate data from effect size and variance sampling distributions1. The unstandardized effect size is a mean difference between independent groups. The sampling distribution is:\n\\[\ny_i \\sim \\mathcal{N}(\\Delta, \\frac{1}{n_1} + \\frac{1}{n_2})\n\\]\nWhere \\(\\Delta\\) is the population level effect size and \\(n_{1,2}\\) are the sample sizes of the two studies. The sampling variances are generated from a \\(\\chi^2\\) distribution:\n\\[\n\\sigma_i^2 \\sim \\frac{\\chi^2_{n_1 + n_2 - 2}}{n_1 + n_2 - 2} (\\frac{1}{n_1} + \\frac{1}{n_2})\n\\]\n\n\nClearly we can include \\(\\tau^2\\) to include between-study variability. For an equal-effects model \\(\\Delta = \\theta\\) thus the equation is unchanged. For a random-effects model, \\(\\Delta = \\theta_i = \\mu_\\theta + \\delta_i\\)\n\\[\ny_i \\sim \\mathcal{N}(\\Delta, \\tau^2 + \\frac{1}{n_1} + \\frac{1}{n_2})\n\\]\n\n\nThe simulation is implemented in the sim_studies function:\n\nsim_studies &lt;- function(k, theta, tau2, n0, n1, summary = FALSE){\n  yi &lt;- rnorm(k, theta, sqrt(tau2 + 1/n0 + 1/n1))\n  vi &lt;- (rchisq(k, n0 + n1 - 2) / (n0 + n1 - 2)) * (1/n0 + 1/n1)\n  out &lt;- data.frame(yi, vi, sei = sqrt(vi))\n  if(summary){\n    out &lt;- summary_es(out)\n  }\n  return(out)\n}\n\n\ndat &lt;- sim_studies(k = 10, theta = 0.5, tau2 = 0.2, n0 = 30, n1 = 30)\ndat\n#&gt;             yi         vi       sei\n#&gt; 1   0.03722128 0.06456132 0.2540892\n#&gt; 2   0.86109241 0.07728935 0.2780096\n#&gt; 3  -0.20386019 0.08004369 0.2829199\n#&gt; 4   0.97094251 0.07066803 0.2658346\n#&gt; 5   0.57983381 0.06377108 0.2525294\n#&gt; 6   1.53919586 0.07696243 0.2774210\n#&gt; 7   0.19259021 0.07343220 0.2709838\n#&gt; 8   0.38608057 0.05963238 0.2441974\n#&gt; 9   1.19627080 0.07010046 0.2647649\n#&gt; 10  0.85360445 0.07567874 0.2750977\n\n\n\n\nsee https://www.jepusto.com/simulating-correlated-smds/ for a nice blog post about simulations"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#heterogeneity",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#heterogeneity",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Heterogeneity",
    "text": "Heterogeneity\nWe discussed so far about estimating the average effect (\\(\\theta\\) or \\(\\mu_\\theta\\)). But how to estimate the heterogeneity?\nThere are several estimators for \\(\\tau^2\\)\n\nHunter‚ÄìSchmidt\nHedges\nDerSimonian‚ÄìLaird\nMaximum-Likelihood\nRestricted Maximum-Likelihood (REML)\n\nWe will mainly use the REML estimator. See Viechtbauer (2005) and Veroniki et al. (2016) for more details."
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#the-q-statistics",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#the-q-statistics",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "The Q Statistics1\n",
    "text": "The Q Statistics1\n\nThe Q statistics is used to make statistical inference on the heterogeneity. Can be considered as a weighted sum of squares:\n\\[\nQ = \\sum^k_{i = 1}w_i(y_i - \\hat \\mu)^2\n\\]\nWhere \\(\\hat \\mu\\) is EE estimation (regardless if \\(\\tau^2 \\neq 0\\)) and \\(w_i\\) are the inverse-variance weights. Note that in the case of \\(w_1 = w_2 ... = w_i\\), Q is just a standard sum of squares (or deviance).\nSee Harrer et al. (2021) (Chapter 5) and Hedges and Schauer (2019) for an overview about the Q statistics"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#the-q-statistics-1",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#the-q-statistics-1",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "The Q Statistics",
    "text": "The Q Statistics\n\nGiven that we are summing up squared distances, they should be approximately \\(\\chi^2\\) with \\(df = k - 1\\). In case of no heterogeneity (\\(\\tau^2 = 0\\)) the observed variability is caused by sampling error only. The expectd value of the \\(\\chi^2\\) is just the degrees of freedom (\\(df = k - 1\\)).\nIn case of \\(\\tau^2 \\neq 0\\), the expected value is \\(k - 1 + \\lambda\\) where \\(\\lambda\\) is a non-centrality parameter.\nIn other terms, if the expected value of \\(Q\\) exceed the expected value assuming no heterogeneity, we have evidence that \\(\\tau^2 \\neq 0\\)."
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#the-q-statistics-2",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#the-q-statistics-2",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "The Q Statistics",
    "text": "The Q Statistics\nLet‚Äôs try a more practical approach. We simulate a lot of meta-analysis with and without heterogeneity and we check the Q statistics.\n\nCodeget_Q &lt;- function(yi, vi){\n  wi &lt;- 1/vi\n  theta_ee &lt;- weighted.mean(yi, wi)\n  sum(wi*(yi - theta_ee)^2)\n}\n\nk &lt;- 30\nn &lt;- 30\ntau2 &lt;- 0.1\nnsim &lt;- 1e4\n\nQs_tau2_0 &lt;- rep(0, nsim)\nQs_tau2 &lt;- rep(0, nsim)\nres2_tau2_0 &lt;- vector(\"list\", nsim)\nres2_tau2 &lt;- vector(\"list\", nsim)\n\nfor(i in 1:nsim){\n  dat_tau2_0 &lt;- sim_studies(k = 30, theta = 0.5, tau2 = 0, n0 = n, n1 = n)\n  dat_tau2 &lt;- sim_studies(k = 30, theta = 0.5, tau2 = tau2, n0 = n, n1 = n)\n  \n  theta_ee_tau2_0 &lt;- weighted.mean(dat_tau2_0$yi, 1/dat_tau2_0$vi)\n  theta_ee &lt;- weighted.mean(dat_tau2$yi, 1/dat_tau2$vi)\n  \n  res2_tau2_0[[i]] &lt;- dat_tau2_0$yi - theta_ee_tau2_0\n  res2_tau2[[i]] &lt;- dat_tau2$yi - theta_ee\n  \n  Qs_tau2_0[i] &lt;- get_Q(dat_tau2_0$yi, dat_tau2_0$vi)\n  Qs_tau2[i] &lt;- get_Q(dat_tau2$yi, dat_tau2$vi)\n}\n\ndf &lt;- k - 1\n\npar(mfrow = c(2,2))\nhist(Qs_tau2_0, probability = TRUE, ylim = c(0, 0.08), xlim = c(0, 150),\n     xlab = \"Q\",\n     main = latex2exp::TeX(\"$\\\\tau^2 = 0$\"))\ncurve(dchisq(x, df), 0, 100, add = TRUE, col = \"firebrick\", lwd = 2)\n\nhist(unlist(res2_tau2_0), probability = TRUE, main = latex2exp::TeX(\"$\\\\tau^2 = 0$\"), ylim = c(0, 2),\n     xlab = latex2exp::TeX(\"$y_i - \\\\hat{\\\\mu}$\"))\ncurve(dnorm(x, 0, sqrt(1/n + 1/n)), add = TRUE, col = \"dodgerblue\", lwd = 2)\n\nhist(Qs_tau2, probability = TRUE, ylim = c(0, 0.08), xlim = c(0, 150),\n     xlab = \"Q\",\n     main = latex2exp::TeX(\"$\\\\tau^2 = 0.1$\"))\ncurve(dchisq(x, df), 0, 100, add = TRUE, col = \"firebrick\", lwd = 2)\n\nhist(unlist(res2_tau2), probability = TRUE, main = latex2exp::TeX(\"$\\\\tau^2 = 0.1$\"), ylim = c(0, 2),\n     xlab = latex2exp::TeX(\"$y_i - \\\\hat{\\\\mu}$\"))\ncurve(dnorm(x, 0, sqrt(1/n + 1/n)), add = TRUE, col = \"dodgerblue\", lwd = 2)"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#the-q-statistics-3",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#the-q-statistics-3",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "The Q Statistics",
    "text": "The Q Statistics\nLet‚Äôs try a more practical approach. We simulate a lot of meta-analysis with and without heterogeneity and we check the Q statistics.\n\n\nclearly, in the presence of heterogeneity, the expected value of the Q statistics is higher (due to \\(\\lambda\\)) and also residuals are larger.\n\n\n\n\nwe can calculate a p-value for deviation from the \\(\\tau^2 = 0\\) case as evidence agaist the absence of heterogeneity"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#estimating-tau2",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#estimating-tau2",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Estimating \\(\\tau^2\\)\n",
    "text": "Estimating \\(\\tau^2\\)\n\nThe Q statistics is rarely used to directly represent the heterogeneity. The raw measure of heterogeneity is \\(\\tau^2\\). The REML (restricted maximum likelihood) estimator is often used.\n\\[\n\\hat \\tau^2 = \\frac{\\sum_{i = 1}^k w_i^2[(y_i - \\hat \\mu)^2 - \\sigma^2_i]}{\\sum_{i = 1}^k w_i^2} + \\frac{1}{\\sum_{i = 1}^k w_i}\n\\]\nWhere \\(\\hat{\\mu}\\) is the weighted average (i.e., maximum likelihood estimation, see Equation¬†6 and Equation¬†7)."
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#tau2-as-heterogeneity-measure",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#tau2-as-heterogeneity-measure",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "\n\\(\\tau^2\\) as heterogeneity measure",
    "text": "\\(\\tau^2\\) as heterogeneity measure\n\n\n\\(\\tau^2\\) is the direct measure of heterogeneity in meta-analysis\nit is intepreted as a standard deviation (or variance) of the distribution of true effects\na phenomenon associated with an higher \\(\\tau^2\\) is interpred as more variable"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#tau2-as-heterogeneity-measure-1",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#tau2-as-heterogeneity-measure-1",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "\n\\(\\tau^2\\) as heterogeneity measure",
    "text": "\\(\\tau^2\\) as heterogeneity measure\n\nCode# we generate 1e5 values from a random-effect model with certain parameters and different tau2 values\n# and check the expected distrubution of effect sizes\n\nn &lt;- 100\nk &lt;- 1e5\ntau2 &lt;- c(0, 0.1, 0.2, 0.5)\n\ndats &lt;- lapply(tau2, function(x) sim_studies(k, 0.5, x, n, n))\nnames(dats) &lt;- tau2\n\ndat &lt;- bind_rows(dats, .id = \"tau2\") \ndat$tau2 &lt;- factor(dat$tau2)\ndat$tau2 &lt;- factor(dat$tau2, labels = latex2exp::TeX(sprintf(\"$\\\\tau^2 = %s$\", levels(dat$tau2))))\n\ndat |&gt; \n  ggplot(aes(x = yi, y = after_stat(density))) +\n  geom_histogram(bins = 50) +\n  geom_vline(xintercept = 0.5, lwd = 0.5, color = \"firebrick\") +\n  facet_wrap(~tau2, labeller = label_parsed) +\n  ggtitle(latex2exp::TeX(\"Expected $y_i$ distribution, $d = 0.5$, $n_{1,2} = 100$\")) +\n  xlab(latex2exp::TeX(\"$y_i$\"))"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#i2-higgins2002-fh",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#i2-higgins2002-fh",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "\n\\(I^2\\) (Higgins and Thompson 2002)\n",
    "text": "\\(I^2\\) (Higgins and Thompson 2002)\n\nWe have two sources of variability in a random-effects meta-analysis, the sampling variabilities \\(\\sigma_i^2\\) and \\(\\tau^2\\). We can use the \\(I^2\\) to express the interplay between the two. \\[\nI^2 = 100\\% \\times \\frac{\\hat{\\tau}^2}{\\hat{\\tau}^2 + \\tilde{v}}\n\\tag{8}\\]\n\\[\n\\tilde{v} = \\frac{(k-1) \\sum w_i}{(\\sum w_i)^2 - \\sum w_i^2},\n\\]\nWhere \\(\\tilde{v}\\) is the typical sampling variability. \\(I^2\\) is intepreted as the proportion of total variability due to real heterogeneity (i.e., \\(\\tau^2\\))"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#i2-higgins2002-fh-1",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#i2-higgins2002-fh-1",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "\n\\(I^2\\) (Higgins and Thompson 2002)1\n",
    "text": "\\(I^2\\) (Higgins and Thompson 2002)1\n\nNote that we can have the same \\(I^2\\) in two completely different meta-analysis. An high \\(I^2\\) does not represent high heterogeneity. Let‚Äôs assume to have two meta-analysis with \\(k\\) studies and small (\\(n = 30\\)) vs large (\\(n = 500\\)) sample sizes.\nLet‚Äôs solve Equation¬†8 for \\(\\tau^2\\) (using filor::tau2_from_I2()) and we found that the same \\(I^2\\) can be obtained with two completely different \\(\\tau^2\\) values:\n\nn_1 &lt;- 30\nvi_1 &lt;- 1/n_1 + 1/n_1\ntau2_1 &lt;- filor::tau2_from_I2(0.8, vi_1)\ntau2_1\n#&gt; [1] 0.2666667\n\nn_2 &lt;- 500\nvi_2 &lt;- 1/n_2 + 1/n_2\ntau2_2 &lt;- filor::tau2_from_I2(0.8, vi_2)\ntau2_2\n#&gt; [1] 0.016\n\nsee https://www.meta-analysis-workshops.com/download/common-mistakes1.pdf"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#i2-higgins2002-fh-2",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#i2-higgins2002-fh-2",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "\n\\(I^2\\) (Higgins and Thompson 2002)\n",
    "text": "\\(I^2\\) (Higgins and Thompson 2002)\n\n\nn_1 &lt;- 30\nvi_1 &lt;- 1/n_1 + 1/n_1\ntau2_1 &lt;- filor::tau2_from_I2(0.8, vi_1)\ntau2_1\n#&gt; [1] 0.2666667\n\nn_2 &lt;- 500\nvi_2 &lt;- 1/n_2 + 1/n_2\ntau2_2 &lt;- filor::tau2_from_I2(0.8, vi_2)\ntau2_2\n#&gt; [1] 0.016\n\n\nIn other terms, the \\(I^2\\) can be considered a good index of heterogeneity only when the total variance (\\(\\tilde{v} + \\tau^2\\)) is the same."
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#meta-analysis-in-r-1",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#meta-analysis-in-r-1",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Meta-analysis in R",
    "text": "Meta-analysis in R\nIn R there are several packages to conduct a meta-analysis. For me, the best package is metafor (Viechtbauer 2010). The package support all steps in meta-analysis providing also an amazing documentation:"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#equal-effects-model-in-r",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#equal-effects-model-in-r",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Equal-effects model in R",
    "text": "Equal-effects model in R\nDisclaimer: we are omitting the important part of collecting the information from published studies and calculating the (un)standardized effect sizes. Clerly this part is highly dependent on the actual dataset.\nThere are few useful resources:\n\nChapters 1-9 from Borestein et al. (2009)\n\nThe metafor::escalc() function that calculate all effects size and report an useful documentation"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#equal-effects-model-in-r-1",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#equal-effects-model-in-r-1",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Equal-effects model in R",
    "text": "Equal-effects model in R\nWe can simulate an EE dataset (i.e., \\(\\tau^2 = 0\\)) and then fit the model with metafor:\n\ntheta &lt;- 0.3\nk &lt;- 30 # number of studies\nn &lt;- round(runif(k, 10, 60)) # random sample sizes with a plausible range\ndat &lt;- sim_studies(k = k, theta = theta, tau2 = 0, n0 = n, n1 = n)\ndat$n &lt;- n # n1 and n2 are the same, put only one\nhead(dat)\n#&gt;           yi         vi       sei  n\n#&gt; 1  0.5006860 0.05474557 0.2339777 33\n#&gt; 2  0.8160800 0.12421999 0.3524486 17\n#&gt; 3  0.7038806 0.11651275 0.3413396 17\n#&gt; 4  0.3023013 0.14427791 0.3798393 19\n#&gt; 5 -0.1286906 0.14727102 0.3837591 18\n#&gt; 6  0.5989998 0.03710777 0.1926338 49"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#equal-effects-model-in-r-2",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#equal-effects-model-in-r-2",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Equal-effects model in R",
    "text": "Equal-effects model in R\nWe can start by some summary statistics:\n\n# unweighted summary statistics for the effect\nsummary(dat$yi)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt; -0.1602  0.1105  0.2959  0.3077  0.5019  0.8161\n\n# unweighted summary statistics for the variances\nsummary(dat$vi)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt; 0.03592 0.04446 0.06456 0.07630 0.09004 0.21512\n\n# sample sizes\nsummary(dat$n) # n0 and n1 are the same\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;   14.00   19.25   32.00   32.70   43.75   60.00"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#equal-effects-model-in-r-3",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#equal-effects-model-in-r-3",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Equal-effects model in R",
    "text": "Equal-effects model in R\nThen we can use the metafor::rma.uni() function (or more simply metafor::rma()) providing the effect size, variances and method = \"EE\" to specify that we are fitting an equal-effects model.\n\nfit_ee &lt;- rma(yi, vi, method = \"EE\", data = dat)\nsummary(fit_ee)\n#&gt; \n#&gt; Equal-Effects Model (k = 30)\n#&gt; \n#&gt;   logLik  deviance       AIC       BIC      AICc   \n#&gt;  -0.7274   27.3050    3.4547    4.8559    3.5976   \n#&gt; \n#&gt; I^2 (total heterogeneity / total variability):   0.00%\n#&gt; H^2 (total variability / sampling variability):  0.94\n#&gt; \n#&gt; Test for Heterogeneity:\n#&gt; Q(df = 29) = 27.3050, p-val = 0.5553\n#&gt; \n#&gt; Model Results:\n#&gt; \n#&gt; estimate      se    zval    pval   ci.lb   ci.ub      \n#&gt;   0.3046  0.0449  6.7781  &lt;.0001  0.2166  0.3927  *** \n#&gt; \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#equal-effects-model-in-r-4",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#equal-effects-model-in-r-4",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Equal-effects model in R",
    "text": "Equal-effects model in R\nWe can easily show that the results is the same as performing a simple weighted average using study-specific variances as weight:\n\nwi &lt;- 1/dat$vi\n\nweighted.mean(dat$yi, wi)\n#&gt; [1] 0.3046422\n\nsummary(fit_ee)\n#&gt; \n#&gt; Equal-Effects Model (k = 30)\n#&gt; \n#&gt;   logLik  deviance       AIC       BIC      AICc   \n#&gt;  -0.7274   27.3050    3.4547    4.8559    3.5976   \n#&gt; \n#&gt; I^2 (total heterogeneity / total variability):   0.00%\n#&gt; H^2 (total variability / sampling variability):  0.94\n#&gt; \n#&gt; Test for Heterogeneity:\n#&gt; Q(df = 29) = 27.3050, p-val = 0.5553\n#&gt; \n#&gt; Model Results:\n#&gt; \n#&gt; estimate      se    zval    pval   ci.lb   ci.ub      \n#&gt;   0.3046  0.0449  6.7781  &lt;.0001  0.2166  0.3927  *** \n#&gt; \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#random-effects-model-in-r",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#random-effects-model-in-r",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Random-effects model in R",
    "text": "Random-effects model in R\nThe syntax for a random-effects model is the same, we just need to use method = \"REML\" (or another \\(\\tau^2\\) estimation method)\n\nfit_re &lt;- rma(yi, vi, method = \"REML\", data = dat)\nsummary(fit_re)\n#&gt; \n#&gt; Random-Effects Model (k = 30; tau^2 estimator: REML)\n#&gt; \n#&gt;   logLik  deviance       AIC       BIC      AICc   \n#&gt;  -1.2101    2.4203    6.4203    9.1549    6.8818   \n#&gt; \n#&gt; tau^2 (estimated amount of total heterogeneity): 0 (SE = 0.0148)\n#&gt; tau (square root of estimated tau^2 value):      0\n#&gt; I^2 (total heterogeneity / total variability):   0.00%\n#&gt; H^2 (total variability / sampling variability):  1.00\n#&gt; \n#&gt; Test for Heterogeneity:\n#&gt; Q(df = 29) = 27.3050, p-val = 0.5553\n#&gt; \n#&gt; Model Results:\n#&gt; \n#&gt; estimate      se    zval    pval   ci.lb   ci.ub      \n#&gt;   0.3046  0.0449  6.7781  &lt;.0001  0.2166  0.3927  *** \n#&gt; \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#random-effects-model-in-r-1",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#random-effects-model-in-r-1",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Random-effects model in R",
    "text": "Random-effects model in R\nWe can easily compare the models using the compare_rma() function:\n\nfilor::compare_rma(fit_ee, fit_re)  |&gt;\n  round(5)\n#&gt;        fit_ee  fit_re\n#&gt; b     0.30464 0.30464\n#&gt; se    0.04495 0.04495\n#&gt; zval  6.77808 6.77808\n#&gt; pval  0.00000 0.00000\n#&gt; ci.lb 0.21655 0.21655\n#&gt; ci.ub 0.39273 0.39273\n#&gt; I2    0.00000 0.00000\n#&gt; tau2  0.00000 0.00000\n\nWhat do you think? are there differences between the two models? What could be the reasons?"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#random-effects-model-in-r-2",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#random-effects-model-in-r-2",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Random-effects model in R",
    "text": "Random-effects model in R\nClearly, given that we simulated \\(\\tau^2 = 0\\) the RE model results is very close (if not equal) to the EE model. We can simulate a RE model:\n\ntau2 &lt;- 0.2\ndat &lt;- sim_studies(k = k, theta = theta, tau2 = tau2, n0 = n, n1 = n)\n\nfit_re &lt;- rma(yi, vi, method = \"REML\", data = dat)\nsummary(fit_re)\n#&gt; \n#&gt; Random-Effects Model (k = 30; tau^2 estimator: REML)\n#&gt; \n#&gt;   logLik  deviance       AIC       BIC      AICc   \n#&gt; -26.0957   52.1914   56.1914   58.9260   56.6529   \n#&gt; \n#&gt; tau^2 (estimated amount of total heterogeneity): 0.2855 (SE = 0.0937)\n#&gt; tau (square root of estimated tau^2 value):      0.5343\n#&gt; I^2 (total heterogeneity / total variability):   82.53%\n#&gt; H^2 (total variability / sampling variability):  5.72\n#&gt; \n#&gt; Test for Heterogeneity:\n#&gt; Q(df = 29) = 167.7055, p-val &lt; .0001\n#&gt; \n#&gt; Model Results:\n#&gt; \n#&gt; estimate      se    zval    pval   ci.lb   ci.ub     \n#&gt;   0.3436  0.1093  3.1430  0.0017  0.1293  0.5578  ** \n#&gt; \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#forest-plot",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#forest-plot",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Forest Plot",
    "text": "Forest Plot\nThe most common plot to represent meta-analysis results is the forest plot:\n\nforest(fit_re)"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#forest-plot-1",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#forest-plot-1",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Forest Plot",
    "text": "Forest Plot\nThe most common plot to represent meta-analysis results is the forest plot:\n\nThe x axis represent the effect size\n\nThe y axis represent the studies\n\nThe size of the square is the weight of that study (\\(w_i = 1/\\sigma_i^2\\))\nThe interval is the 95% confidence interval\n\nThe diamond is the estimated effect and the 95% confidence interval"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#forest-plot-2",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#forest-plot-2",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Forest Plot",
    "text": "Forest Plot\nIt is also common to plot studies ordering by the size of the effect, showing asymmetries or other patterns:\n\nforest(fit_re, order = \"yi\")"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#multilab-studies-1",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#multilab-studies-1",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Multilab Studies",
    "text": "Multilab Studies\nSo far we reasoned about collecting published studies and conducting a meta-analysis. Despite pooling evidence from multiple studies there are several limitations:\n\n\npublication bias\n\n\n\n\nsmall number of studies (\\(k\\)) thus low power and low estimation precision\n\n\n\n\nthe estimated \\(\\tau^2\\) could be high because the methdological heterogeneity is high ‚Äì&gt; each research group use a different methodology"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#multilab-studies-2",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#multilab-studies-2",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Multilab Studies",
    "text": "Multilab Studies\nWith multilab studies we define a new data collection where different research group conduct an experiment with a similar (or the exact same) methodology. In this way we can:\n\n\neliminate the publication bias\n\n\n\n\n\n\ncalibrate the number of studies (\\(k\\)) and observations (\\(n\\)) according to our criteria (power, precision, etc.)"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#multilab-studies-and-meta-analysis",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#multilab-studies-and-meta-analysis",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Multilab Studies and meta-analysis",
    "text": "Multilab Studies and meta-analysis\nFrom a statistical point of view, the only difference is the source of the data (planned and collected vs collected from the literature). In fact, a multilab study can be analyzed also using standard meta-analysis tools.\n\nWhen intepreting the results we could highlight some differences:\n\n\n\n\n\\(\\tau^2\\) in standard meta-analysis is considered the true variability of the phenomenon. In multilab studies (assuming an exact replication approach) should be low or close to zero.\n\n\n\n\nIn standard meta-analysis we could try to explain \\(\\tau^2\\) using meta-regression (e.g., putting the average participant age as predictor) while in multilab studies we could plan to use the same age thus removing the age effect."
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#extra---planning-a-multilab-study",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#extra---planning-a-multilab-study",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Extra - Planning a Multilab Study",
    "text": "Extra - Planning a Multilab Study\nLet‚Äôs imagine to plan a multilab study with the same setup as the previous meta-analysis. We are not collecting data from the literature but we want to know the number of studies \\(k\\) that we need to achieve e.g.¬†a good statistical Power.\n\nWe define power as the probability of correctly rejecting the null hypothesis of no effect when the effect is actually present"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#extra---planning-a-multilab-study-1",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#extra---planning-a-multilab-study-1",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Extra - Planning a Multilab Study",
    "text": "Extra - Planning a Multilab Study\nWe can do a Power analysis using Monte Carlo simulations:\n\nsimulate a meta-analysis given a set of parameters\nfit the meta-analysis model\nextract the p-value\nrepeat the steps 1-3 a lot of times e.g.¬†10000\ncount the number of significant (e.g., \\(\\leq \\alpha\\)) p-values"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#extra---planning-a-multilab-study-2",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#extra---planning-a-multilab-study-2",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Extra - Planning a Multilab Study",
    "text": "Extra - Planning a Multilab Study\nWe can implement a simple simulation as:\n\nlibrary(metafor)\nlibrary(ggplot2)\n\n# set.seed(2023)\n\nk &lt;- seq(10, 100, 10)\nn &lt;- seq(10, 100, 20)\nes &lt;- 0.3\ntau2 &lt;- 0.1\nalpha &lt;- 0.05\nnsim &lt;- 1e3\n\nsim &lt;- tidyr::expand_grid(k, n, es, tau2)\nsim$power &lt;- 0\n\n# handle errors, return NA\nsrma &lt;- purrr::possibly(rma, otherwise = NA)\n\nfor(i in 1:nrow(sim)){ # iterate for each condition\n  pval &lt;- rep(0, nsim)\n  for(j in 1:nsim){ # iterate for each simulation\n    dat &lt;- sim_studies(k = sim$k[i], \n                       theta = sim$es[i], \n                       tau2 = sim$tau2[i],\n                       n0 = sim$n[i],\n                       n1 = sim$n[i])\n    fit &lt;- srma(yi, vi, data = dat, method = \"REML\")\n    pval[j] &lt;- fit$pval\n  }\n  sim$power[i] &lt;- mean(pval &lt;= alpha) # calculate power\n  filor::pb(nrow(sim), i)\n}\n\nsaveRDS(sim, here(\"04-meta-analysis/objects/power-example.rds\"))"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#extra---planning-a-multilab-study-3",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#extra---planning-a-multilab-study-3",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Extra - Planning a Multilab Study",
    "text": "Extra - Planning a Multilab Study\nThen we can plot the results:\n\nCodesim &lt;- readRDS(here(\"slides/03-meta-analysis/objects/power-example.rds\"))\n\ntitle &lt;- \"$\\\\mu_\\\\theta = 0.3$, $\\\\tau^2 = 0.1$, $\\\\alpha = 0.05\"\n\nsim |&gt; \n  ggplot(aes(x = k, y = power, group = n, color = factor(n))) +\n  geom_line(lwd = 1) +\n  xlab(\"Number of Studies (k)\") +\n  ylab(\"Power\") +\n  labs(color = \"Sample Size\") +\n  theme(legend.position = \"bottom\") +\n  ggtitle(latex2exp::TeX(title))"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#extra---planning-a-multilab-study-4",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#extra---planning-a-multilab-study-4",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Extra - Planning a Multilab Study",
    "text": "Extra - Planning a Multilab Study\nThis is a flexible way to simulate and plan a multilab study:\n\nWe could prefer increasing \\(k\\) (i.e., the research units) and limiting \\(n\\) thus reducing the effort for each lab. The downside are difficulties in managing multiple labs, increased dropout rate, difficulty in reaching the planned \\(k\\)\n\nWe could prefer increasing \\(n\\) and limiting \\(k\\). Each lab need to put more resources but the overall project could be easier.\nWe could try to estimate a cost function according to several parameters and find the best trade-off as implemented in Hedges and Schauer (2021)"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#publication-bias-pb-1",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#publication-bias-pb-1",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Publication Bias (PB)",
    "text": "Publication Bias (PB)\nThe PB is one of the most problematic aspects of meta-analysis. Essentially the probability of publishing a paper (~and thus including into the meta-analysis) is not the same regardless the result. Clearly we could include also the gray literature to mitigate the problem."
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#publication-bias-disclaimer",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#publication-bias-disclaimer",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Publication Bias Disclaimer!",
    "text": "Publication Bias Disclaimer!\nWe cannot (completely) solve the PB using statistical tools. The PB is a problem related to the publishing process and publishing incentives (significant results are more catchy and easy to explain).\n\n\n\npre-registrations, multi-lab studies, etc. can (almost) completely solve the problem filling the literature with unbiased studies (at least from the publishing point of view)\n\n\n\n\nthere are statistical tools to detect, estimate and correct for the publication bias. As every statistical method, they are influenced by statistical assumptions, number of studies and sample size, heterogeneity, etc."
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#publication-bias-pb---the-big-picture",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#publication-bias-pb---the-big-picture",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Publication Bias (PB) - The Big Picture1\n",
    "text": "Publication Bias (PB) - The Big Picture1\n\n\n\n\nPublication BiasVisual InspectionRobustnessDetectionCorrectionFail-safe NFunnel PlotRegression MethodsTrim and FillSelection Models\n\n\n\n\nThanks to the Wolfgang Viechtbauer‚Äôs course https://www.wvbauer.com/doku.php/course_ma"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#publication-bias-pb---funnel-plot",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#publication-bias-pb---funnel-plot",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Publication Bias (PB) - Funnel Plot",
    "text": "Publication Bias (PB) - Funnel Plot\nThe first tool is called funnel plot. This is a visual tool to check the presence of asymmetry that could be caused by publication bias. If meta-analysis assumptions are respected, and there is no publication bias:\n\neffects should be normally distributed around the average effect\nmore precise studies should be closer to the average effect\nless precise studies could be equally distributed around the average effect\n\nLet‚Äôs simulate a lot of studies to show:\n\nset.seed(2023)\nk &lt;- 1e3\nn &lt;- round(runif(k, 10, 200))\ndat &lt;- sim_studies(k = k, theta = 0.5, tau2 = 0, n0 = n, n1 = n)\nfit &lt;- rma(yi, vi, method = \"EE\", data = dat)"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#publication-bias-pb---funnel-plot-1",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#publication-bias-pb---funnel-plot-1",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Publication Bias (PB) - Funnel Plot",
    "text": "Publication Bias (PB) - Funnel Plot\nLet‚Äôs plot the distribution of the data:\n\nhist(dat$yi)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n  \n  \n  \n  \n  \n  \n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n\n\n\n\n\n\n  \n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\nThe distribution is clearly normal and centered on the true simulated value. Now let‚Äôs add the precision (in this case standard error thus \\(\\sqrt{\\sigma_i^2}\\)) on the y-axis."
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#publication-bias-pb---funnel-plot-2",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#publication-bias-pb---funnel-plot-2",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Publication Bias (PB) - Funnel Plot",
    "text": "Publication Bias (PB) - Funnel Plot\nNote that the y axis is reversed so high-precise studies (\\(\\sqrt{\\sigma_i^2}\\) close to 0) are on top.\n\nCodeplot(dat$yi, dat$sei, ylim = rev(range(dat$sei)),\n     xlab = latex2exp::TeX(\"$y_i$\"),\n     ylab = latex2exp::TeX(\"$\\\\sqrt{\\\\sigma_i^2}$\"),\n     pch = 19,\n     col = scales::alpha(\"firebrick\", 0.5))\nabline(v = fit$b[[1]], col = \"black\", lwd = 1.2)"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#publication-bias-pb---funnel-plot-3",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#publication-bias-pb---funnel-plot-3",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Publication Bias (PB) - Funnel Plot",
    "text": "Publication Bias (PB) - Funnel Plot\nThe plot assume the typical funnel shape and there are not missing spots on the at the bottom. The presence of missing spots is a potential index of publication bias."
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#publication-bias-pb---funnel-plot-4",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#publication-bias-pb---funnel-plot-4",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Publication Bias (PB) - Funnel Plot",
    "text": "Publication Bias (PB) - Funnel Plot\nThe plot assume the typical funnel shape and there are not missing spots on the at the bottom. The presence of missing spots is a potential index of publication bias."
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#robustness-to-pb---fail-safe-n",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#robustness-to-pb---fail-safe-n",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Robustness to PB - Fail-safe N",
    "text": "Robustness to PB - Fail-safe N\nThe Fail-safe N (Rosenthal 1979) idea is very simple. Given a meta-analysis with a significant result (i.e., \\(p \\leq \\alpha\\)). How many null studies (i.e., \\(\\hat \\theta = 0\\)) do I need to obtain \\(p &gt; \\alpha\\)?\n\nmetafor::fsn(yi, vi, data = dat)\n#&gt; \n#&gt; Fail-safe N Calculation Using the Rosenthal Approach\n#&gt; \n#&gt; Observed Significance Level: &lt;.0001\n#&gt; Target Significance Level:   0.05\n#&gt; \n#&gt; Fail-safe N: 4396757"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#robustness-to-pb---fail-safe-n-1",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#robustness-to-pb---fail-safe-n-1",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Robustness to PB - Fail-safe N",
    "text": "Robustness to PB - Fail-safe N\nThere are several criticism to the Fail-safe N procedure:\n\n\nis not actually detecting the PB but suggesting the required PB size to remove the effect. A very large N suggest that even with PB, it is unlikely that the results could be completely changed by actually reporting null studies\n\n\n\n\n\nOrwin (1983) proposed a new method calculating the number of studies required to reduce the effect size to a given target\n\n\n\n\n\nRosenberg (2005) proposed a method similar to Rosenthal (1979) but combining the (weighted) effect sizes and not the p-values."
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#detecting-pb---egger-regression",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#detecting-pb---egger-regression",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Detecting PB - Egger Regression",
    "text": "Detecting PB - Egger Regression\nA basic method to test the funnel plot asymmetry is using an the Egger regression test. Basically we calculate the relationship between \\(y_i\\) and \\(\\sqrt{\\sigma^2_i}\\). In the absence of asimmetry, the line slope should be not different from 0.\nWe can use the metafor::regtest() function:\n\negger &lt;- regtest(fit)\negger\n#&gt; \n#&gt; Regression Test for Funnel Plot Asymmetry\n#&gt; \n#&gt; Model:     fixed-effects meta-regression model\n#&gt; Predictor: standard error\n#&gt; \n#&gt; Test for Funnel Plot Asymmetry: z = 1.0847, p = 0.2781\n#&gt; Limit Estimate (as sei -&gt; 0):   b = 0.4821 (CI: 0.4521, 0.5122)"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#publication-bias-pb---egger-regression",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#publication-bias-pb---egger-regression",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Publication Bias (PB) - Egger Regression",
    "text": "Publication Bias (PB) - Egger Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n\n\n\n\n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n  \n\n\n  \n\n\n\n  \n\n\n  \n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n\n\n\n\n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n  \n\n\n  \n\n\n\n\n\n  \n\n\n  \n\n\n\n  \n\n\n\n\n\n\nThis is a standard (meta) regression thus the number of studies, the precision of each study and heterogeneity influence the reliability (power, type-1 error rate, etc.) of the procedure."
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#correcting-pb---trim-and-fill",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#correcting-pb---trim-and-fill",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Correcting PB - Trim and Fill",
    "text": "Correcting PB - Trim and Fill\nThe Trim and Fill method (Duval and Tweedie 2000) is used to impute the hypothetical missing studies according to the funnel plot and recomputing the meta-analysis effect. Shi and Lin (Shi and Lin 2019) provide an updated overview of the method with some criticisms.\n\nLet‚Äôs simulate again a publication bias with \\(k = 100\\) studies:\n\nset.seed(2023)\nk &lt;- 100 # we increased k to better show the effect\ntheta &lt;- 0.5\ntau2 &lt;- 0.1\n\ndat &lt;- sim_pub_bias(selmodel = list(method = \"2step\", param = \"pval\", th = 0.05, side = \"&lt;=\"), k = k, theta = theta, tau2 = tau2, nmin = 10, nmax = 200)\nfit &lt;- rma(yi, vi, data = dat, method = \"REML\")"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#correcting-pb---trim-and-fill-1",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#correcting-pb---trim-and-fill-1",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Correcting PB - Trim and Fill",
    "text": "Correcting PB - Trim and Fill\nNow we can use the metafor::trimfill() function:\n\ntaf &lt;- metafor::trimfill(fit)\ntaf\n#&gt; \n#&gt; Estimated number of missing studies on the left side: 29 (SE = 6.4513)\n#&gt; \n#&gt; Random-Effects Model (k = 129; tau^2 estimator: REML)\n#&gt; \n#&gt; tau^2 (estimated amount of total heterogeneity): 0.1382 (SE = 0.0203)\n#&gt; tau (square root of estimated tau^2 value):      0.3718\n#&gt; I^2 (total heterogeneity / total variability):   88.56%\n#&gt; H^2 (total variability / sampling variability):  8.74\n#&gt; \n#&gt; Test for Heterogeneity:\n#&gt; Q(df = 128) = 1073.8812, p-val &lt; .0001\n#&gt; \n#&gt; Model Results:\n#&gt; \n#&gt; estimate      se     zval    pval   ci.lb   ci.ub      \n#&gt;   0.4851  0.0357  13.5951  &lt;.0001  0.4152  0.5551  *** \n#&gt; \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#correcting-pb---trim-and-fill-2",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#correcting-pb---trim-and-fill-2",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Correcting PB - Trim and Fill",
    "text": "Correcting PB - Trim and Fill\nThe trim-and-fill estimates that 29 are missing. The new effect size after including the studies is reduced and closer to the simulated value (but in this case still significant)."
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#correcting-pb---trim-and-fill-3",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#correcting-pb---trim-and-fill-3",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Correcting PB - Trim and Fill",
    "text": "Correcting PB - Trim and Fill\nWe can also visualize the funnel plot highligting the points that are included by the method.\n\nfunnel(taf)\n\n\nCodefunnel(taf)\negg &lt;- regtest(fit)\negg_npb &lt;- regtest(taf)\nse &lt;- seq(0,1.8,length=100)\nlines(coef(egg$fit)[1] + coef(egg$fit)[2]*se, se, lwd=3, col = \"black\")\nlines(coef(egg_npb$fit)[1] + coef(egg_npb$fit)[2]*se, se, lwd=3, col = \"firebrick\")\nlegend(\"topleft\", legend = c(\"Original\", \"Corrected\"), fill = c(\"black\", \"firebrick\"))"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#correcting-pb---selection-models-sm",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#correcting-pb---selection-models-sm",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Correcting PB - Selection Models (SM)",
    "text": "Correcting PB - Selection Models (SM)\n\n\nSM assume a relationship between the p-value and the probability of publishing\n\n\n\n\n\nSM estimate this relationship from available studies and correct the average effect\n\n\n\n\n\n\nThe models are complicated (number of parameters) and need a large \\(k\\)\n\n\n\n\n\n\n\nThey provide a very elegant framework to formalize the publication bias supporting simulations and methods development"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#sm---publication-bias-function",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#sm---publication-bias-function",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "SM - Publication Bias Function",
    "text": "SM - Publication Bias Function\n\nThe publication bias can be formalized using a weight function that assign a probability to a certain study properties (e.g., p-value, sample size, z-score, etc.) representing the likelihood of that study being published.\n\n\n\nThe general idea (e.g., Citkowicz and Vevea 2017) is to use a weighted probability density function (wPDF). In the presence of publication bias, the parameters of the wPDF will be different (i.e., adjusted) compared to unweighted PDF (i.e., assuming no publication bias)"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#sm---publication-bias-function-1",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#sm---publication-bias-function-1",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "SM - Publication Bias Function",
    "text": "SM - Publication Bias Function\nThe random-effect meta-analysis PDF can be written as (e.g., Citkowicz and Vevea 2017):\n\\[\nf\\left(y_i \\mid \\beta, \\tau^2 ; \\sigma_i^2\\right)=\\phi\\left(\\frac{y_i-\\Delta_i}{\\sqrt{\\sigma_i^2+\\tau^2}}\\right) / \\sqrt{\\sigma_i^2+\\tau^2},\n\\]\nAnd adding the weight function:\n\\[\nf\\left(Y_i \\mid \\beta, \\tau^2 ; \\sigma_i^2\\right)=\\frac{\\mathrm{w}\\left(p_i\\right) \\phi\\left(\\frac{y_i-\\Delta_i}{\\sqrt{\\sigma_i^2+\\tau^2}}\\right) / \\sqrt{\\sigma_i^2+\\tau^2}}{\\int_{-\\infty}^{\\infty} \\mathrm{w}\\left(p_i\\right) \\phi\\left(\\frac{Y_i-\\Delta_i}{\\sqrt{\\sigma_i^2+\\tau^2}}\\right) / \\sqrt{\\sigma_i^2+\\tau^2} d Y_i}\n\\]"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#sec-pub-bias-fun",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#sec-pub-bias-fun",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "SM - Publication Bias Function",
    "text": "SM - Publication Bias Function\nFor example, Citkowicz and Vevea (2017) proposed a model using a weight function based on the Beta distribution with two parameters \\(a\\) and \\(b\\)1 \\(w(p_i) = p_i^{a - 1} \\times (1 - p_i)^{b - 1}\\):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n\n\n\n\n\n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n\n  \n  \n  \n  \n  \n  \n  \n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n\n\nhttps://www.youtube.com/watch?v=ucmOCuyCk-c"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#selection-models",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#selection-models",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Selection Models",
    "text": "Selection Models\nIn R we can use the metafor::selmodel() function to implement several type of models. For example we can apply the Citkowicz and Vevea (2017) model:\n\nsel_beta &lt;- selmodel(fit, type = \"beta\")\n\n\n\nResults\nPlot\n\n\n\n\n#&gt; \n#&gt; Random-Effects Model (k = 100; tau^2 estimator: ML)\n#&gt; \n#&gt; tau^2 (estimated amount of total heterogeneity): 0.0857 (SE = 0.0255)\n#&gt; tau (square root of estimated tau^2 value):      0.2928\n#&gt; \n#&gt; Test for Heterogeneity:\n#&gt; LRT(df = 1) = 293.8104, p-val &lt; .0001\n#&gt; \n#&gt; Model Results:\n#&gt; \n#&gt; estimate      se    zval    pval   ci.lb   ci.ub      \n#&gt;   0.4847  0.1020  4.7521  &lt;.0001  0.2848  0.6846  *** \n#&gt; \n#&gt; Test for Selection Model Parameters:\n#&gt; LRT(df = 2) = 14.3271, p-val = 0.0008\n#&gt; \n#&gt; Selection Model Results:\n#&gt; \n#&gt;          estimate      se     zval    pval   ci.lb   ci.ub     \n#&gt; delta.1    0.8174  0.0700  -2.6075  0.0091  0.6802  0.9547  ** \n#&gt; delta.2    0.7017  0.2009  -1.4846  0.1376  0.3080  1.0955     \n#&gt; \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nplot(sel_beta)"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#selection-models-1",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#selection-models-1",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "Selection Models",
    "text": "Selection Models\nLet‚Äôs try the Beta selection model without publication bias:\n\nset.seed(2023)\ndat &lt;- sim_studies(30, 0.5, 0, 30, 30)\nfit &lt;- rma(yi, vi, data = dat, method = \"ML\")\nsel_beta &lt;- selmodel(fit, type = \"beta\")\nplot(sel_beta)"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#more-on-sm-and-publication-bias",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#more-on-sm-and-publication-bias",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "More on SM and Publication Bias",
    "text": "More on SM and Publication Bias\n\nThe SM documentation of metafor::selmodel() https://wviechtb.github.io/metafor/reference/selmodel.html\n\nWolfgang Viechtbauer overview of PB https://www.youtube.com/watch?v=ucmOCuyCk-c\n\n\nHarrer et al. (2021) - Doing Meta-analysis in R - Chapter 9\n\n\nMcShane, B√∂ckenholt, and Hansen (2016) for a nice introduction about publication bias and SM\nAnother good overview by Jin, Zhou, and He (2015)\n\nSee also Guan and Vandekerckhove (2016), Maier, Barto≈°, and Wagenmakers (2023) and Barto≈° et al. (2022) for Bayesian approaches to PB"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#more-on-sm-and-publication-bias-1",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#more-on-sm-and-publication-bias-1",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "More on SM and Publication Bias",
    "text": "More on SM and Publication Bias\nAssessing, testing and developing sofisticated models for publication bias is surely important and interesting. But as Wolfgang Viechtbauer (the author of metafor) said:\n\nhopefully there won‚Äôt be need for these models in the future (Viechtbauer 2021)"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#more-on-meta-analysis",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#more-on-meta-analysis",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "More on meta-analysis",
    "text": "More on meta-analysis\n\nthe metafor website contains a lot of materials, examples, tutorials about meta-analysis models\nI have an entire workshop on meta-analysis using data simulation https://stat-teaching.github.io/metasimulation/\n\nThe book Doing meta-analysis in R is an amazing resource"
  },
  {
    "objectID": "slides/03-meta-analysis/03-meta-analysis.html#references",
    "href": "slides/03-meta-analysis/03-meta-analysis.html#references",
    "title": "Meta-Analysis and Multi-Lab Replication studies",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\nBarto≈°, Franti≈°ek, Maximilian Maier, Daniel S Quintana, and Eric-Jan Wagenmakers. 2022. ‚ÄúAdjusting for Publication Bias in JASP and r: Selection Models, PET-PEESE, and Robust Bayesian Meta-Analysis.‚Äù Advances in Methods and Practices in Psychological Science 5 (July): 251524592211092. https://doi.org/10.1177/25152459221109259.\n\n\nBorenstein, Michael, Larry V Hedges, Julian P T Higgins, and Hannah R Rothstein. 2009. ‚ÄúIntroduction to Meta-Analysis.‚Äù https://doi.org/10.1002/9780470743386.\n\n\nCitkowicz, Martyna, and Jack L Vevea. 2017. ‚ÄúA Parsimonious Weight Function for Modeling Publication Bias.‚Äù Psychological Methods 22 (March): 28‚Äì41. https://doi.org/10.1037/met0000119.\n\n\nDuval, S, and R Tweedie. 2000. ‚ÄúTrim and Fill: A Simple Funnel-Plot-Based Method of Testing and Adjusting for Publication Bias in Meta-Analysis.‚Äù Biometrics 56 (June): 455‚Äì63. https://doi.org/10.1111/j.0006-341x.2000.00455.x.\n\n\nGuan, Maime, and Joachim Vandekerckhove. 2016. ‚ÄúA Bayesian Approach to Mitigation of Publication Bias.‚Äù Psychonomic Bulletin & Review 23 (February): 74‚Äì86. https://doi.org/10.3758/s13423-015-0868-6.\n\n\nHarrer, Mathias, Pim Cuijpers, Toshi Furukawa, and David Ebert. 2021. Doing Meta-Analysis with r: A Hands-on Guide. 1st ed. London, England: CRC Press.\n\n\nHedges, Larry V, and Jacob M Schauer. 2019. ‚ÄúStatistical Analyses for Studying Replication: Meta-Analytic Perspectives.‚Äù Psychological Methods 24 (October): 557‚Äì70. https://doi.org/10.1037/met0000189.\n\n\n‚Äî‚Äî‚Äî. 2021. ‚ÄúThe Design of Replication Studies.‚Äù Journal of the Royal Statistical Society. Series A, 184 (March): 868‚Äì86. https://doi.org/10.1111/rssa.12688.\n\n\nHiggins, Julian P T, and Simon G Thompson. 2002. ‚ÄúQuantifying Heterogeneity in a Meta-Analysis.‚Äù Statistics in Medicine 21 (June): 1539‚Äì58. https://doi.org/10.1002/sim.1186.\n\n\nJin, Zhi-Chao, Xiao-Hua Zhou, and Jia He. 2015. ‚ÄúStatistical Methods for Dealing with Publication Bias in Meta-Analysis.‚Äù Statistics in Medicine 34 (January): 343‚Äì60. https://doi.org/10.1002/sim.6342.\n\n\nMaier, Maximilian, Franti≈°ek Barto≈°, and Eric-Jan Wagenmakers. 2023. ‚ÄúRobust Bayesian Meta-Analysis: Addressing Publication Bias with Model-Averaging.‚Äù Psychological Methods 28 (February): 107‚Äì22. https://doi.org/10.1037/met0000405.\n\n\nMcShane, Blakeley B, Ulf B√∂ckenholt, and Karsten T Hansen. 2016. ‚ÄúAdjusting for Publication Bias in Meta-Analysis: An Evaluation of Selection Methods and Some Cautionary Notes: An Evaluation of Selection Methods and Some Cautionary Notes.‚Äù Perspectives on Psychological Science: A Journal of the Association for Psychological Science 11 (September): 730‚Äì49. https://doi.org/10.1177/1745691616662243.\n\n\nOrwin, Robert G. 1983. ‚ÄúA Fail-SafeN for Effect Size in Meta-Analysis.‚Äù Journal of Educational Statistics 8 (June): 157‚Äì59. https://doi.org/10.3102/10769986008002157.\n\n\nRosenberg, Michael S. 2005. ‚ÄúThe File-Drawer Problem Revisited: A General Weighted Method for Calculating Fail-Safe Numbers in Meta-Analysis.‚Äù Evolution; International Journal of Organic Evolution 59 (February): 464‚Äì68. https://doi.org/10.1111/j.0014-3820.2005.tb01004.x.\n\n\nRosenthal, Robert. 1979. ‚ÄúThe File Drawer Problem and Tolerance for Null Results.‚Äù Psychological Bulletin 86 (May): 638‚Äì41. https://doi.org/10.1037/0033-2909.86.3.638.\n\n\nShi, Linyu, and Lifeng Lin. 2019. ‚ÄúThe Trim-and-Fill Method for Publication Bias: Practical Guidelines and Recommendations Based on a Large Database of Meta-Analyses: Practical Guidelines and Recommendations Based on a Large Database of Meta-Analyses.‚Äù Medicine 98 (June): e15987. https://doi.org/10.1097/MD.0000000000015987.\n\n\nVeroniki, Areti Angeliki, Dan Jackson, Wolfgang Viechtbauer, Ralf Bender, Jack Bowden, Guido Knapp, Oliver Kuss, Julian P T Higgins, Dean Langan, and Georgia Salanti. 2016. ‚ÄúMethods to Estimate the Between-Study Variance and Its Uncertainty in Meta-Analysis.‚Äù Research Synthesis Methods 7 (March): 55‚Äì79. https://doi.org/10.1002/jrsm.1164.\n\n\nViechtbauer, Wolfgang. 2005. ‚ÄúBias and Efficiency of Meta-Analytic Variance Estimators in the Random-Effects Model.‚Äù Journal of Educational and Behavioral Statistics: A Quarterly Publication Sponsored by the American Educational Research Association and the American Statistical Association 30 (September): 261‚Äì93. https://doi.org/10.3102/10769986030003261.\n\n\n‚Äî‚Äî‚Äî. 2010. ‚ÄúConducting Meta-Analyses in R with the metafor Package.‚Äù Journal of Statistical Software. https://doi.org/10.18637/jss.v036.i03.\n\n\n‚Äî‚Äî‚Äî. 2021. ‚ÄúSelection Models for Publication Bias in Meta-Analysis. Presentation at ESMARConf2021.‚Äù figshare. https://doi.org/10.6084/M9.FIGSHARE.13637900.V1."
  }
]