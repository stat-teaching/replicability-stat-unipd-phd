---
title: "Multiverse Meta-Analysis Exploring the Efficacy of Psychological Interventions for Depression"
subtitle: "6. Results"
author: "Constantin Yves Plessen"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
editor_options: 
  markdown: 
    wrap: 72
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(metafor)
library(metaviz)
library(puniform)
library(grid)
library(nord)
library(officer)        # exporting word files
library(flextable)      # easily create tables
library(gtsummary)
library(officer)


options(scipen = 1, digits = 2)

# helper function
PET.PEESE <- function(data) {
  mod <- list()
  fit_PET <- lm(yi ~ sqrt(vi), 
                weights = 1/vi, 
                data = data)
  
  pet_p <- coef(summary(fit_PET))["(Intercept)", "Pr(>|t|)"] # pet p-value < .10 -> peese
  
  if(pet_p >= .1) {
    mod$b <- coef(summary(fit_PET))["(Intercept)", "Estimate"] # pet estimate
    mod$ci.lb <- confint(fit_PET)["(Intercept)", "2.5 %"] 
    mod$ci.ub<- confint(fit_PET)["(Intercept)", "97.5 %"] 
    mod$pval <- pet_p
    mod$type <- "PET"
    
  }else{
    
    fit_PEESE <- lm(yi ~ vi, 
                    weights = 1/vi, 
                    data = data)
    
    mod$pval <- coef(summary(fit_PEESE))["(Intercept)", "Pr(>|t|)"] # pet p-value < .10 -> peese
    mod$b  <- coef(summary(fit_PEESE))["(Intercept)", "Estimate"] # peese estimate
    mod$ci.lb <- confint(fit_PEESE)["(Intercept)", "2.5 %"] 
    mod$ci.ub <- confint(fit_PEESE)["(Intercept)", "97.5 %"] 
    mod$type <- "PEESE"

  }
  return(mod)
}
```

# Load data and specifications

```{r}
data <- read_csv("data/tidy/data_cleaned.csv")

specifications_full <- read_csv2("data/tidy/specifications_cleaned.csv") %>% 
  filter(dependency != "ignore") # I remove the ignore specification, at is not reasonable!
```

<br>

# Data cleaning

## as.factor()
```{r}
data <- data %>% 
  mutate_if(names(.) %in% c("target_group","diagnosis","format", "type", "rob", "control", "condition_arm1", "condition_arm2", "outcome_type", "rating" ), funs(as.factor(.)))

summary(data)
```

## Sample size

Individuals should be counted for the sample size and not multiple outcomes per person. Therefore I group by each study and keep only distinct sample sizes.

```{r}
sample_size_data <- data %>% 
  group_by(study) %>%
  distinct(sample_size) %>% 
  plyr::summarise(sample_size_sum = sum(sample_size, na.rm = T),
            sample_size_median = median(sample_size, na.rm = T),
            sample_size_mean = mean(sample_size, na.rm = T)) 

sample_size_data
```

<br>

## Included primary studies
```{r}
unique_studies <- length(unique(data$study))
```

<br>

## Effect size distribution
```{r}
data %>% 
  dplyr::summarise(mean_g = mean(yi),
                   mean_vi = mean(vi),
                   range_min = range(yi)[1],
                   range_max = range(yi)[2],
                   quantile1 = quantile(yi)[1],
                   quantile2 = quantile(yi)[2],
                   quantile3 = quantile(yi)[3],
                   quantile4 = quantile(yi)[4],
                   quantile5 = quantile(yi)[5],
                   )
```

```{r}
ggplot(data, aes(x = yi, y = sample_size)) +
  geom_point(alpha = .4, shape = 1) +
  theme_classic()
```

<br>

## Outliers

There are some studies with very large Effect sizes >= 2 in the data
which might skew the ES estimates!

```{r}
data %>% 
  filter(yi > 2) %>% 
  arrange(desc(yi))
```

This is further investigated with GOSH plots in a previous rmd file


<br>
# Investigating lb < 0

```{r}
data %>% 
  filter(
    yi < 0
    ) %>% 
  summarise(
    mean = mean(yi)
  )
specifications_full %>% 
  filter(lb < 0) %>% dim()

specifications_full %>% 
  filter(lb < 0) %>% 
  mutate(across(where(is.character), as.factor)) %>% 
  summary()
```
# Investigating g < 0

```{r}
data %>% 
  filter(
    yi < 0
    ) %>% 
  summarise(
    mean = mean(yi)
  )
specifications_full %>% 
  filter(mean < 0) %>% dim()
specifications_full %>% 
  filter(mean < 0) %>% 
  mutate(across(where(is.character), as.factor)) %>% 
  summary()
```

what is the mean yi in ma reporting mean < 0?

## First meta-analysis
```{r}
string <- specifications_full %>% 
  filter(mean < 0) %>% 
  slice(1) %>% 
  pull(set)

# Split the string into a list of strings
list <- strsplit(string, ",")[[1]]

# Convert the list to a numeric vector
vector <- as.numeric(list)
```

```{r}
specifications_full %>% 
  filter(mean < 0) %>% 
  slice(1)

data %>% 
  slice(vector) %>% 
  pull(yi) %>% 
  mean
```

```{r}
# Print the resulting vector
print(vector)

data %>% 
  slice(vector) %>% 
  ggplot(aes(y = yi, x = vi)) +
  geom_point() +
  geom_smooth(method = "lm",
              formula = y ~ x, #pet
              se = F) +
    geom_smooth(method = "lm",
              formula = y ~ x^2, #peese
              se = F,
              color = "red",
              linetype = 2)
```


# Results: Conventional meta-analyses

### REM

#### Dependend

```{r}
reml_dep <- rma.uni(data = data,
                    yi = yi, 
                    vi = vi, 
                    method = "REML")
reml_dep
```

<br>

#### Averaged
```{r}
data_avg <- data %>% 
 # select(study, target_group, diagnosis, type, control, region, rob, yi, vi) %>% 
  escalc(yi=yi, vi=vi, data=.)

data_avg <- as.data.frame(aggregate(data_avg, cluster = study,                                      
                                    struct="CS" , #compound symmetric structure as nested are not indpendent
                                     rho = 0.5))

      
reml_avg <- rma(yi = data_avg$yi, vi = data_avg$vi, 
                method = "REML", 
                control = list(stepadj = 0.5,
                               maxiter = 2000))  
reml_avg
```

<br>

##### These Tau estimates are used for inferential specification curve
```{r}
confint(reml_avg)

tau_reml_avg <- confint(reml_avg)
tau_reml_avg$random[2]
tau_reml_avg$random[2,3]
```
<br>

##### Automatic results reporting using metafor reporter() function.
```{r}
#reporter(reml_avg)
```

See <https://rdrr.io/cran/metafor/src/R/reporter.rma.uni.r> for further
info!

<br>

##### Forest Plot

```{r, fig.height = 35}
viz_forest(reml_avg,
           study_labels = data_avg$study,
           variant = "rain") 
```

<br>

### 3-lvl

```{r}
reml_3lvl <-  rma.mv(data = data, 
                   yi = yi, 
                   V = vi, 
                   method = "REML", 
                   control=list(optimizer="optim", optmethod="Nelder-Mead"),
                   random = list(~1 | es_id,
                                 ~1 | study), 
                   sparse=TRUE)
reml_3lvl

W <- diag(1/data$vi)
X <- model.matrix(reml_3lvl)
P <- W - W %*% X %*% solve(t(X) %*% W %*% X) %*% t(X) %*% W
I2_overall <- 100 * sum(reml_3lvl$sigma2) / (sum(reml_3lvl$sigma2) + (reml_3lvl$k-reml_3lvl$p)/sum(diag(P)))

I2 <- 100 * reml_3lvl$sigma2 / (sum(reml_3lvl$sigma2) + (reml_3lvl$k-reml_3lvl$p)/sum(diag(P)))

confint(reml_3lvl)
```

```{r}
reml_3lvl_new <-  rma.mv(data = data, 
                   yi = yi, 
                   V = vi, 
                   method = "REML", 
                   control=list(optimizer="optim", optmethod="Nelder-Mead"),
                   random = ~ 1 | study/es_id, 
                   sparse=TRUE)
reml_3lvl_new
summary(reml_3lvl_new)
confint(reml_3lvl_new)


l3.removed <- rma.mv(data = data, 
                     yi = yi, 
                     V = vi, 
                     random = ~ 1 | study/es_id, 
                     test = "t", 
                     method = "REML",
                     sigma2 =  c(0, NA))
summary(l3.removed)

anova(reml_3lvl_new, l3.removed)
```

sigma^2.1   0.3635 0.3032 0.4372 
<br>

```{r}
mlm.variance.distribution = var.comp = function(x){

  m = x

  # Check class
  if (!(class(m)[1] %in% c("rma.mv", "rma"))){
    stop("x must be of class 'rma.mv'.")
  }

  # Check for three level model
  if (m$sigma2s != 2){
    stop("The model you provided does not seem to be a three-level model. This function can only be used for three-level models.")
  }

  # Check for right specification (nested model)
  if (sum(grepl("/", as.character(m$random[[1]]))) < 1){
    stop("Model must contain nested random effects. Did you use the '~ 1 | cluster/effect-within-cluster' notation in 'random'? See ?metafor::rma.mv for more details.")
  }

  # Get variance diagonal and calculate total variance
  n = m$k.eff
  vector.inv.var = 1/(diag(m$V))
  sum.inv.var = sum(vector.inv.var)
  sum.sq.inv.var = (sum.inv.var)^2
  vector.inv.var.sq = 1/(diag(m$V)^2)
  sum.inv.var.sq = sum(vector.inv.var.sq)
  num = (n-1)*sum.inv.var
  den = sum.sq.inv.var - sum.inv.var.sq
  est.samp.var = num/den

  # Calculate variance proportions
  level1=((est.samp.var)/(m$sigma2[1]+m$sigma2[2]+est.samp.var)*100)
  level2=((m$sigma2[2])/(m$sigma2[1]+m$sigma2[2]+est.samp.var)*100)
  level3=((m$sigma2[1])/(m$sigma2[1]+m$sigma2[2]+est.samp.var)*100)

  # Prepare df for return
  Level=c("Level 1", "Level 2", "Level 3")
  Variance=c(level1, level2, level3)
  df.res=data.frame(Variance)
  colnames(df.res) = c("% of total variance")
  rownames(df.res) = Level
  I2 = c("---", round(Variance[2:3], 2))
  df.res = as.data.frame(cbind(df.res, I2))

  totalI2 = Variance[2] + Variance[3]


  # Generate plot
  df1 = data.frame("Level" = c("Sampling Error", "Total Heterogeneity"),
                  "Variance" = c(df.res[1,1], df.res[2,1]+df.res[3,1]),
                  "Type" = rep(1,2))

  df2 = data.frame("Level" = rownames(df.res),
                   "Variance" = df.res[,1],
                   "Type" = rep(2,3))

  df = as.data.frame(rbind(df1, df2))


  g = ggplot(df, aes(fill=Level, y=Variance, x=as.factor(Type))) +
    coord_cartesian(ylim = c(0,1), clip = "off") +
    geom_bar(stat="identity", position="fill", width = 1, color="black") +
    scale_y_continuous(labels = scales::percent)+
    theme(axis.title.x=element_blank(),
          axis.text.y = element_text(color="black"),
          axis.line.y = element_blank(),
          axis.title.y=element_blank(),
          axis.line.x = element_blank(),
          axis.ticks.x = element_blank(),
          axis.text.x = element_blank(),
          axis.ticks.y = element_line(lineend = "round"),
          legend.position = "none",
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          panel.background = element_blank(),
          legend.background = element_rect(linetype="solid",
                                           colour ="black"),
          legend.title = element_blank(),
          legend.key.size = unit(0.75,"cm"),
          axis.ticks.length=unit(.25, "cm"),
          plot.margin = unit(c(1,3,1,1), "lines")) +
    scale_fill_manual(values = c("darkseagreen3", "deepskyblue3", "darkseagreen2",
                                 "deepskyblue1", "deepskyblue2")) +

    # Add Annotation

    # Total Variance
    annotate("text", x = 1.5, y = 1.05,
             label = paste("Total Variance:",
                           round(m$sigma2[1]+m$sigma2[2]+est.samp.var, 3))) +

    # Sampling Error
    annotate("text", x = 1, y = (df[1,2]/2+df[2,2])/100,
             label = paste("Sampling Error Variance: \n", round(est.samp.var, 3)), size = 3) +

    # Total I2
    annotate("text", x = 1, y = ((df[2,2])/100)/2-0.02,
             label = bquote("Total"~italic(I)^2*":"~.(round(df[2,2],2))*"%"), size = 3) +
    annotate("text", x = 1, y = ((df[2,2])/100)/2+0.05,
             label = paste("Variance not attributable \n to sampling error: \n", round(m$sigma2[1]+m$sigma2[2],3)), size = 3) +

    # Level 1
    annotate("text", x = 2, y = (df[1,2]/2+df[2,2])/100, label = paste("Level 1: \n",
                                                                       round(df$Variance[3],2), "%", sep=""), size = 3) +

    # Level 2
    annotate("text", x = 2, y = (df[5,2]+(df[4,2]/2))/100,
             label = bquote(italic(I)[Level2]^2*":"~.(round(df[4,2],2))*"%"), size = 3) +

    # Level 3
    annotate("text", x = 2, y = (df[5,2]/2)/100,
             label = bquote(italic(I)[Level3]^2*":"~.(round(df[5,2],2))*"%"), size = 3)

  returnlist = list(results = df.res,
                    totalI2 = totalI2,
                    plot = g)
  class(returnlist) = c("mlm.variance.distribution", "list")

  invisible(returnlist)

  returnlist

}
```

```{r}
i2 <- var.comp(reml_3lvl_new)
summary(i2$results)
```

i2 <- var.comp(full.model)
summary(i2)

### RVE

```{r}
rve <- robust(reml_3lvl, cluster=data$study, clubSandwich=TRUE)
rve
```


### FEM

```{r}
fem_avg <- rma.uni(data = data_avg,
                    yi = yi, 
                    vi = vi, 
                    method = "FE")
fem_avg
```

<br>

### Unweighted

<br>

#### Averaged
```{r}
uw_avg <- rma(data = data_avg, yi = yi, vi = vi,
                   method = "FE", 
                   weights = 1/nrow(data_avg))

uw_avg
```

<br>

### p-uniform

```{r}
puni_avg <- puni_star(yi = data_avg$yi, vi = data_avg$vi, 
                           side = "right")
puni_avg
```

<br>

### PET-PEESE
```{r}
pet_peese_avg <- PET.PEESE(data_avg)
pet_peese_avg
```


# Results: 3-level subgroup analyses

> Reviewer 2 comment: Along with doing all these separate analysis for the “which” factors, why not do a regression model assuming all which factors at the same time and mitigate the problems of multiple testing and confounding?

http://www.metafor-project.org/doku.php/tips:multiple_factors_interactions 
```{r}
reml_3lvl_multiple_regression <- rma.mv(data = data,
                             yi = yi,
                             V = vi, 
                             method = "REML",
                             mods = ~ target_group + format + type + control + risk_of_bias + diagnosis-1,
                             random = list(~ 1 | es_id, 
                                           ~ 1 | study),
                            sparse = T)

reml_3lvl_multiple_regression
```



## Target group
```{r, eval = FALSE}
reml_3lvl_target_group <- rma.mv(data = data,
                             yi = yi,
                             V = vi, 
                             method = "REML",
                             mods = ~ target_group,
                             random = list(~ 1 | es_id, 
                                           ~ 1 | study),
                             sparse = T)
reml_3lvl_target_group
```


## Format
```{r, eval = FALSE}
reml_3lvl_format <- rma.mv(data = data,
                             yi = yi,
                             V = vi, 
                             method = "REML",
                             mods = ~ format,
                             random = list(~ 1 | es_id, 
                                           ~ 1 | study))
reml_3lvl_format
```

## Type
```{r, eval = FALSE}
reml_3lvl_type <- rma.mv(data = data,
                             yi = yi,
                             V = vi, 
                             method = "REML",
                             mods = ~ type,
                             random = list(~ 1 | es_id, 
                                           ~ 1 | study))
reml_3lvl_type
```

## Control
```{r, eval = FALSE}
reml_3lvl_control <- rma.mv(data = data,
                             yi = yi,
                             V = vi, 
                             method = "REML",
                             mods = ~ control,
                             random = list(~ 1 | es_id, 
                                           ~ 1 | study))
reml_3lvl_control
```

## Risk of bias
```{r, eval = FALSE}
reml_3lvl_rob <- rma.mv(data = data,
                             yi = yi,
                             V = vi, 
                             method = "REML",
                             mods = ~ rob,
                             random = list(~ 1 | es_id, 
                                           ~ 1 | study))
reml_3lvl_rob
```

## Diagnosis
```{r, eval = FALSE}
reml_3lvl_diagnosis <- rma.mv(data = data,
                             yi = yi,
                             V = vi, 
                             method = "REML",
                             mods = ~ diagnosis,
                             random = list(~ 1 | es_id, 
                                           ~ 1 | study))
reml_3lvl_diagnosis
```

<br>

# Results: Sensitivity Analyses with different specification sets

As it is not feasible to visualize specification sets with only 2 studies, we investigate potential differences in multiverse meta-analyses containing >= 2, 5, 10, 25, 50 studies. We have to reduce the total number of specification to be able to visualized the descriptive specification plot, but of course this is also a flexible data-analytical decision in itself and has to be taken into account. The number of extreme meta-analyses with only 2 included primary studies is of course much higher than in studies that include at least 5, 10, 25, or 50 primary studies.

<br>

## Defining multiverse sizes

```{r}
specifications_k10 <- specifications_full %>% 
  mutate(group = "k10")
specifications_k25 <- specifications_full %>% 
  filter(k >25) %>% 
  mutate(group = "k25")
specifications_k50 <- specifications_full %>% 
  filter(k >50) %>% 
  mutate(group = "k50")

long_data <- rbind(specifications_k10,
                   specifications_k25,
                   specifications_k50) %>% 
  mutate(group = factor(group,levels=c("k10", "k25", "k50"))) %>% 
  dplyr::add_count(group) %>% 
  mutate(label = paste0(group,"\n","(n=",n,")"), 
         label = factor(label, 
                        levels=c("k10\n(n=4281)",
                                 "k25\n(n=2138)",
                                 "k50\n(n=1110)")))
```

<br>

## Summary effect size

```{r}
figure_s2 <- ggplot(long_data, 
       aes(x=label, 
           y=mean, 
           fill=group)) + 
  geom_violin(draw_quantiles = NULL, alpha = .5) +
  geom_boxplot(width=0.1, outlier.shape = 1, outlier.alpha = .4) +
  #geom_jitter(height = 0, width = 0.1,
  #            alpha = .01,
  #            shape = 1) +
  theme_minimal() +
  viridis::scale_fill_viridis(discrete = TRUE)+
  xlab("Size of Multiverse") +
  ylab("Mean Hedges´ g") +
 theme(legend.position = "none") + 
  coord_flip()

figure_s2
```

```{r}
##ggsave("figures/figure_s2.pdf", 
#       figure_s2, 
#       width = 10, 
#       height = 5,
#       dpi = "retina"
#       )
```

<br>

## Table 3: Mean summary effect & 95% CI

### Quantiles

```{r}
quantile_k10 <- quantile(specifications_k10$mean)
iqr_k10 <- c(quantile_k10[2], quantile_k10[4])
iqr_k10
quantile_k25 <- quantile(specifications_k25$mean)
iqr_k25 <- c(quantile_k25[2], quantile_k25[4])
iqr_k25
quantile_k50 <- quantile(specifications_k50$mean)
iqr_k50 <- c(quantile_k50[2], quantile_k50[4])
iqr_k50
```

```{r}
table_3 <- tribble(~"# Specifications", ~"k", ~"Mean g", ~"95% CI LB", ~"95% CI UB", ~"Minimum", ~"Maximum", ~"25% Quantile", ~"75% Quantile",
        
        "k  > 10",nrow(specifications_k10), mean(specifications_k10$mean), mean(specifications_k10$lb), mean(specifications_k10$ub), min(specifications_k10$mean), max(specifications_k10$mean), quantile_k10[2], quantile_k10[4],
        "k  > 25", nrow(specifications_k25), mean(specifications_k25$mean), mean(specifications_k25$lb), mean(specifications_k25$ub), min(specifications_k25$mean), max(specifications_k25$mean), quantile_k25[2], quantile_k25[4],
        "k  > 50", nrow(specifications_k50), mean(specifications_k50$mean), mean(specifications_k50$lb), mean(specifications_k50$ub), min(specifications_k50$mean), max(specifications_k50$mean), quantile_k50[2], quantile_k50[4],
        )  
table_3
```


<br>

## Smallest reported meta-analysis

```{r}
specifications_k10 %>% 
  slice_min(mean)
```

Situation Minimum: A meta-analysis investigating the efficacy of CBT-based group therapy comparing the intervention group with a wait-list control group for all target groups in Europe, all diagnosisnoses and p-uniform to adress publication bias: -2.381, k = 13.

```{r}
specifications_k10 %>% 
  slice_max(mean)
```
Situation Maximum: Meta-analysis investigating all groups in regions other than

<br>

## Example meta-analysis

```{r}
specifications_k10 %>% 
  arrange(mean) %>% 
  slice(1000)
```

<br>

# Results: Multiverse

## % of the estimated means were greater than 0

```{r}
mean_over_zero_k10 <- specifications_k10 %>% 
  filter(mean > 0) %>% 
  nrow()

mean_over_zero_k10

percentage_mean_over_zero_k10 <- mean_over_zero_k10/nrow(specifications_k10) * 100

percentage_mean_over_zero_k10
```

<br>

## % of these had 95%*CIs*that did not include zero

```{r}
ci_over_zero_k10 <- specifications_k10 %>% 
  filter( lb > 0) %>% 
  nrow()

ci_over_zero_k10

percentage_ci_over_zero_k10 <- ci_over_zero_k10/nrow(specifications_k10) * 100

percentage_ci_over_zero_k10
```

<br>

## Clinically relevant effect sizes

```{r}
mean_over_clinically_relevance <- specifications_k10 %>% 
  filter(mean > .24) %>% 
  nrow()

mean_over_clinically_relevance

percentage_mean_over_clinically_relevance <- mean_over_clinically_relevance/nrow(specifications_k10) * 100

percentage_mean_over_clinically_relevance
```


```{r}
ci_over_clinically_relevance <- specifications_k10 %>% 
  filter( lb > .24) %>% 
  nrow()

ci_over_clinically_relevance

percentage_ci_over_clinically_relevance <- ci_over_clinically_relevance/nrow(specifications_k10) * 100

percentage_ci_over_clinically_relevance
```


# Which Factors

<br>

### target_group

#### Table

```{r}
s3_target_group <- specifications_k10 %>% 
  group_by(target_group) %>% 
  dplyr::summarise("Mean g" = mean(mean),
                   "Mean CI Lower" = mean(lb),
                   "Mean CI Upper" = mean(ub),
                   "k" = n()) %>% 
  arrange(desc(`Mean g`)) %>% 
  mutate("Target group" = str_to_sentence(target_group)) %>% 
  select("Target group", everything(), -target_group) 
  
table_s3_target_group <- s3_target_group %>%
  flextable() %>% 
  theme_booktabs() %>% 
  italic(italic = TRUE, part = "header") %>% 
  autofit()  %>% 
  add_header_lines(values = c("Summary effect sizes by target group"))

table_s3_target_group

# Export to word
doc_s3_target_group <- read_docx()
doc_s3_target_group <- body_add_flextable(doc_s3_target_group, value = table_s3_target_group)
#print(doc_s3_target_group, target = "tables/table_s3_target_group.docx")
```

<br>

#### Plot

```{r}
figure_s3_target_group <- long_data %>% 
  filter(group == "k10") %>% 
  mutate(target_group = str_to_sentence(target_group)) %>% 
  ggplot(aes(x=reorder(target_group, mean), 
             y=mean,
             fill = target_group)) + 
  geom_violin(draw_quantiles = NULL, alpha = .5) +
  geom_boxplot(width = .1) +
  theme_minimal() +
viridis::scale_fill_viridis(discrete = "TRUE")+
  coord_flip() +
  theme(legend.position = "none") +
  ylab("Mean g") +
  xlab(str_to_sentence(str_replace("target_group", "_", " ")))

figure_s3_target_group

##ggsave("figures/figure_s3_target_group.pdf", 
#       figure_s3_target_group, 
#       width = 10, 
#       height = 5,
#       dpi = "retina"
#       )
```

<br>

### format

#### Table

```{r}
s3_format <- specifications_k10 %>% 
  group_by(format) %>% 
  dplyr::summarise("Mean g" = mean(mean),
                   "Mean CI Lower" = mean(lb),
                   "Mean CI Upper" = mean(ub),
                   "k" = n()) %>% 
  arrange(desc(`Mean g`)) %>% 
  mutate("Format" = str_to_sentence(format)) %>% 
  select("Format", everything(), -format)

table_s3_format <- s3_format %>% flextable() %>% 
  theme_booktabs() %>% 
  italic(italic = TRUE, part = "header") %>% 
  autofit()  %>% 
  add_header_lines(values = c("Summary effect sizes by format"))

table_s3_format

# Export to word
doc_s3_format <- read_docx()
doc_s3_format <- body_add_flextable(doc_s3_format, value = table_s3_format)
#print(doc_s3_format, target = "tables/table_s3_format.docx")
```

<br>

#### Plot

```{r}
figure_s3_format <- long_data %>% 
  filter(group == "k10") %>% 
  mutate(format =str_to_sentence(format)) %>% 
  ggplot(aes(x=reorder(format, mean), 
             y=mean,
             fill = format)) + 
  geom_violin(draw_quantiles = NULL, alpha = .5) +
  geom_boxplot(width = .1) +
  theme_minimal() +
viridis::scale_fill_viridis(discrete = "TRUE")+
  coord_flip() +
  theme(legend.position = "none") +
  ylab("Mean g") +
  xlab(str_to_sentence(str_replace("format", "_", " ")))

figure_s3_format

#ggsave("figures/figure_s3_format.pdf", 
#       figure_s3_format, 
#       width = 10, 
#       height = 5,
#       dpi = "retina"
#       )
```
<br>

### type

#### Table

```{r}
s3_type <- specifications_k10 %>% 
  group_by(type) %>% 
  dplyr::summarise("Mean g" = mean(mean),
                   "Mean CI Lower" = mean(lb),
                   "Mean CI Upper" = mean(ub),
                   "k" = n()) %>% 
  arrange(desc(`Mean g`)) %>% 
  mutate("Type" = str_to_sentence(type)) %>% 
  select("Type", everything(), -type)

table_s3_type <- s3_type %>% flextable() %>% 
  theme_booktabs() %>% 
  italic(italic = TRUE, part = "header") %>% 
  autofit()  %>% 
  add_header_lines(values = c("Summary effect sizes by type"))

table_s3_type

# Export to word
doc_s3_type <- read_docx()
doc_s3_type <- body_add_flextable(doc_s3_type, value = table_s3_type)
#print(doc_s3_type, target = "tables/table_s3_type.docx")
```

<br>

#### Plot

```{r}
figure_s3_type <- long_data %>% 
  filter(group == "k10") %>% 
  mutate(type =str_to_sentence(type)) %>% 
  ggplot(aes(x=reorder(type, mean), 
             y=mean,
             fill = type)) + 
  geom_violin(draw_quantiles = NULL, alpha = .5) +
  geom_boxplot(width = .1) +
  theme_minimal() +
viridis::scale_fill_viridis(discrete = "TRUE")+
  coord_flip() +
  theme(legend.position = "none") +
  ylab("Mean g") +
  xlab(str_to_sentence(str_replace("type", "_", " ")))

figure_s3_type

#ggsave("figures/figure_s3_type.pdf", 
#       figure_s3_type, 
#       width = 10, 
#       height = 5,
#       dpi = "retina"
#       )
```

### control

#### Table

```{r}
s3_control <- specifications_k10 %>% 
  group_by(control) %>% 
  dplyr::summarise("Mean g" = mean(mean),
                   "Mean CI Lower" = mean(lb),
                   "Mean CI Upper" = mean(ub),
                   "k" = n()) %>% 
  arrange(desc(`Mean g`)) %>% 
  mutate("Control" = str_to_sentence(control)) %>% 
  select("Control", everything(), -control)

table_s3_control <- s3_control %>% flextable() %>% 
  theme_booktabs() %>% 
  italic(italic = TRUE, part = "header") %>% 
  autofit()  %>% 
  add_header_lines(values = c("Summary effect sizes by control"))

table_s3_control

# Export to word
doc_s3_control <- read_docx()
doc_s3_control <- body_add_flextable(doc_s3_control, value = table_s3_control)
#print(doc_s3_control, target = "tables/table_s3_control.docx")
```

<br>

#### Plot

```{r}
figure_s3_control <- long_data %>% 
  filter(group == "k10") %>% 
  mutate(control =str_to_sentence(control)) %>% 
  ggplot(aes(x=reorder(control,mean), 
             y=mean,
             fill = control)) + 
  geom_violin(draw_quantiles = NULL, alpha = .5) +
  geom_boxplot(width = .1) +
  theme_minimal() +
viridis::scale_fill_viridis(discrete = "TRUE")+
  coord_flip() +
  theme(legend.position = "none") +
  ylab("Mean g") +
  xlab(str_to_sentence(str_replace("control", "_", " ")))

figure_s3_control

#ggsave("figures/figure_s3_control.pdf", 
#       figure_s3_control, 
#       width = 10, 
#       height = 5,
#       dpi = "retina"
#       )
```
### risk of bias

#### Table

```{r}
s3_rob <- specifications_k10 %>% 
  group_by(rob) %>% 
  dplyr::summarise("Mean g" = mean(mean),
                   "Mean CI Lower" = mean(lb),
                   "Mean CI Upper" = mean(ub),
                   "k" = n()) %>% 
  arrange(desc(`Mean g`)) %>% 
  mutate("Risk of Bias" = str_to_sentence(rob)) %>% 
  select("Risk of Bias", everything(), -rob)

table_s3_rob <- s3_rob %>% flextable() %>% 
  theme_booktabs() %>% 
  italic(italic = TRUE, part = "header") %>% 
  autofit()  %>% 
  add_header_lines(values = c("Summary effect sizes by rob"))

table_s3_rob

# Export to word
doc_s3_rob <- read_docx()
doc_s3_rob <- body_add_flextable(doc_s3_rob, value = table_s3_rob)
#print(doc_s3_rob, target = "tables/table_s3_rob.docx")
```

<br>

#### Plot

```{r}
figure_s3_rob <- long_data %>% 
  filter(group == "k10") %>% 
  mutate(rob =str_to_sentence(rob)) %>% 
  ggplot(aes(x=reorder(rob, mean), 
             y=mean,
             fill = rob)) + 
  geom_violin(draw_quantiles = NULL, alpha = .5) +
  geom_boxplot(width = .1) +
  theme_minimal() +
viridis::scale_fill_viridis(discrete = "TRUE")+
  coord_flip() +
  theme(legend.position = "none") +
  ylab("Mean g") +
  xlab(str_to_sentence(str_replace("rob", "_", " ")))

figure_s3_rob

#ggsave("figures/figure_s3_rob.pdf", 
#       figure_s3_rob, 
#       width = 10, 
#       height = 5,
#       dpi = "retina"
#       )
```

### diagnosis

#### Table

```{r}
s3_diagnosis <- specifications_k10 %>% 
  group_by(diagnosis) %>% 
  dplyr::summarise("Mean g" = mean(mean),
                   "Mean CI Lower" = mean(lb),
                   "Mean CI Upper" = mean(ub),
                   "k" = n()) %>% 
  arrange(desc(`Mean g`)) %>% 
  mutate("Diagnosis" = str_to_sentence(diagnosis)) %>% 
  select("Diagnosis", everything(), -diagnosis)

table_s3_diagnosis <- s3_diagnosis %>% flextable() %>% 
  theme_booktabs() %>% 
  italic(italic = TRUE, part = "header") %>% 
  autofit()  %>% 
  add_header_lines(values = c("Summary effect sizes by diagnosis"))

table_s3_diagnosis

# Export to word
doc_s3_diagnosis <- read_docx()
doc_s3_diagnosis <- body_add_flextable(doc_s3_diagnosis, value = table_s3_diagnosis)
#print(doc_s3_diagnosis, target = "tables/table_s3_diagnosis.docx")
```

<br>

#### Plot

```{r}
figure_s3_diagnosis <- long_data %>% 
  filter(group == "k10") %>% 
  mutate(diagnosis =str_to_sentence(diagnosis)) %>% 
  ggplot(aes(x=reorder(diagnosis, mean), 
             y=mean,
             fill = diagnosis)) + 
  geom_violin(draw_quantiles = NULL, alpha = .5) +
  geom_boxplot(width = .1) +
  theme_minimal() +
viridis::scale_fill_viridis(discrete = "TRUE")+
  coord_flip() +
  theme(legend.position = "none") +
  ylab("Mean g") +
  xlab(str_to_sentence(str_replace("diagnosis", "_", " ")))

figure_s3_diagnosis

#ggsave("figures/figure_s3_diagnosis.pdf", 
#       figure_s3_diagnosis, 
#       width = 10, 
#       height = 5,
#       dpi = "retina"
#       )
```


### dependency

#### Table

```{r}
s3_dependency <- specifications_k10 %>% 
  group_by(dependency) %>% 
  dplyr::summarise("Mean g" = mean(mean),
                   "Mean CI Lower" = mean(lb),
                   "Mean CI Upper" = mean(ub),
                   "k" = n()) %>% 
  arrange(desc(`Mean g`)) %>% 
  mutate("Method" = str_to_sentence(dependency)) %>% 
  select("Method", everything(), -dependency)

table_s3_dependency <- s3_dependency %>% flextable() %>% 
  theme_booktabs() %>% 
  italic(italic = TRUE, part = "header") %>% 
  autofit()  %>% 
  add_header_lines(values = c("Summary effect sizes by different methods to deal with effect size dependency."))

table_s3_dependency

# Export to word
doc_s3_dependency <- read_docx()
doc_s3_dependency <- body_add_flextable(doc_s3_dependency, value = table_s3_dependency)
#print(doc_s3_dependency, target = "tables/table_s3_dependency.docx")
```

<br>

#### Plot

```{r}
figure_s3_dependency <- long_data %>% 
  filter(group == "k10") %>% 
    mutate(dependency = fct_reorder(dependency, mean, .fun='median')) %>%
  #mutate(dependency = str_to_upper(reorder(dependency, mean))) %>% 
  ggplot(aes(x=dependency, 
             y=mean,
             fill = dependency)) + 
  geom_violin(draw_quantiles = NULL, alpha = .5) +
  geom_boxplot(width = .1) +
  theme_minimal() +
viridis::scale_fill_viridis(discrete = "TRUE")+
  theme(legend.position = "none") +
  ylab("Mean Hedges´ g") +
  xlab("Meta-Analyical Method") +
    coord_flip()

figure_s3_dependency

#ggsave("figures/figure_s3_dependency.pdf", 
#       figure_s3_dependency, 
#       width = 10, 
#       height = 5,
#       dpi = "retina"
#       )
```

### ma_method

#### Table

```{r}
s3_ma_method <- specifications_k10 %>% 
  group_by(ma_method) %>% 
  dplyr::summarise("Mean g" = mean(mean),
                   "Mean CI Lower" = mean(lb),
                   "Mean CI Upper" = mean(ub),
                   "k" = n()) %>% 
  arrange(desc(`Mean g`)) %>% 
  mutate("Method" = str_to_upper(ma_method)) %>% 
  select("Method", everything(), -ma_method)

table_s3_ma_method <- s3_ma_method %>% flextable() %>% 
  theme_booktabs() %>% 
  italic(italic = TRUE, part = "header") %>% 
  autofit()  %>% 
  add_header_lines(values = c("Summary effect sizes by meta-analytical method"))

table_s3_ma_method

# Export to word
doc_s3_ma_method <- read_docx()
doc_s3_ma_method <- body_add_flextable(doc_s3_ma_method, value = table_s3_ma_method)
#print(doc_s3_ma_method, target = "tables/table_s3_ma_method.docx")
```

<br>

#### Plot

```{r}
figure_s3_ma_method <- long_data %>% 
  filter(group == "k10") %>% 
    mutate(ma_method = fct_reorder(ma_method, mean, .fun='median')) %>%
  #mutate(ma_method = str_to_upper(reorder(ma_method, mean))) %>% 
  ggplot(aes(x=ma_method, 
             y=mean,
             fill = ma_method)) + 
  geom_violin(draw_quantiles = NULL, alpha = .5) +
  geom_boxplot(width = .1) +
  theme_minimal() +
viridis::scale_fill_viridis(discrete = "TRUE")+
  theme(legend.position = "none") +
  ylab("Mean Hedges´ g") +
  xlab("Meta-Analyical Method") +
    coord_flip()

figure_s3_ma_method

#ggsave("figures/figure_s3_ma_method.pdf", 
#       figure_s3_ma_method, 
#       width = 10, 
#       height = 5,
#       dpi = "retina"
#       )
```
<br>
## Saving all plots

```{r}
cowplot::plot_grid(figure_s3_target_group,
                   figure_s3_type,
                   figure_s3_format,
                   figure_s3_control,
                   figure_s3_diagnosis,
                   figure_s3_rob,
                   #figure_s3_dependency,
                   #figure_s3_ma_method,
                   ncol = 2,
                   align = "v",
                   rel_heights = c(4,5))
```


## Saving all tables

```{r}
table_s3_control      <- theme_vanilla(table_s3_control)
table_s3_diagnosis    <- theme_vanilla(table_s3_diagnosis)
table_s3_format       <- theme_vanilla(table_s3_format)
table_s3_ma_method    <- theme_vanilla(table_s3_ma_method)
table_s3_rob <- theme_vanilla(table_s3_rob)
table_s3_target_group <- theme_vanilla(table_s3_target_group)
table_s3_type         <- theme_vanilla(table_s3_type)

table_s3_control  <- add_footer_lines(table_s3_control , "Note: Mean Hedges´g effect size information, including 95% Confidence Intervall lower and upper bounds and number of included meta-analyses in the multiverse of meta-analyses.")
table_s3_diagnosis <- add_footer_lines(table_s3_diagnosis, "Note: Mean Hedges´g effect size information, including 95% Confidence Intervall lower and upper bounds and number of included meta-analyses in the multiverse of meta-analyses.")
table_s3_format <- add_footer_lines(table_s3_format, "Note: Mean Hedges´g effect size information, including 95% Confidence Intervall lower and upper bounds and number of included meta-analyses in the multiverse of meta-analyses.")
table_s3_ma_method <- add_footer_lines(table_s3_ma_method, "Note: Mean Hedges´g effect size information, including 95% Confidence Intervall lower and upper bounds and number of included meta-analyses in the multiverse of meta-analyses.")
table_s3_rob <- add_footer_lines(table_s3_rob, "Note: Mean Hedges´g effect size information, including 95% Confidence Intervall lower and upper bounds and number of included meta-analyses in the multiverse of meta-analyses.")
table_s3_target_group <- add_footer_lines(table_s3_target_group, "Note: Mean Hedges´g effect size information, including 95% Confidence Intervall lower and upper bounds and number of included meta-analyses in the multiverse of meta-analyses.")
table_s3_type <- add_footer_lines(table_s3_type, "Note: Mean Hedges´g effect size information, including 95% Confidence Intervall lower and upper bounds and number of included meta-analyses in the multiverse of meta-analyses.")


table_s3_control     <- color(table_s3_control     , part = "footer", color = "#666666")
table_s3_diagnosis   <- color(table_s3_diagnosis   , part = "footer", color = "#666666")
table_s3_format      <- color(table_s3_format      , part = "footer", color = "#666666")
table_s3_ma_method   <- color(table_s3_ma_method   , part = "footer", color = "#666666")
table_s3_rob<- color(table_s3_rob, part = "footer", color = "#666666")
table_s3_target_group<- color(table_s3_target_group, part = "footer", color = "#666666")
table_s3_type        <- color(table_s3_type        , part = "footer", color = "#666666")
```



# Tables 

## Manuscript

### Table 1: Summary characteristics of included effect sizes from primary studies.
```{r}
table_1_summary <- data %>% 
  select("Target Group" = target_group,
         "Region" = region,
         "Intervention" = type,
         "Format" = format,
         "Control" = control,
         "diagnosis" = diagnosis,
         "Risk of Bias" = risk_of_bias) %>%  
  mutate(across("Target Group":"Risk of Bias", str_to_title)) %>% 
  mutate(Intervention = str_replace(Intervention, "Cbt", "CBT"),
         Control = str_replace(Control, "Cau", "CAU"),
         Control = str_replace(Control, "Wl", "WL")) %>% 
  mutate(across("Target Group":"Risk of Bias", as.factor ))

table_1_summary <- table_1_summary %>% tbl_summary() %>% as_flex_table() %>% 
  theme_booktabs() %>% 
  italic(italic = TRUE, part = "header") %>% 
  autofit()  %>% 
  add_header_lines(values = c("Summary characteristics of included effect sizes from primary studies.", "Table 1"))

# Export to word
doc_1 <- read_docx()
doc_1 <- body_add_flextable(doc_1, value = table_1_summary)
#print(doc_1, target = "tables/table_1_summary.docx")
```

```{r}
table_1_summary_study_level <- data_avg %>% 
  select("Target Group" = target_group,
         "Region" = region,
         "Intervention" = type,
         "Format" = format,
         "Control" = control,
         "diagnosis" = diagnosis,
         "Risk of Bias" = risk_of_bias) %>%  
  mutate(across("Target Group":"Risk of Bias", str_to_title)) %>% 
  mutate(Intervention = str_replace(Intervention, "Cbt", "CBT"),
         Control = str_replace(Control, "Cau", "CAU"),
         Control = str_replace(Control, "Wl", "WL")) %>% 
  mutate(across("Target Group":"Risk of Bias", as.factor ))

table_1_summary_study_level <- table_1_summary_study_level %>% tbl_summary() %>% as_flex_table() %>% 
  theme_booktabs() %>% 
  italic(italic = TRUE, part = "header") %>% 
  autofit()  %>% 
  add_header_lines(values = c("Summary characteristics of included primary studies.", "Table 1"))

# Export to word
doc_1 <- read_docx()
doc_1 <- body_add_flextable(doc_1, value = table_1_summary_study_level)
#print(doc_1, target = "tables/table_1_summary_study_level.docx")
```

<br>

## Supplementary Material 

### Table S1: Study overview

```{r}
table_s1 <- data %>% 
  mutate("Sample Size" = n_1 + n_2) %>% 
  select("Study" = study, 
         "Hedges g" = yi, 
         "SE" = sei,
         "Sample Size",
         "Target Group" = target_group,
         "Intervention" = type,
         "Format" = format,
         "Control" = control,
         "Diagnosis" = diagnosis,
         "Risk of Bias" = risk_of_bias) %>%  
  mutate_if(is.character, str_to_title) %>% 
  mutate(across(2:3, round, 2))
table_s1
#write.csv(table_s1, "tables/table_s1.csv")
```



### Table S1: Study overview

```{r}
table_s1 <- data %>% 
  mutate("Sample Size" = n_1 + n_2) %>% 
  select("Study" = study, 
         yi, 
         sei,
         vi,
         n = "Sample Size",
          target_group,
         "Region" = region,
         "Intervention" = type,
         "Format" = format,
         "Control" = control,
         "Diagnosis" = diagnosis,
         risk_of_bias) %>%  
  mutate_if(is.factor, str_to_title) %>% 
  mutate_if(is.character, str_to_title) %>% 
  mutate(across(2:3, round, 2)) %>% 
  mutate(Control = str_replace(Control,  "Cau",  "CAU"),
         Control = str_replace(Control, "Wl", "WL"),
         Control = str_replace(Control, "Other Ctr", "Other"),
         Intervention = str_replace(Intervention, "Cbt", "CBT")
  ) %>% glimpse()

table_s1_combined <- table_s1 %>% 
  group_by(Study) %>% 
  dplyr::summarise(target_group = str_c(unique(target_group), collapse = " & "), 
            Region = str_c(unique(Region), collapse = " & "),
            Intervention = str_c(unique(Intervention), collapse = " & "),
            Format = str_c(unique(Format), collapse = " & "), 
            Control = str_c(unique(Control), collapse = " & "),
            Diagnosis = str_c(unique(Diagnosis), collapse = " & "),
            risk_of_bias = str_c(unique(risk_of_bias), collapse = " & "),
            sample_size = mean(n)
            )

table_s1 <- escalc(data = table_s1, yi = yi, sei = sei)

agg <- aggregate(table_s1, cluster=Study, struct="ID", digits = 2)

agg <- agg %>% 
  as.data.frame(.) %>% 
  select(yi,
         sei
         )

eTable1 <- cbind(agg, table_s1_combined)

eTable1 <- eTable1 %>% 
  select(Study, 
         "Sample Size" = sample_size,
         "Target group" = target_group, 
         Intervention, 
         Format,
         Control,
         Diagnosis,
         "Risk of bias" = risk_of_bias,
         Region,
         "Hedges´ g" = yi,
         "SE" = sei
         ) %>% 
  as.data.frame(.)

eTable1
```


```{r}
#eTable1 <- 
#Export to word: file to large, doesnt work with flextable

eTable1_flex <- eTable1 %>% 
  flextable() %>% 
  colformat_num(col_keys = colkeys, digits = 0, na_str = "NA", big.mark = "") %>% 
  add_header_lines(values = "Study Information of Included Primary Studies.") %>%
  add_footer_lines(values = "Notes. Control: WL = Wait list; CAU = Care as Usual. Intervention: CBT = Cognitive Behavioral Therapy. Diagnosis: Diagnosis = Diagnosis by a mental health professional; Cut-Off Score = Diagnosis based on scoring above a critical threshold on a self-reported questionnaire.") %>% 
  theme_booktabs() %>% 
  bold(part = "header") %>% 
  font(fontname = "Arial", part = "all") %>% 
  fontsize(size = 10, part = "body") %>% 
  fontsize(size = 10, part = "header") %>% 
  fontsize(size = 8, part = "footer") %>% 
  autofit() 

#write.csv("tables/table_s1.csv") #Export to word: file to large, #doesnt work with flextable
doc_eTable1 <- read_docx()
doc_eTable1 <- body_add_flextable(doc_eTable1, value = eTable1_flex) %>% 
  body_end_section_landscape()  # a landscape section is ending here
#print(doc_eTable1, target = "tables/etable1.docx")
```

<br>

### Table S2: Effect size dependency

```{r}
table_s2_estimators <- tribble(
  ~"Estimator", ~"Dependency", ~"k", ~"g", ~"95% CI lower bound", ~"95% CI upper bound",
  "REML", "ignored",reml_dep$k[1], reml_dep$b[1] ,reml_dep$ci.lb[1] , reml_dep$ci.ub[1], 
  "REML", "averaged",reml_avg$k[1],  reml_avg$b[1] ,reml_avg$ci.lb[1] , reml_avg$ci.ub[1], 
  "3-lvl", "Modeled", reml_3lvl$k[1], reml_3lvl$b[1], reml_3lvl$ci.lb[1] , reml_3lvl$ci.ub[1], 
  "RVE", "Modeled", rve$k[1], rve$b[1], rve$ci.lb[1] , rve$ci.ub[1], 
  "FEM", "averaged", fem_avg$k[1], fem_avg$b[1], fem_avg$ci.lb[1] , fem_avg$ci.ub[1], 
  "Unweighted", "averaged", uw_avg$k[1], uw_avg$b[1], uw_avg$ci.lb[1] , uw_avg$ci.ub[1],
  "p-uniform", "averaged", puni_avg$k[1], puni_avg$est[1], puni_avg$ci.lb[1] , puni_avg$ci.ub[1],
  "PET-PEESE", "averaged", puni_avg$k[1], pet_peese_avg$b[1], pet_peese_avg$ci.lb[1] , pet_peese_avg$ci.ub[1]
) 

table_S2_estimators <- table_s2_estimators %>% 
  flextable() %>% 
  colformat_num(col_keys = colkeys, digits = 0, na_str = "NA", big.mark = "") %>% 
  add_header_lines(values = c("Effect size estimates based on different approaches to address effect size dependency.", "Table 2")) %>%
  add_footer_lines(values = "Notes. ") %>% 
  theme_booktabs() %>% 
  italic(italic = TRUE, part = "header") %>% 
  autofit() 
table_S2_estimators

doc_s2 <- read_docx()
doc_s2 <- body_add_flextable(doc_s2, value = table_S2_estimators)
#print(doc_s2, target = "tables/table_S2_estimators.docx")
```

<br>

### Table S3: Which factors

```{r}
table_s3_control     
table_s3_diagnosis   
table_s3_format      
table_s3_ma_method   
table_s3_rob
table_s3_target_group
table_s3_type        
```

<br>

### Table S4: Mean summary effect & 95% CI

```{r}
table_s4 <- tribble(~"# Specifications", ~"k", ~"Mean g", ~"95% CI LB", ~"95% CI UB", ~"Minimum", ~"Maximum", ~"25% Quantile", ~"75% Quantile",
        "k  > 10",nrow(specifications_k10), mean(specifications_k10$mean), mean(specifications_k10$lb), mean(specifications_k10$ub), min(specifications_k10$mean), max(specifications_k10$mean), quantile_k10[2], quantile_k10[4],
        "k  > 25", nrow(specifications_k25), mean(specifications_k25$mean), mean(specifications_k25$lb), mean(specifications_k25$ub), min(specifications_k25$mean), max(specifications_k25$mean), quantile_k25[2], quantile_k25[4],
        "k  > 50", nrow(specifications_k50), mean(specifications_k50$mean), mean(specifications_k50$lb), mean(specifications_k50$ub), min(specifications_k50$mean), max(specifications_k50$mean), quantile_k50[2], quantile_k50[4],
        )  
```


<br>


```{r}
table_s4 <- table_s4 %>%  
  flextable() %>% 
  colformat_num(col_keys = colkeys, digits = 0, na_str = "NA", big.mark = "") %>% 
  add_header_lines(values = c("Mean summary effect sizes for different numbers of included primary studies.", "Table S4")) %>%
  add_footer_lines(values = "Notes. k = number of included meta-analyses; Mean g = mean Hedges g of all included meta-analyses; 95% CI LB = Lower bound of 95% confidence interval; 95% CI UB = Upper bound of 95% confidence interval; Minimum = smallest summary effect size estimate; Maximum = largest summary effect size estimate; 25% Quantile = 25% of studies have a Hedges g equal or lower than this value; 75% Quantile = 75% of studies have a Hedges g equal or lower than this value, 50% of all summary effect size estimates are between these two quantiles") %>% 
  theme_booktabs() %>% 
  italic(italic = TRUE, part = "header") %>% 
  autofit() 
table_s4

# Export to word
doc_s4 <- read_docx()
doc_s4 <- body_add_flextable(doc_s4, value = table_s4)
#print(doc_s4, target = "tables/table_s3.docx")
```

<br>

### Saving all Supplementary Tables

```{r}
sect_properties <- prop_section(
  page_size = page_size(orient = "portrait",
                        width = 8.3, height = 11.7),
  type = "nextPage",
  page_margins = page_mar()
)

table_S2_estimators <- autofit(table_S2_estimators)

#save_as_docx(
#  "Table S2 Different estimators for effect size-dependency"  = table_S2_estimators  ,
#  "Table S3 - Control condition" = table_s3_control     ,
#  "Table S3 - Diagnosis" = table_s3_diagnosis   ,
#  "Table S3 - Format" = table_s3_format      ,
#  "Table S3 - Method"= table_s3_ma_method   ,
#  "Table S3 - Risk of bias" = table_s3_rob,
#  "Table S3 - Target gGroup" = table_s3_target_group,
#  "Table S3 - Type"        = table_s3_type        ,
#  "Table S4 Sensitivity analyses for multiverse size" = table_s4             ,
#  path = "tables/supplementary_tables.docx", 
#  pr_section = sect_properties)
```

<br>

# Figures

## Manuscript

### Figure 1: PRISMA Flow Chart
<br>

### Figure 2: Descriptive Specification Curve
```{r , echo=FALSE, out.width = '100%'}
#knitr::include_graphics("figures/descriptive_specification.png")
```

*Descriptive Specification Curve: Psychological Treatments for Depression* 

*Figure 2.* The top panel shows the meta-analytic summary effects (*g*) for each specification with their 95% confidence interval. The summary effects are sorted by their magnitude, ranging from lower to higher. Connecting the different summary effects results in the solid line, which is the specification curve. A horizontal dashed line of no effect is shown at *g* = 0 and a red dotted line is shown to indicate a clinically relevant effect size at *g* = 0.24. The vertical columns in the bottom panel represent factor combinations of How factors (different target groups, formats, therapy types, control groups, and diagnoses) and Which factors (fixed effect model, random effects model, unweighted model, p-uniform*) that constitute a given specification. Each vertical column is color-coded, signifying the number of samples included in a specification (hot spectral colors for more included samples vs. cool spectral colors for less included samples). 

The overall pattern of the specification curve indicates that specifications with many samples and/or narrow confidence intervals are closer to the interquartile range of estimated means as opposed to specifications with only a few samples and/or wider confidence intervals.


<br>

### Figure 3 : Inferential Specification Curve

```{r , echo=FALSE, out.width = '100%'}
#knitr::include_graphics("figures/boot_plot_test.png")
```

*Figure 3.* Depicted are three inferential specification curves, each summarizing magnitude-sorted meta-analytic summary effects and the corresponding pointwise 97.5% and 2.5% quantiles of 1,000 (grey area) specification curves simulated under the scenario of a null effect. The left panel depicts a fixed effect scenario of no heterogeneity (τ = 0), the middle panel a scenario of heterogeneity equal to the random-effects model, and the right panel the scenario of the upper 95% CI estimate of τ from the random-effects model. Each is simulated under the null hypothesis for a given specification number using a parametric bootstrap procedure, but they differ in underlying heterogeneity assumptions.  If the specification curve exceeds the limits of the 95% CI (as is the case in this plot), there is evidence against the null hypothesis (g = 0), indicating that there is a substantial effect. 
<br>


### Figure 4: Combinatorial Meta-Analysis REM


```{r, echo=FALSE, out.width = '100%'}
#knitr::include_graphics("figures/gosh_fem.pdf")
```

*Figure 4.* GOSH Plot. This GOSH plot visualizes the cross-study effect heterogeneity of a random sample of 100,000 subsets for the combinatorial meta-analysis. The y-axis depicts Higgins I^2^ statistic for heterogeneity and the summary effect size is visualized on the x-axis. Density distributions are visualized next to the respective axes. 

<br>
### S-Values

```{r}
# S value densities ---------------------------------------------
## Male Samples
## Compute S-values, simulated S-values and approximate exp(log(2))
## density function
load("fboe/Data/res_male_full.RData")
specifications_full %>% summary(p)
s_val <- data.frame(s_value = sort(-log2(specifications_full$p)))
density_sim <- approxfun(density(-log2(runif(nrow(s_val)))))
s_val$sim <- density_sim(s_val$s_value)
s_val$exp <- dexp(s_val$s_value, log(2))

density_obs <- approxfun(density(s_val$s_value))
s_val$obs <- density_obs(s_val$s_value)

s_val_long <- s_val %>% 
  pivot_longer(-s_value,names_to = "type", values_to = "value" )

s_val_long$type <- factor(s_val_long$type, levels = c("obs", "exp", "sim"), labels = c("Observed", "Expected", "Simulated"))


s_plot <- ggplot(s_val_long, 
                 aes(x = s_value, y = value, color = type))+
  geom_line(lwd = 1)+
  scale_color_manual(values = c("#E69F00", "#56B4E9", "#009E73"), name = "")+
  ggtitle(expression(italic("S")*" Values: Psychotherapy for Depression"))+
  xlab("S")+
  ylab("Density")+
  theme_bw()+
  theme(
    legend.position = c(.9,.8),
    legend.text = element_text(size = 10),
    axis.title.x = element_text(size = 10, face = "italic"),
    axis.title.y = element_text(size = 10),
    axis.text = element_text(size = 10),
    plot.title = element_text(size = 10)
  )

# ggsave("./figures/s_value_male.png", plot = s_plot_male, device = "png", dpi = 600,
#        height = 10, width = 16, units = "cm")

# ggsave("./figures/s_value_male.svg", plot = s_plot_male,
#        height = 
```


### Albatross Plot

#### Hedges *g*
```{r}
# Load data, compute effect sizes, total N and p-values
data %>% 
  mutate(
    pval = 2*(1-pnorm(abs(yi/sqrt(vi)))), 
    N = sample_size
  ) %>% 
  mutate(
    # The p-values are log-transformed. 
    # Note that there are "negative" p-values. This allows for plotting
    # the direction of the effect
    p_log = -sign(yi)*log10(pval)
  )

# Define Contours 
# p := observed p-value
es_contour <- function(r,yi,p){
  # transform p- to z-values
  z <- qnorm(1-10^-abs(p)/2)
  # log10 of the formula given by Harrison et al. (2017)
  # formula for Standardized mean difference (SMD) unequal sized groups
  
  out <- (2*(r+1)^2 + r*yi^2)/(2+r+yi^2)*z^2
  return(out)
}

albatross_plot <- function(data, yi = c(0, 0.24, 0.5), # or := potential odds ratios
                           r = 1, # r := ratio of group sizes (presumably 1)
                           font_size = 11,
                           title_position = 0,
                           title_size = 11,
                           line_width = 1,
                           y_lab = "Sample Size"){
  
  
  # Set and format the breaks on the x-axis
  breaks <- c(log10(.0001), log10(.001), log10(.01), log10(.05), log10(.1), log10(1))
  breaks <- c(breaks, -rev(breaks[-length(breaks)]))
  break_labels <- 10^-abs(breaks)
  break_labels[c(1,length(break_labels))] <- expression(10^-4)
  
    # Supposedly these are colorblind-friendly 
    line_colors <- c( "#E69F00", "#56B4E9", "#009E73",
                     "#F0E442", "#0072B2", "#D55E00", 
                     "#CC79A7", "#999999")[1:length(or)]
  
  p <- ggplot(data, aes(x = p, y = log10(N)))+
    coord_cartesian(ylim = c(1,6), clip = "off", expand = F)+
    scale_x_continuous(limits = c(-4.1, 4.1), breaks = breaks, labels = break_labels)+
    scale_y_continuous(breaks = 2:6,
                       labels = scales::trans_format("identity", scales::math_format(10^.x))
                       )
  
  l <- list()
  for(i in 1:length(or)){
    l[[i]] <- bquote(stat_function(aes(color = paste0('lnOR = +/-', round(log(.(or[i])),2),
                                       '\n',"(OR = ", round(.(or[i]),2),")")), 
                                   fun = es_contour, 
                                   args = list(or = .(or[i]), r = .(r), p2 = .(p2)), 
                                   lwd = .(line_width), n = 500))
  }
  p+
    sapply(l, eval)+
    geom_point()+
    labs(
      title = "Albatross Plot: Male Samples",
      x = expression(italic("P")*" value"),
      y = y_lab
    )+
    scale_color_manual("", values = line_colors)+
    annotate("text", label = c("Negative Association", "Null", "Positive Association"), 
               x = c(-2,0,2), y = .2,
               size = font_size*5/14)+
    theme_bw()+
    theme(
      legend.position = "bottom",
      plot.title = element_text(hjust = title_position, size = font_size),
      legend.text = element_text(size = font_size),
      axis.text.x = element_text(size = font_size, vjust = 0.5),
      axis.text.y = element_text(size = font_size),
      legend.box.margin = margin(.6,.6,.6,.6, unit = "cm")
    )
}

albatross_plot(es_male, or = c(1.01, 1.06, 1.13, 1.3, 2), y_lab = "Number of Older Siblings",
               font_size = 10, line_width = 0.5)

# ggsave("./figures/albatross.svg", width = 20, height = 15, units = "cm")
# ggsave("./figures/albatross.png", width = 20, height = 15, units = "cm", dpi = 600)


```

og johannes
```{r}
# Load data, compute effect sizes, total N and p-values
es_male <- read_csv("fboe/Data/maleData.csv") %>% 
  escalc(
    "OR",
    ai = obHom,
    bi = osHom,
    ci = obHet,
    di = osHet,
    data = .
  )  %>% 
  mutate(
    pval = 2*(1-pnorm(abs(yi/sqrt(vi)))), 
    N = obHom+osHom+obHet+osHet
  ) %>% 
  mutate(
    # The p-values are log-transformed. 
    # Note that there are "negative" p-values. This allows for plotting
    # the direction of the effect
    p_log = -sign(yi)*log10(pval)
  )

# Define Contours 
# p := observed p-value
es_contour <- function(r,p2,or,p){
  # transform p- to z-values
  z <- qnorm(1-10^-abs(p)/2)
  # log10 of the formula given by Harrison et al. (2017)
  out <- log10(ceiling(((r+1)*((1-p2+p2*or)^2+r*or))/(r*p2*(1-p2)*or*log(or)^2)*z^2))
  out[out>=log10(10^6)] <- NA
  out[out<log10(10)] <- log10(10)
  return(out)
}

albatross_plot <- function(data, or = c(1.01, 1.06, 1.13), # or := potential odds ratios
                           r = 1, # r := ratio of group sizes (presumably 1)
                           p2 = .515, # p2 := proportion of cases in reference group (.515)
                           font_size = 11,
                           title_position = 0,
                           title_size = 11,
                           line_width = 1,
                           y_lab = "Sample Size"){
  
  
  # Set and format the breaks on the x-axis
  breaks <- c(log10(.0001), log10(.001), log10(.01), log10(.05), log10(.1), log10(1))
  breaks <- c(breaks, -rev(breaks[-length(breaks)]))
  break_labels <- 10^-abs(breaks)
  break_labels[c(1,length(break_labels))] <- expression(10^-4)
  
    # Supposedly these are colorblind-friendly 
    line_colors <- c( "#E69F00", "#56B4E9", "#009E73",
                     "#F0E442", "#0072B2", "#D55E00", 
                     "#CC79A7", "#999999")[1:length(or)]
  
  p <- ggplot(data, aes(x = p_log, y = log10(N)))+
    coord_cartesian(ylim = c(1,6), clip = "off", expand = F)+
    scale_x_continuous(limits = c(-4.1, 4.1), breaks = breaks, labels = break_labels)+
    scale_y_continuous(breaks = 2:6,
                       labels = scales::trans_format("identity", scales::math_format(10^.x))
                       )
  
  l <- list()
  for(i in 1:length(or)){
    l[[i]] <- bquote(stat_function(aes(color = paste0('lnOR = +/-', round(log(.(or[i])),2),
                                       '\n',"(OR = ", round(.(or[i]),2),")")), 
                                   fun = es_contour, 
                                   args = list(or = .(or[i]), r = .(r), p2 = .(p2)), 
                                   lwd = .(line_width), n = 500))
  }
  p+
    sapply(l, eval)+
    geom_point()+
    labs(
      title = "Albatross Plot: Male Samples",
      x = expression(italic("P")*" value"),
      y = y_lab
    )+
    scale_color_manual("", values = line_colors)+
    annotate("text", label = c("Negative Association", "Null", "Positive Association"), 
               x = c(-2,0,2), y = .2,
               size = font_size*5/14)+
    theme_bw()+
    theme(
      legend.position = "bottom",
      plot.title = element_text(hjust = title_position, size = font_size),
      legend.text = element_text(size = font_size),
      axis.text.x = element_text(size = font_size, vjust = 0.5),
      axis.text.y = element_text(size = font_size),
      legend.box.margin = margin(.6,.6,.6,.6, unit = "cm")
    )
}

albatross_plot(es_male, or = c(1.01, 1.06, 1.13, 1.3, 2), y_lab = "Number of Older Siblings",
               font_size = 10, line_width = 0.5)

# ggsave("./figures/albatross.svg", width = 20, height = 15, units = "cm")
# ggsave("./figures/albatross.png", width = 20, height = 15, units = "cm", dpi = 600)


```


## Supplementary Material

### Figure S1
```{r}
figure_s3_control
figure_s3_diagnosis
figure_s3_format
figure_s3_rob
figure_s3_target_group
figure_s3_type
figure_s3_ma_method
```


<br>

# Manuscript
<br>

## Abstract


*Results:* Our multiverse meta-analysis produced `r nrow(specifications_k10)` non-redundant meta-analyses based on `r max(data$es_id)` included effect sizes from `r length(unique(data$study))` included primary studies. The estimated summary effect sizes ranged from Hedges’ *g* = `r min(specifications_k10$mean, na.rm = T)` to
`r max(specifications_k10$mean)`, *g~mean~* = `r mean(specifications_k10$mean, na.rm = T)`, *g~median~*  =
`r median(specifications_k10$mean, na.rm = T)`. In total, `r round(percentage_mean_over_clinically_relevance, 0)`% of these meta-analyses reached a clinically relevant magnitude of Hedges´ *g* > 0.24, and `r round(percentage_ci_over_clinically_relevance,0)`% had 95% *CIs* larger than this clinically relevant cutoff. 

*Conclusions and relevance:* Most meta-analyses produced clinically relevant effect sizes, suggesting the overall robustness of the effectiveness of psychological interventions for depression. Notably, studies with low risk of bias and waitlist control groups inflated effect sizes slightly. Earlier discrepancies between meta-analyses could be solved. Thus, we propose that these discussions about the effectiveness of psychological interventions for depression can cease.
<br>


## Results

<br>

### Selection and included studies
We screened xxx titles and examined xxx full-text papers, of which xxx were excluded. The PRISMA flow chart depicts the study inclusion process in Figure 1. 


Overall, *k~es~* = `r max(data$es_id)` effect sizes from *k~studies~* = `r length(unique(data$study))` studies were included. The sample sizes of the included primary studies ranged from *N* = `r min(data$sample_size, na.rm = T)` to 
`r max(data$sample_size, na.rm = T)`, *N~mean~* = `r sample_size_data$sample_size_mean`, *N~median~* =
`r sample_size_data$sample_size_median`. The total sample size of all included samples from all primary studies was *N~total~* = `r sample_size_data$sample_size_sum`. See Online Supplement eTable 1 for a detailed description of each included effect size and primary study.

<br>

### Characteristics of included primary studies

Most included primary studies investigated the efficacy of psychological interventions for depression in adults (`r round(nrow(filter(data_avg, target_group == "adults"))/nrow(data_avg)*100,0)`%) or general medical populations (`r round(nrow(filter(data_avg, target_group == "general medical"))/nrow(data_avg)*100,0)`%). Most studies were either conducted in Europe (`r round(nrow(filter(data_avg, region == "europe"))/nrow(data_avg)*100,0)`%), or in Northern America (`r round(nrow(filter(data_avg, region == "north america"))/nrow(data_avg)*100,0)`%).  

Most studies used CBT-based interventions (`r round(nrow(filter(data_avg, type == "cbt-based"))/nrow(data_avg)*100,0)`%) and were primarily delivered in individual therapy format (`r round(nrow(filter(data_avg, format == "individual"))/nrow(data_avg)*100,0)`%). Care as usual was the most common type of control condition (`r round(nrow(filter(data_avg, control == "cau"))/nrow(data_avg)*100,0)`%), and the depressive disorder was diagnosed by a clinician in `r round(nrow(filter(data_avg, diagnosis == "diagnosis"))/nrow(data_avg)*100,0)`% of studies. Only `r round(nrow(filter(data_avg, risk_of_bias == "low"))/nrow(data_avg)*100,0)`% of studies were rated with a low risk of bias. See Table 1 for the study characteristics and Online Supplement eTable 2 for the effect size characteristics.

<br>

### Multiverse Meta-Analysis

#### Magnitude of effect sizes found in meta-analyses

Our multiverse meta-analysis produced `r nrow(specifications_k10)` non-redundant meta-analyses, with effect sizes ranging from `r range(specifications_k10$mean)[1]` to `r range(specifications_k10$mean)[2]`. Half of those effect sizes were in the interquartile range of
`r iqr_k10[1]` to `r iqr_k10[2]`. In total, `r percentage_mean_over_zero_k10` of the effecct sizes were greater than 0, and `r round(percentage_ci_over_zero_k10,0)`% of these had 95% *CIs* that did not include 0 (i.e., estimated *g* was greater than 0 which would have returned a two-tailed *p*-value of less than .05).

In total, `r round(percentage_mean_over_clinically_relevance, 0)`% reached a clinically relevant magnitude of Hedges *g* > 0.24 (Cuijpers et al., 2014), and `r round(percentage_ci_over_clinically_relevance,0)`% of the summary effect sizes had 95%*CIs* larger than the clinically relevant cutoff.

<br>

#### Pattern

The overall pattern of the specification curve indicates that larger meta-analyses, including more primary studies, had medium-large effect sizes and were close to the interquartile range of estimated effect sizes of the multiverse. More extreme meta-analyses were associated with few included studies and, therefore broader confidence intervals. 

<br>

### Factors systematically influencing the magnitude

Several Which factors produced—on average—larger summary effect size estimates compared to others. In the following, we summarize the most important results. For a more detailed breakdown of each Which Factor, see eFigures 1-8 and eTables 3-10 in the Online Supplement.

#### **Target group**

Meta-analyses including studies investigating 
`r slice_max(s3_target_group, s3_target_group$"Mean g")[[1]]`, mean *g* =
`r slice_max(s3_target_group, s3_target_group$"Mean g")[[2]]`, 95% *CI*
[`r slice_max(s3_target_group, s3_target_group$"Mean g")[[3]]`,
`r slice_max(s3_target_group, s3_target_group$"Mean g")[[4]]`] with *k* =
`r slice_max(s3_target_group, s3_target_group$"Mean g")[[5]]`,
produced larger effect size estimates than meta-analyses on
`r slice_min(s3_target_group, s3_target_group$"Mean g")[[1]]`, mean *g* =
`r slice_min(s3_target_group, s3_target_group$"Mean g")[[2]]`, 95% *CI*
[`r slice_min(s3_target_group, s3_target_group$"Mean g")[[3]]`,
`r slice_min(s3_target_group, s3_target_group$"Mean g")[[4]]`] with *k* =
`r slice_min(s3_target_group, s3_target_group$"Mean g")[[5]]`.

<br>

#### Format

Meta-analyses including studies delivered in 
`r slice_max(s3_format, s3_format$"Mean g")[[1]]` format, mean *g* =
`r slice_max(s3_format, s3_format$"Mean g")[[2]]`, 95% *CI*
[`r slice_max(s3_format, s3_format$"Mean g")[[3]]`,
`r slice_max(s3_format, s3_format$"Mean g")[[4]]`] with *k* =
`r slice_max(s3_format, s3_format$"Mean g")[[5]]`,
produced larger effect size estimates than meta-analyses delivered in 
`r slice_min(s3_format, s3_format$"Mean g")[[1]]` format, mean *g* =
`r slice_min(s3_format, s3_format$"Mean g")[[2]]`, 95% *CI*
[`r slice_min(s3_format, s3_format$"Mean g")[[3]]`,
`r slice_min(s3_format, s3_format$"Mean g")[[4]]`] with *k* =
`r slice_min(s3_format, s3_format$"Mean g")[[5]]`.

<br>

#### Type

Meta-analyses including samples based on 
`r slice_max(s3_type, s3_type$"Mean g")[[1]]`, mean *g* =
`r slice_max(s3_type, s3_type$"Mean g")[[2]]`, 95% *CI*
[`r slice_max(s3_type, s3_type$"Mean g")[[3]]`,
`r slice_max(s3_type, s3_type$"Mean g")[[4]]`] with *k* =
`r slice_max(s3_type, s3_type$"Mean g")[[5]]`,
produced larger effect size estimates than meta-analyses based on
`r slice_min(s3_type, s3_type$"Mean g")[[1]]`, mean *g* =
`r slice_min(s3_type, s3_type$"Mean g")[[2]]`, 95% *CI*
[`r slice_min(s3_type, s3_type$"Mean g")[[3]]`,
`r slice_min(s3_type, s3_type$"Mean g")[[4]]`] with *k* =
`r slice_min(s3_type, s3_type$"Mean g")[[5]]`.

<br>

#### Control Group

Meta-analyses that included samples compared with a
`r slice_max(s3_control, s3_control$"Mean g")[[1]]`, mean *g* =
`r slice_max(s3_control, s3_control$"Mean g")[[2]]`, 95% *CI*
[`r slice_max(s3_control, s3_control$"Mean g")[[3]]`,
`r slice_max(s3_control, s3_control$"Mean g")[[4]]`] with *k* =
`r slice_max(s3_control, s3_control$"Mean g")[[5]]`,
produced larger effect size estimates than meta-analyses compared with
`r slice_min(s3_control, s3_control$"Mean g")[[1]]`, mean *g* =
`r slice_min(s3_control, s3_control$"Mean g")[[2]]`, 95% *CI*
[`r slice_min(s3_control, s3_control$"Mean g")[[3]]`,
`r slice_min(s3_control, s3_control$"Mean g")[[4]]`] with *k* =
`r slice_min(s3_control, s3_control$"Mean g")[[5]]`.

<br>

#### Risk of Bias
```{r}
slice_max(s3_rob, s3_rob$"Mean g")[[1]]
```

Meta-analyses that included samples with a
`r slice_max(s3_rob, s3_rob$"Mean g")[[1]]`, mean *g* =
`r slice_max(s3_rob, s3_rob$"Mean g")[[2]]`, 95% *CI*
[`r slice_max(s3_rob, s3_rob$"Mean g")[[3]]`,
`r slice_max(s3_rob, s3_rob$"Mean g")[[4]]`] with *k* =
`r slice_max(s3_rob, s3_rob$"Mean g")[[5]]` included
samples, produced larger effect size estimates than meta-analyses
including with a `r slice_min(s3_rob, s3_rob$"Mean g")[[1]]`, mean
*g* = `r slice_min(s3_rob, s3_rob$"Mean g")[[2]]`, 95% *CI*
[`r slice_min(s3_rob, s3_rob$"Mean g")[[3]]`,
`r slice_min(s3_rob, s3_rob$"Mean g")[[4]]`] with *k* =
`r slice_min(s3_rob, s3_rob$"Mean g")[[5]]`.

<br>

#### Diagnosis

Meta-analyses that choose to 
`r slice_max(s3_diagnosis, s3_diagnosis$"Mean g")[[1]]`, mean *g* =
`r slice_max(s3_diagnosis, s3_diagnosis$"Mean g")[[2]]`, 95% *CI*
[`r slice_max(s3_diagnosis, s3_diagnosis$"Mean g")[[3]]`,
`r slice_max(s3_diagnosis, s3_diagnosis$"Mean g")[[4]]`] with *k* =
`r slice_max(s3_diagnosis, s3_diagnosis$"Mean g")[[5]]`,
produced larger effect size meta-analyses that 
`r slice_min(s3_diagnosis, s3_diagnosis$"Mean g")[[1]]`, mean *g* =
`r slice_min(s3_diagnosis, s3_diagnosis$"Mean g")[[2]]`, 95% *CI*
[`r slice_min(s3_diagnosis, s3_diagnosis$"Mean g")[[3]]`,
`r slice_min(s3_diagnosis, s3_diagnosis$"Mean g")[[4]]`] with *k* =
`r slice_min(s3_diagnosis, s3_diagnosis$"Mean g")[[5]]`.

<br>

#### Meta-Analytical Method

Meta-analyses were analyzed with
`r slice_max(s3_ma_method, s3_ma_method$"Mean g")[[1]]`, mean *g* =
`r slice_max(s3_ma_method, s3_ma_method$"Mean g")[[2]]`, 95% *CI*
[`r slice_max(s3_ma_method, s3_ma_method$"Mean g")[[3]]`,
`r slice_max(s3_ma_method, s3_ma_method$"Mean g")[[4]]`] with *k* =
`r slice_max(s3_ma_method, s3_ma_method$"Mean g")[[5]]`,
produced larger effect size estimates than meta-analyses analyzed with
`r slice_min(s3_ma_method, s3_ma_method$"Mean g")[[1]]`, mean *g* =
`r slice_min(s3_ma_method, s3_ma_method$"Mean g")[[2]]`, 95% *CI*
[`r slice_min(s3_ma_method, s3_ma_method$"Mean g")[[3]]`,
`r slice_min(s3_ma_method, s3_ma_method$"Mean g")[[4]]`] with *k* =
`r slice_min(s3_ma_method, s3_ma_method$"Mean g")[[5]]`.

<br>

## Inferential Specification Curve Plot
The inferential specification curve analysis substantially deviated from the scenario of no effect (*g* = 0). Ranging from simulating no heterogeneity (τ = 0), to the identified heterogeneity of the random effects model (τ = `r round(tau_reml_avg$random[2], 2)` ) and the 95% CI upper threshold of τ =`r round(tau_reml_avg$random[2,3], 2)`, 100% of meta-analyses were outside the expected area for values under the scenario of no effect. This indicates that in all scenarios, treatments of depression would be considered effective.

## Combinatorial Meta-Analysis
One hundred thousand random samples from all possible subset combinations of the xxx included effect sizes revealed a similar picture as the descriptive specification curve: most meta-analyses fall in the interquartile range `r iqr_k10[1]` to `r iqr_k10[2]` of the multiverse meta-analysis, while heterogeneity is substantial. Meta-analyses containing studies with effect sizes larger than Hedges g >= 3 produced larger summary effect size estimates and had overall more heterogeneity than studies without those potential outliers.

<br>

## Conventional 3-level Meta-Analysis
Choosing one exemplary meta-analytical specification out of the `r nrow(specifications_k10)` meta-analyses could be the following: a three-level meta-analysis including all `r max(data$es_id)` effect sizes from all  `r length(unique(data$study))` studies included in the database. The pooled Hedges´ *g* based on this three-level meta-analytic model was *g* = `r reml_3lvl$b`, 95%CI [`r reml_3lvl$ci.lb`; `r reml_3lvl$ci.ub`], *p* < 0.001). 

The overall *I^2^* value, indicating how much of the total variance can be attributed to the total amount of heterogeneity, is very large, with approximately `r I2_overall`% of the total variance attributable to heterogeneity. About *I^2^ ~Level3~* = `r I2[1]`% of the total variance is estimated to be due to between-cluster heterogeneity, with the remaining *I^2^ ~Level2~* = `r I2[2]`% due to within-cluster heterogeneity. 

<br>

```{r}
sessionInfo()
```

