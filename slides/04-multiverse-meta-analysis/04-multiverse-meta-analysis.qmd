---
title: Multiverse
subtitle: Replicability Crisis in Science?
code-fold: true
---

```{r}
#| label: setup
#| include: false
knitr::opts_chunk$set(echo = TRUE,
                      dev = "svg",
                      fig.align = "center")

library(kableExtra)
library(tidyverse)
library(cowplot)
library(latex2exp)
library(metafor)

specif <- readRDS("objects/plessen2023-specification-data.rds")
pimaex <- readRDS("objects/pima-example.rds")
specif_res <- specif$specif |> 
  mutate(res = map(fit, filor::summary_rma)) |> 
  unnest(res)

mtheme <- function(){
  theme_minimal(15)
}

theme_set(mtheme())
```

# Multiverse analysis {.section}

## Multiverse analysis @Steegen2016-lz

- Even a simple data analysis is characterized by several (often arbitrary) choices
- The impact of these choices is not always clear, known or easy to predict
- Researchers (with or without awareness) usually report a single set of choices 

## Some examples

- using a predictor as continous or choosing some thresholds and transforming to categorical
- including or excluding an observation (e.g., outlier)
- including or excluding a covariate (e.g., controlling for age or not)
- using a linear model or an ordinal regression for discrete ordered data
- ...

## The garden of forking paths

A tree of possibilities...

![](img/forking-paths.svg)

## The garden of forking paths

... Where only some of them produce a certain result.

![](img/forking-paths-color.svg)


## Only plausible vs all scenarios

Not all scenarios are equally plausible or reasonable. Thus the multiverse is not the entire set but a non-random sample of plausibile choices.

```{r}
x <- seq(0, 1, 0.0001)
y <- dexp(x, 2)
y <- (y-min(y))/(max(y)-min(y))

plot(x*100, y, type = "l",
     xlab = "All possible scenarios",
     ylab = "Plausibility",
     lwd = 2)
```

# Multiverse meta-analysis {.section}

## Meta-analysis many choices

Despite useful and very powerful, meta-analysis is characterized by several (arbitrary) choices. For example:

- Should the study $x$ be excluded for theoretical or statistical (e.g., outliers) reasons?
- Should we use an equal or random-effects model?
- Which value should take the pre-post missing correlation?
- ...

## An example: Pre-post designs

Let's imagine that we have $k$ studies about the efficacy of a certain treatment. They collected a sample of participants measuring a certain variable $y$ before and after the treatment.

With this design we can summarise the effect computing the difference between the average of the two time points.

An effect size (e.g., Cohen's $d$) can be calculated as:

$$
d = \frac{\overline y_{post} - \overline y_{pre}}{\sigma_p}
$$

And $\sigma_p$ is the pooled pre-post standard deviation.

## An example: Pre-post Cohen's $d$

With a pre-post Cohen's $d$ we need the pre-post correlation $\rho$ to calculate the sampling variance:

$$
\sigma^2_{\epsilon_{pp}} = \frac{2(1 - \rho)}{n} + \frac{d^2}{2n}
$$

$\rho$ is usually non reported and need to be chosen from previous literature or a plausible guess.

## Pre-post Cohen's $d$

```{r}
#| out-width: 100%
#| fig-width: 10
v_dm <- function(r, d, n){
    2*(1-r)/n + d^2/(2*n)
}

vdd <- data.frame(x = seq(-1, 1, 0.001))
vdd$y <- v_dm(vdd$x, 0, 40)

ggplot(vdd, aes(x = x, y = y)) +
    geom_line() +
    xlab(expression(rho)) +
    ylab(expression(sigma^2)) +
    theme_minimal(25)
```

## A simulated example

```{r}
#| include: false

tidy.flip <- function(x){
    res <- data.frame(x@res)
    names(res) <- tolower(names(res))
    return(res)
}

set.seed(6264)
k <- 10 # number of studies
p <- 5 # number of measures
n <- round(runif(k, 10, 200)) # number of subjects
mu <- round(runif(p, -0.5, 1), 1)
tau2 <- round(runif(p, 0.01, 0.2), 2)
rho_pp <- 0.6 # pre-post correlation
rho_out <- round(runif(filor::ncor(p), 0, 0.7), 2) # correlation between outcomes
Rpp <- rho_pp + diag(1 - rho_pp, 2)
R <- filor::rmat(rho_out) # correlation matrix between outcomes
TM <- diag(sqrt(tau2)) %*% R %*% diag(sqrt(tau2)) # var-cov matrix of random effects
TM <- Matrix::nearPD(TM)$mat

j <- sample(1:p, k, replace = TRUE) # number of outcomes per study
mui <- MASS::mvrnorm(k, mu, TM)

dat <- data.frame(
    study = rep(1:k, j),
    outcome = filor::mseq(1, j)
)

dat$n <- rep(n, j)
dat$m_pre <- dat$m_post <- dat$sd_pre <- dat$sd_post <- dat$rpp <- NA

for(i in 1:k){ # each study
    m_pre <- m_post <- sd_pre <- sd_post <- rpp <- rep(NA, j[i])
    for(o in 1:j[i]){
        pre_post <- MASS::mvrnorm(n[i], c(0, mui[i, o]), Rpp)
        m_pre_post <- colMeans(pre_post)
        sd_pre_post <- apply(pre_post, 2, sd)
        m_pre[o] <- m_pre_post[1]
        m_post[o] <- m_pre_post[2]
        sd_pre[o] <- sd_pre_post[1]
        sd_post[o] <- sd_pre_post[2]
        rpp[o] <- cor(pre_post)[1, 2]
    }
    dat[dat$study == i, "m_pre"] <- m_pre
    dat[dat$study == i, "m_post"] <- m_post
    dat[dat$study == i, "sd_pre"] <- sd_pre
    dat[dat$study == i, "sd_post"] <- sd_post
    dat[dat$study == i, "rpp"] <- rpp
}

dat$sp <- sqrt((dat$sd_post^2 + dat$sd_pre^2)/2)

# multiverse

multi <- tidyr::expand_grid(rpp = c(0, 0.3, 0.5, 0.8),
                            rout = c(0, 0.3, 0.5, 0.8),
                            method = c("EE", "REML"))


multi_res <- vector(mode = "list", length = nrow(multi))

for(i in 1:length(multi_res)){
    dati <- dat
    dati$grpp <- multi$rpp[i]
    dati <- escalc("SMCR", m1i = m_pre, m2i = m_post, sd1i = sp, ri = grpp, ni = n, data = dati)
    dati <- aggregate(dati, cluster = study, rho = multi$rout[i])
    multi_res[[i]] <- rma(yi, vi, data = dati, method = multi$method[i])
}

B <- 1000
M <- nrow(multi)
S <- flip::make.signSpace(k, B)

zi <- matrix(NA, k, M)

for(i in 1:M){
    # estimating tau2 under the null hypothesis
    dati <- multi_res[[i]]$data
    fit0 <- rma.mv(yi, vi, random = ~1|study, data = dati, beta = 0)
    tau20 <- fit0$sigma2
    zi[, i] <- as.numeric(dati$yi) / sqrt(dati$vi + tau20)
}

multi_flip <- flip::flip(zi, perms = S)
multi_flip <- flip::flip.adjust(multi_flip)
multi_res_d <- bind_rows(map(multi_res, filor::summary_rma)) 
multi_flip_d <- bind_rows(map(multi_flip, tidy.flip))
multi <- cbind(multi, multi_flip_d, multi_res_d)
rownames(multi) <- NULL
multi <- multi |> 
    mutate(id = 1:n(),
           sign_before = p.value <= 0.05,
           sign_after = adjust.maxt <= 0.05,
           sign = case_when(
               !sign_before & !sign_after ~ "never",
               sign_before & !sign_after ~ "before",
               sign_before & sign_after ~ "always"
           ),
           sign = factor(sign, levels = c("never", "before", "always"))
    )
```

The data structure: multiple measures of the same outcome within each paper

```{r}
#| echo: false
dat_example <- escalc("SMCR", m1i = m_pre, m2i = m_post, sd1i = sp, ri = rep(0, nrow(dat)), ni = n, data = dat)

dat_example |>
  select(study, outcome, n, yi, vi) |> 
  data.frame() |> 
  mutate(across(where(is.numeric), \(x) round(x, 2))) |> 
  rename("ni" = n) |> 
  filor::trim_df(5) |> 
  kable() |> 
  kable_styling(full_width = FALSE, bootstrap_options = c("condensed"), font_size = 25)
```

## Simulation approach

Let's make an example for a paper with $j = 3$ measures of the outcome:

$$
\begin{bmatrix}
        y_{11} \\
        y_{12} \\
        y_{13}
    \end{bmatrix}
    =
    \begin{bmatrix}
        \mu_{\theta_1} \\
        \mu_{\theta_2} \\
        \mu_{\theta_3}
    \end{bmatrix}
    +
    \begin{bmatrix}
        \delta_{\theta_1} \\
        \delta_{\theta_2} \\
        \delta_{\theta_3}
    \end{bmatrix}
    +
    \begin{bmatrix}
        \epsilon_{\theta_{11}} \\
        \epsilon_{\theta_{12}} \\
        \epsilon_{\theta_{13}}
\end{bmatrix}
$$

$$
\boldsymbol{\delta} \sim \text{MVN}\left(
    \begin{matrix}
    0 \\
    0 \\
    0
    \end{matrix}
    \
    \begin{matrix}
        \tau^2_1 &        & \\
        \rho_{21}\tau_2\tau_1        & \tau^2_1 & \\
         \rho_{31}\tau_2\tau_1        &  \rho_{32}\tau_2\tau_1        & \tau^2_1 \\
    \end{matrix}
    \right)
$$

$$
\boldsymbol{\epsilon} \sim \text{MVN}\left(
    \begin{matrix}
    0 \\
    0 \\
    0
    \end{matrix}
    \
    \begin{matrix}
        \sigma^2_{\epsilon_1} &        & \\
        \rho_{21}\sigma^2_{\epsilon_2}\sigma^2_{\epsilon_1}        & \sigma^2_{\epsilon_2} & \\
        \rho_{31}\sigma^2_{\epsilon_3}\sigma^2_{\epsilon_1}         &  \rho_{32}\sigma^2_{\epsilon_3}\sigma^2_{\epsilon_2}        & \sigma^2_{\epsilon_3} \\
    \end{matrix}
    \right)
$$

## Simulation approach

We simulated individual participant data, thus:

1. Sampling the true values $\theta_{ij}$ for each study $i$ and outcome $j$ from the multivariate distribution
2. Generating $n_i$ pre and post data with correlation $\rho$
3. Calculating the effect size (imputing the pre-post correlation)
5. Aggregating multiple outcomes within the same paper (imputing the correlation)
4. Fitting the meta-analyis model
5. Calculating the scores
7. Repeating 3-4 for each scenario
6. Using PIMA

## Simulation approach

We simulated a relatively simple but plausible multiverse with:

- `r length(unique(multi$rpp))` pre-post correlations
- `r length(unique(multi$rout))` correlations between multiple measures of the same outcome
- `r length(unique(multi$method))` meta-analysis models (fixed and random-effects)

For a total of `r nrow(multi)` multiverse scenarios.

## Results

```{r}
#| out-width: 80%
#| fig-align: center
p_beta <- ggplot(multi, aes(y = b)) +
    xlim(c(-0.5,0.5)) +
    geom_boxplot(width = 0.5,
                 fill = "dodgerblue",
                 alpha = 0.6) + 
    ylab(latex2exp::TeX("$\\mu_{\\,\\, \\theta}$")) +
    theme_minimal(30) +
    theme(axis.text.x = element_blank())

p_pval <- ggplot(multi, aes(y = -log10(p.value))) +
    xlim(c(-0.5,0.5)) +
    geom_hline(yintercept = -log10(0.05)) +
    geom_boxplot(width = 0.5,
                 fill = "dodgerblue",
                 alpha = 0.6) + 
    ylab(latex2exp::TeX("Raw $-log_{10}(p)$")) +
    theme_minimal(30) +
    theme(axis.text.x = element_blank())

rdata <- cor(zi)
rdata <- data.frame(r = rdata[upper.tri(rdata, diag = FALSE)])

p_cor <- ggplot(rdata, aes(y = r)) +
    xlim(c(-0.5,0.5)) +
    geom_boxplot(width = 0.5,
                 fill = "dodgerblue",
                 alpha = 0.6) + 
    ylab(latex2exp::TeX("$\\rho$")) +
    theme_minimal(30) +
    theme(axis.text.x = element_blank()) +
    ylim(c(-1,1))

cowplot::plot_grid(p_beta, p_cor, p_pval, nrow = 1)
```

## An example from @Plessen2023-ex

![](img/plessen2023.png)

## An example from @Plessen2023-ex

- Over the last four decades, more than 80 meta-analyses have examined the efficacy of psychotherapies for depression
- More than 700 randomised controlled trials (RCTs)
- Not all these studies goes in the same direction

## An example from @Plessen2023-ex

Discrepancies in results could be due to:

### Which factors (which data to meta-analyze)

- inclusion/esclusion of a subset of studies (e.g., low quality studies)
- type of control group or control therapy
- ...

### How factors (how to meta-analyze)

- type of model (e.g., equal vs random)
- model complexity (two-level, three level, robust, etc.)
- correcting for publication bias

# Descriptive tools {.section}

## Describing the multiverse

@Hall2022-mp and @Liu2021-xv proposed several tools to describe the results of a multiverse analysis.

The increase in complexity NEED to be managed using appropriate tools to summarise and visualize the results.

## Estimated effect of interest

```{r}
specif_res |> 
  ggplot(aes(x = b)) +
  geom_histogram(bins = 50,
                 fill = "dodgerblue",
                 col = "black") +
  ylab("Frequency") +
  xlab("Estimated Effect") +
  geom_vline(xintercept = 0, col = "red") +
  geom_vline(xintercept = 0.25, col = "green")
```

## Estimated effect as a function of scenarios^[The red line is the 0 effect and the green line is a minimum effect of interest in the research field]

```{r}
#| fig-height: 6
above <- specif_res |> 
  arrange(b) |> 
  mutate(id = 1:n()) |> 
  ggplot(aes(x = id, y = b)) +
  geom_segment(aes(xend = id, y = ci.lb, yend = ci.ub)) +
  geom_hline(yintercept = 0, col = "red") +
  geom_hline(yintercept = 0.25, col = "green") +
  theme(axis.title.y = element_blank(),
        axis.text.x = element_blank(),
        axis.title.x = element_blank())
  
below <- specif_res |> 
  arrange(b) |> 
  mutate(id = 1:n()) |> 
  pivot_longer(c(
    target_group,
    format,
    diagnosis,
    risk_of_bias,
    type,
    method
  )) |> 
  ggplot(aes(x = id, y = value)) +
  geom_tile() +
  theme(axis.title.y = element_blank(),
        axis.text.x = element_blank()) +
  xlab("Specification")

plot_grid(above, below, nrow = 2, align = "hv")
```

## Fixing a specific parameter

We can the effect of a set of scenarios having a certain parameter:

```{r}
specif_res |> 
  arrange(b) |> 
  mutate(id = 1:n()) |> 
  ggplot(aes(x = b, fill = diagnosis)) +
  geom_density(alpha = 0.5)
```

# Inferential tools {.section}

## Specification Curve @Simonsohn2020-sr

The idea of the specification curve is both descriptive (see previous plots) and inferential.

Inferentially, the approach requires to generate a new dataset under the null hypothesis and compare with the observed value.

## Specification Curve in meta-analysis @Plessen2023-ex

@Plessen2023-ex describe how to implement the method for meta-analysis.

1. For each dataset/model generate $k_s$ data (where $k$ is the number of effect in the specification $s$) sampling from $\mathcal{N}(0,\sigma^2_{\epsilon_i} + \tau^2)$.
2. Fit the model (FE or RE)
3. Get the specification curve
4. Repeat 1-3 for a large number of times (e.g., 1000)
5. Compute the 2.5% and 95.75% quantiles
6. Check if the observed specification is outside the 95% CI

## Specification Curve in meta-analysis @Plessen2023-ex

```{r}
#| code-fold: true
specif$SD |> 
  filter(spec %in% sample(1:1000, 1)) |> 
  ggplot(aes(x = id, y = b)) +
  geom_line(aes(group = spec))
```

## Specification Curve in meta-analysis @Plessen2023-ex

```{r}
#| code-fold: true
specif$SD |> 
  filter(spec %in% sample(1:1000, 1)) |> 
  ggplot(aes(x = id, y = b)) +
  geom_line(aes(group = spec))
```

## Specification Curve in meta-analysis @Plessen2023-ex

```{r}
#| code-fold: true
specif$SD |> 
  filter(spec %in% sample(1:1000, 1)) |> 
  ggplot(aes(x = id, y = b)) +
  geom_line(aes(group = spec))
```

## Specification Curve in meta-analysis @Plessen2023-ex

```{r}
#| code-fold: true
specif_res <- specif_res |> 
  arrange(b) |> 
  mutate(id = 1:n())

ggplot(data = specif$SD,
       aes(x = id, y = b)) +
  geom_line(aes(group = spec)) +
  geom_line(data = specif_res,
            aes(x = id, y = b),
            col = "firebrick",
            lwd = 2)
```

## PIMA

**P**-selection **I**nference in **M**ultiverse **M**eta-analysis (PIMA) is an inferential approach to multiverse analysis is an inferential framework for:

- obtaining an overall p-value across the multiverse
- obtaining corrected p-values based on resampling methods

## PIMA [@Girardi2024-ip]

![](img/girardi2024.pdf){width=80% fig-align="center"}

## PIMA on meta-analysis

We implemented PIMA also on meta-analysis but (for the moment) limited to two-level models without moderators.

![](img/wip.jpg)

## PIMA Results

```{r}
multi_flip_overall <- flip::npc(multi_flip, comb.funct = "maxT")
multi_p <- multi_flip_overall@res$`p-value`
fp <- function(p){
    ifelse(p <= 0.001, "p < 0.001", as.character(round(p, 3)))
}
```

The multiverse is associated with an overall p value of `r fp(multi_p)` ^[combined using the maxT method by @Westfall1993-ek]. Then we can describe the overall results:

```{r}
#| fig-out: 80%
#| fig-align: center
p_beta <- ggplot(multi, aes(y = b)) +
    xlim(c(-0.5,0.5)) +
    geom_boxplot(width = 0.5,
                 fill = "dodgerblue",
                 alpha = 0.6) + 
    ylab(latex2exp::TeX("$\\mu_{\\,\\, \\theta}$")) +
    theme_minimal(30) +
    theme(axis.text.x = element_blank())

p_pval <- ggplot(multi, aes(y = -log10(p.value))) +
    xlim(c(-0.5,0.5)) +
    geom_hline(yintercept = -log10(0.05)) +
    geom_boxplot(width = 0.5,
                 fill = "dodgerblue",
                 alpha = 0.6) + 
    ylab(latex2exp::TeX("Raw $-log_{10}(p)$")) +
    theme_minimal(30) +
    theme(axis.text.x = element_blank())

rdata <- cor(zi)
rdata <- data.frame(r = rdata[upper.tri(rdata, diag = FALSE)])

p_cor <- ggplot(rdata, aes(y = r)) +
    xlim(c(-0.5,0.5)) +
    geom_boxplot(width = 0.5,
                 fill = "dodgerblue",
                 alpha = 0.6) + 
    ylab(latex2exp::TeX("$\\rho$")) +
    theme_minimal(30) +
    theme(axis.text.x = element_blank()) +
    ylim(c(-1,1))

cowplot::plot_grid(p_beta, p_cor, p_pval, nrow = 1)
```

## Impact of multiplicity correction

```{r}
#| fig-out: 100%
#| fig-height: 5
#| fig-width: 8
#| fig-align: center
tp <- function(p){
    -log10(p)
}

lbl <- c("Never $p \\leq 0.05$", 
         "Before correction $p \\leq 0.05$", 
         "After correction $p \\leq 0.05$")


multi$sign <- factor(multi$sign, 
                     labels = latex2exp::TeX(lbl))

ggplot(multi, aes(x = tp(p.value), tp(adjust.maxt), color = sign)) +
    geom_hline(yintercept = tp(0.05), alpha = 0.4) +
    geom_vline(xintercept = tp(0.05), alpha = 0.4) +
    geom_abline(linetype = "dashed", lwd = 0.5) +
    geom_point(size = 5,
               position = position_jitter(width = 0.1),
               alpha = 0.5) +
    xlim(c(0, 3)) +
    ylim(c(0, 3)) +
    xlab(TeX("Raw p value ($-log_{10}$)")) +
    ylab(TeX("maxT p value ($-log_{10}$)")) +
    theme_minimal(base_size = 20) +
    theme(legend.title = element_blank(),
          legend.position = "bottom") +
    scale_color_manual(labels = scales::parse_format(), values = c("#F8766D", "#7CAE00", "#00BFC4"))
```

# Important aspects of multiverse analysis {.section}

## Important aspects of multiverse analysis

- Include only plausible scenarios. Regardless the aim (description or inference) of the multiverse, the results are meaningful if and only if scenarios are plausible.
- Scenarios are assumed to be equally plausible but we could imagine a plausibility weight (future direction)
- Multiverse analysis is also useful to estimate the degree of variability according to plausible choices

# Some other multiverse examples and resources {.section}

## R Packages

- https://mucollective.github.io/multiverse/
- https://mverseanalysis.github.io/mverse/
- https://github.com/uwdata/boba

## Multiverse projects

- @Dafflon2022-zl implemented a multiverse analysis for neuroimaging data
- @Hoogeveen2023-dw implemented a Bayesian multiverse analysis within the Many Labs 4 project (a large scale replication project in Psychology)
- @Olsson-Collentine2023-sj re-analyzed some replication projects using a multiverse approach

## Multiverse is catchy!

```{r}
multi_cit <- data.frame(
        year = c(2024L,2023L,2022L,2021L,2020L,2019L,
                 2018L,2017L,2016L,2015L,2014L,2013L),
      papers = c(30L, 46L, 33L, 19L, 10L, 11L, 4L, 2L, 3L, 0L, 1L, 1L),
       total = c(652674L,1295092L,1317981L,1299609L,
                 1161910L,1044346L,955091L,907118L,874112L,840256L,804004L,
                 777242L)
)

multi_cit |> 
    mutate(year = as.integer(year)) |> 
    ggplot(aes(x = year, y = papers/total)) +
    geom_line() +
    geom_label(aes(label = papers)) +
    scale_x_continuous(breaks = seq(2012, 2024, 1)) +
    theme(axis.title.x = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks.y = element_blank()) +
    ylab("# papers on Multiverse")
```

## References

::: {#refs}
:::

# Extra - PIMA implementation for meta-analysis {.section}

## General Workflow
 
![](img/workflow.png){fig-align="center"}

## Meta-analysis with permutations [@Follmann1999-ha]

With $k$ observed studies where $y_i$ and $\sigma^2_{\epsilon_i}$ being the observed effect sizes and sampling variances:

1. Generate a random vector $\mathbf{s}$ of $\pm 1$ of length $k$
2. Multiply the $\mathbf{y}$ vector with the $s$ vector
3. Fit the meta-analysis model and calculate $z^{*}_j$ ($j$ for permuted)
4. Repeat 1-3 for a large number of times $B$. With small $k$ we can do all the permutations $B = 2^k \times k$

The first permutation ($j = 1$) is the observed data. The p value can be computed as:

$$
p = \frac{\text{\#}(|z^{*}_j| > |z^{*}_1|)}{B}
$$

## Fast meta-analysis using permutations

- Meta-analysis using permutations requires recomputing $\tau^2$ and $\mu_\theta$ after each permutation.

- We proposed to estimate $\tau^2$ under $H_0$ and use the value for the permutations (without re-estimating it)

- This is extremely fast especially for large datasets and several multiverse scenarios

## Estimating $\tau^2$ under $H_0$

The crucial step is the point (1). This requires maximizing the log-likelihood fixing $\mu_\theta = 0$:

$$
L(\mu_{\theta}, \tau^2|\mathbf{y}) = -\frac{1}{2}\sum_{i = 1}^k \ln(\tau^2 + \sigma_{\epsilon_i}^2) - \frac{1}{2} \sum_{i = 1}^k \frac{(y_i - \mu_{\theta})^2}{\tau^2 +  \sigma_{\epsilon_i}^2}
$$

This can be done in R using some optimizer function (e.g., `optim`) or using directly the `metafor` package that allows fixing some parameters that are usually estimated.
