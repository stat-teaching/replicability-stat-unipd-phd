{
  "hash": "fcef6a1cdf7131efba78472a0cf0e97b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Meta-Analysis and Multi-Lab Replication studies\nsubtitle: Replicability Crisis in Science?\nparams:\n  solutions: true\n---\n\n\n\n\n## Setup\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n### Packages\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse) # for data manipulation\nlibrary(metafor) # for meta-analysis\ndevtools::load_all() # load all utils functions\n```\n:::\n\n\n\n\n\n\n\n\n\n\n# Meta-analysis {.section}\n\n## Meta-analysis\n\n- The meta-analysis is a statistical procedure to combine evidence from a group of studies.\n\n- It is usually combined with a systematic review of the literature\n\n- Is somehow the gold-standard approach when we want to summarise and make inference on a specific research area\n\n## Effect size\n\nTo compare results from different studies, we should use a common metrics. Frequently meta-analysts use *standardized* effect sizes. For example the Pearson correlation or the Cohen's $d$.\n\n$$\nr = \\frac{\\sum{(x_i - \\bar{x})(y_i - \\bar{y})}}{\\sqrt{\\sum{(x_i - \\bar{x})^2}\\sum{(y_i - \\bar{y})^2}}}\n$$\n\n$$\nd = \\frac{\\bar{x_1} - \\bar{x_2}}{s_p}\n$$\n\n$$\ns_p = \\sqrt{\\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}}\n$$\n\n## Effect size\n\nThe advantage of standandardized effect size is that regardless the original variable, the intepretation and the scale is the same. For example the pearson correlation ranges between -1 and 1 and the Cohen's $d$ between $- \\infty$ and $\\infty$ and is intepreted as how many standard deviations the two groups differs.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nS <- matrix(c(1, 0.7, 0.7, 1), nrow = 2)\nX <- MASS::mvrnorm(100, c(0, 2), S, empirical = TRUE)\n\npar(mfrow = c(1,2))\nplot(X, xlab = \"x\", ylab = \"y\", cex = 1.3, pch = 19,\n     cex.lab = 1.2, cex.axis = 1.2,\n     main = latex2exp::TeX(sprintf(\"$r = %.2f$\", cor(X[, 1], X[, 2]))))\nabline(lm(X[, 2] ~ X[, 1]), col = \"firebrick\", lwd = 2)\n\n\nplot(density(X[, 1]), xlim = c(-5, 7), ylim = c(0, 0.5), col = \"dodgerblue\", lwd = 2,\n     main = latex2exp::TeX(sprintf(\"$d = %.2f$\", lsr::cohensD(X[, 1], X[, 2]))),\n     xlab = \"\")\nlines(density(X[, 2]), col = \"firebrick\", lwd = 2)\n```\n\n::: {.cell-output-display}\n![](03-meta-analysis_files/figure-revealjs/unnamed-chunk-6-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n## Effect size sampling variability {#sec-effsize-se}\n\n::: {.panel-tabset}\n\nCrucially, we can calculate also the **sampling variability** of each effect size. The **sampling variability** is the precision of estimated value.\n\n\n### Formula\n\nFor example, there are multiple methods to estimate the Cohen's $d$ sampling variability. For example:\n\n$$\nV_d = \\frac{n_1 + n_2}{n_1 n_2} + \\frac{d^2}{2(n_1 + n_2)}\n$$\n\nEach effect size has a specific formula for the sampling variability. The sample size is usually the most important information. Studies with high sample size have low sampling variability.\n\n### Plot\n\nAs the sample size grows and tends to infinity, the sampling variability approach zero.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](03-meta-analysis_files/figure-revealjs/unnamed-chunk-7-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n:::\n\n## Notation {.smaller}\n\nMeta-analysis notation is a little bit inconsistent in textbooks and papers. We define here some rules to simplify the work.\n\n- $k$ is the number of studies\n- $n_j$ is the sample size of the group $j$ within a study\n- $y_i$ are the observed effect size included in the meta-analysis\n- $\\sigma_i^2$ are the observed sampling variance of studies and $\\epsilon_i$ are the sampling errors\n- $\\theta$ is the equal-effects parameter (see @eq-ee1)\n- $\\delta_i$ is the random-effect (see @eq-re-mod2)\n- $\\mu_\\theta$ is the average effect of a random-effects model (see @eq-re-mod1)\n- $w_i$ are the meta-analysis weights (e.g., see @eq-wi)\n- $\\tau^2$ is the heterogeneity (see @eq-re-mod2)\n- $\\Delta$ is the (generic) population effect size\n- $s_j^2$ is the variance of the group $j$ within a study\n \n## Extra - Simulating Meta-Analysis {.extra}\n\nFor the examples and plots I'm going to use simulated data^[If you are interested in meta-analysis simulation we wrote a preprint [https://psyarxiv.com/br6vy/](https://psyarxiv.com/br6vy/)]. We simulate *unstandardized* effect sizes (UMD) because the computations are easier and the estimator is unbiased [e.g., @Viechtbauer2005-zt]\n\nMore specifically we simulate hypothetical studies where two independent groups are compared:\n\n$$\n\\Delta = \\overline{X_1} - \\overline{X_2}\n$$\n\n$$\nSE_{\\Delta} = \\sqrt{\\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2}}\n$$\n\nWith $X_{1_i} \\sim \\mathcal{N}(0, 1)$ and $X_{2_i} \\sim \\mathcal{N}(\\Delta, 1)$\n\nThe main advantage is that, compared to standardized effect size, the sampling variability do not depends on the effect size itself, simplifying the computations.\n\n## Meta-analysis as a (weighted) average\n\nLet's imagine to have $k = 10$ studies. The segments are the 95% confidence intervals.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nk <- 10\nni <- round(runif(k, 10, 100))\ndat <- sim_studies(k, 0.5, 0, ni, ni)\nqf <- quick_forest(dat)\nqf\n```\n\n::: {.cell-output-display}\n![](03-meta-analysis_files/figure-revealjs/unnamed-chunk-8-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n\n\nWhat could you say about the phenomenon?\n\n## Meta-analysis as a (weighted) average\n\n::: {.panel-tabset}\n\n### Plot\n\nWe could say that the average effect is roughly ~0.5 and there is some variability around the average.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\navg <- mean(dat$yi)\nqf + geom_vline(xintercept = avg, color = \"firebrick\") +\ngeom_segment(aes(x = yi, y = id-0.3, xend = avg, yend = id-0.3),\ncolor = \"firebrick\")\n```\n\n::: {.cell-output-display}\n![](03-meta-analysis_files/figure-revealjs/unnamed-chunk-9-1.svg){fig-align='center' width=60%}\n:::\n:::\n\n\n\n\n### Formula\n\n$$\n\\overline y = \\frac{\\sum_{i = 1}^k y_i}{k}\n$$\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nybar <- mean(dat$yi)\nybar\n#> [1] 0.5600977\n```\n:::\n\n\n\n\n:::\n\n## Meta-analysis as a (weighted) average\n\n::: {.panel-tabset}\n\n### Plot\n\nThe simple average is a good statistics. But some studies are clearly more precise (narrower confidence intervals) than others i.e. the sampling variance is lower.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](03-meta-analysis_files/figure-revealjs/unnamed-chunk-11-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n### Equation\n\nWe can compute a weighted average where each study is weighted ($w_i$) by the inverse of the variance. This is called *inverse-variance weighting*. Clearly, a weighted average were all weights are the same reduced to a simple unweighted average.\n\n$$\n\\overline y = \\frac{\\sum_{i = 1}^k y_iw_i}{\\sum_{i = 1}^k w_i}\n$$\n$$\nw_i = \\frac{1}{v_i}\n$${#eq-wi}\n\n### R Code\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nwi <- 1/dat$vi\nywbar <- weighted.mean(dat$yi, wi) \nywbar\n#> [1] 0.4558081\n```\n:::\n\n\n\n\nThe value is (not so) different compared to the unweighted version (0.5600977). They are not exactly the same but given that weights are pretty homogeneous the two estimate are similar.\n\n:::\n\n## Meta-analysis as a (weighted) average\n\nWhat we did is a very simple model but actually is a meta-analysis model. This is commonly known as **equal-effects model** (or fixed-effect) **model**.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nquick_forest(dat, weigth = TRUE) +\n  geom_vline(xintercept = ywbar, color = \"firebrick\") +\n  ggtitle(\"Weighted Average\")\n```\n\n::: {.cell-output-display}\n![](03-meta-analysis_files/figure-revealjs/unnamed-chunk-13-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n## Equal-effects model (EE) {#sec-ee}\n\n::: {.panel-tabset}\n\n### Assumptions\n\nThe EE model is the simplest meta-analysis model. The assumptions are:\n\n- there is a unique, true effect size to estimate $\\theta$\n- each study is a more or less precise estimate of $\\theta$\n- there is no TRUE variability among studies. The observed variability is due to studies that are imprecise (i.e., sampling error)\n    - assuming that each study has a very large sample size, the observed variability is close to zero.\n\n### Equations\n\nFormally, we can define the EE model as:\n$$\ny_i = \\theta + \\epsilon_i\n$$ {#eq-ee1}\n\n$$\n\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2_i)\n$$ {#eq-ee1}\n\nWhere $\\sigma^2_i$ is the vector of sampling variabilities of $k$ studies. This is a standard linear model but with heterogeneous sampling variances.\n\n### Plots\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n:::\n\n## Equal-effects model (EE) {#sec-ee-high-low}\n\nA crucial part of the EE model is that, assuming studies with very large sample sizes, $\\epsilon_i$ will tend to 0 and each study is an almost perfect estimation of $\\theta$. Let's simulate two models, with $k = 10$ studies and $n = 30$ and $n = 500$. The effect size is the same $0.5$.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\ndat_low  <- sim_studies(10, 0.5, 0, 30, 30)\ndat_high <- sim_studies(10, 0.5, 0, 500, 500)\n\nqf_low <- quick_forest(dat_low) + \n  geom_vline(xintercept = 0.5, color = \"firebrick\") +\n  xlim(c(-2, 2)) +\n  ggtitle(latex2exp::TeX(\"$n_{1,2} = 30$\"))\n\nqf_high <- quick_forest(dat_high) + \n  geom_vline(xintercept = 0.5, color = \"firebrick\") +\n  xlim(c(-2, 2)) +\n  ggtitle(latex2exp::TeX(\"$n_{1,2} = 500$\"))\n\nplt_high_low <- cowplot::plot_grid(\n  qf_low,\n  qf_high\n)\n\nplt_high_low\n```\n\n::: {.cell-output-display}\n![](03-meta-analysis_files/figure-revealjs/unnamed-chunk-15-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n## Equal-effects model (EE)\n\nClearly, as $n$ increase, each study is essentially a perfect estimation of $\\theta$ as depicted in the theoretical figure (see slide [-@sec-ee]).\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](03-meta-analysis_files/figure-revealjs/unnamed-chunk-16-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n# Are the EE assumptions realistic? {.question}\n\n## Are the EE assumptions realistic?\n\nThe EE model is appropriate if our studies are somehow exact replications of the exact same effect. We are assuming that there is no real variability.\n\n. . .\n\nHowever, meta-analysis rarely report the results of $k$ exact replicates. It is more common to include studies with the same **underlying** objective but a roughly similar method.\n\n- people with different ages or other participant-level differences\n- different methodology\n- ...\n\n## Are the EE assumptions realistic?\n\nIn this case, even with extremely large studies, our effect could be larger in some conditions or smaller or absent in other conditions.\n\nIn other terms we are assuming that there could be some variability (i.e., **heterogeneity**) among studies that is independent from the sample size (or more generally the precision)\n\n## Random-effects model (RE)\n\n::: {.panel-tabset}\n\n### Theory\n\nWe can extend the EE model including another source of variability, $\\tau^2$. $\\tau^2$ is the true heterogeneity among studies caused by methdological differences or intrisic variability in the phenomenon.\n\nFormally we can extend the equation [-@eq-ee1] as:\n$$\ny_i = \\mu_{\\theta} + \\delta_i + \\epsilon_i\n$$ {#eq-re-mod1}\n\n$$\n\\delta_i \\sim \\mathcal{N}(0, \\tau^2)\n$$ {#eq-re-mod2}\n\n$$\n\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2_i)\n$$\n\nWhere $\\mu_{\\theta}$ is the average effect size and $\\delta_i$ is the study-specific deviation from the average effect (regulated by $\\tau^2$).\n\n### Estimation\n\nGiven that we extended the EE model equation. Also the estimation of the average effect need to be extended. Basically the RE is still a weighted average but weights need to include also $\\tau^2$.\n\n$$\n\\overline y = \\frac{\\sum_{i = 1}^k y_iw^*_i}{\\sum_{i = 1}^k w^*_i}\n$$ {#eq-re1}\n\n$$\nw^*_i = \\frac{1}{v_i + \\tau^2}\n$$ {#eq-re2}\n\nThe consequence is that weights are different where extremely precise/imprecise studies will impact less the estimation of the average effect under the RE model compared to EE.\n\n### Plots\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](03-meta-analysis_files/figure-revealjs/unnamed-chunk-17-1.svg){fig-align='center' width=60%}\n:::\n:::\n\n\n\n\n:::\n\n## Random-effects model\n\nThe crucial difference with the EE model is that even with large $n$, only the $\\mu_{\\theta} + \\delta_i$ are estimated (almost) without error. As long $\\tau^2 \\neq 0$ there will be variability in the effect sizes.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](03-meta-analysis_files/figure-revealjs/unnamed-chunk-18-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n## Random-effects model\n\nAgain, we can easily demonstrate this with a simulation. We use the same simulation as slide [-@sec-ee-high-low] but including $\\tau^2 = 0.2$.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\ndat_low  <- sim_studies(10, 0.5, 0.2, 30, 30)\ndat_high <- sim_studies(10, 0.5, 0.2, 500, 500)\n\nqf_low <- quick_forest(dat_low) + \n  geom_vline(xintercept = 0.5, color = \"firebrick\") +\n  xlim(c(-3, 3)) +\n  ggtitle(latex2exp::TeX(\"$n_{1,2} = 30$\"))\n\nqf_high <- quick_forest(dat_high) + \n  geom_vline(xintercept = 0.5, color = \"firebrick\") +\n  xlim(c(-3, 3)) +\n  ggtitle(latex2exp::TeX(\"$n_{1,2} = 500$\"))\n\nqf_tau_high_low <- cowplot::plot_grid(\n  qf_low,\n  qf_high\n)\nqf_tau_high_low\n```\n\n::: {.cell-output-display}\n![](03-meta-analysis_files/figure-revealjs/unnamed-chunk-19-1.svg){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\nEven with $n \\to \\infty$ the observed variance is not reduced as long $\\tau^2 \\neq 0$.\n\n## Extra - Simulating Meta-Analysis {.extra}\n\n::: {.panel-tabset}\n\n### Equations 1\n\nFor the simulations, we can generate data from effect size and variance sampling distributions^[see https://www.jepusto.com/simulating-correlated-smds/ for a nice blog post about simulations]. The *unstandardized* effect size is a mean difference between independent groups. The sampling distribution is:\n\n$$\ny_i \\sim \\mathcal{N}(\\Delta, \\frac{1}{n_1} + \\frac{1}{n_2})\n$$\n\nWhere $\\Delta$ is the population level effect size and $n_{1,2}$ are the sample sizes of the two studies. The sampling variances are generated from a $\\chi^2$ distribution:\n\n$$\n\\sigma_i^2 \\sim \\frac{\\chi^2_{n_1 + n_2 - 2}}{n_1 + n_2 - 2} (\\frac{1}{n_1} + \\frac{1}{n_2})\n$$\n\n### Equations 1\n\nClearly we can include $\\tau^2$ to include between-study variability. For an equal-effects model $\\Delta = \\theta$ thus the equation is unchanged. For a random-effects model, $\\Delta = \\theta_i = \\mu_\\theta + \\delta_i$\n\n$$\ny_i \\sim \\mathcal{N}(\\Delta, \\tau^2 + \\frac{1}{n_1} + \\frac{1}{n_2})\n$$\n\n### Code {.smaller}\n\nThe simulation is implemented in the `sim_studies` function:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n```r\nsim_studies <- function(k, theta, tau2, n0, n1, summary = FALSE){\n  yi <- rnorm(k, theta, sqrt(tau2 + 1/n0 + 1/n1))\n  vi <- (rchisq(k, n0 + n1 - 2) / (n0 + n1 - 2)) * (1/n0 + 1/n1)\n  out <- data.frame(yi, vi, sei = sqrt(vi))\n  if(summary){\n    out <- summary_es(out)\n  }\n  return(out)\n}\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat <- sim_studies(k = 10, theta = 0.5, tau2 = 0.2, n0 = 30, n1 = 30)\ndat\n#>             yi         vi       sei\n#> 1   0.03722128 0.06456132 0.2540892\n#> 2   0.86109241 0.07728935 0.2780096\n#> 3  -0.20386019 0.08004369 0.2829199\n#> 4   0.97094251 0.07066803 0.2658346\n#> 5   0.57983381 0.06377108 0.2525294\n#> 6   1.53919586 0.07696243 0.2774210\n#> 7   0.19259021 0.07343220 0.2709838\n#> 8   0.38608057 0.05963238 0.2441974\n#> 9   1.19627080 0.07010046 0.2647649\n#> 10  0.85360445 0.07567874 0.2750977\n```\n:::\n\n\n\n\n:::\n\n## Heterogeneity\n\nWe discussed so far about estimating the average effect ($\\theta$ or $\\mu_\\theta$). But how to estimate the heterogeneity?\n\nThere are several estimators for $\\tau^2$\n\n- Hunter–Schmidt\n- Hedges\n- DerSimonian–Laird\n- Maximum-Likelihood\n- Restricted Maximum-Likelihood (REML)\n\nWe will mainly use the REML estimator. See @Viechtbauer2005-zt and @Veroniki2016-nw for more details.\n\n## The Q Statistics^[See @Harrer2021-go (Chapter 5) and @Hedges2019-ry for an overview about the Q statistics]\n\nThe Q statistics is used to make statistical inference on the heterogeneity. Can be considered as a weighted sum of squares:\n\n$$\nQ = \\sum^k_{i = 1}w_i(y_i - \\hat \\mu)^2\n$$\n\nWhere $\\hat \\mu$ is EE estimation (regardless if $\\tau^2 \\neq 0$) and $w_i$ are the inverse-variance weights. Note that in the case of $w_1 = w_2 ... = w_i$, Q is just a standard sum of squares (or deviance).\n\n## The Q Statistics\n\n- Given that we are summing up squared distances, they should be approximately $\\chi^2$ with $df = k - 1$. In case of no heterogeneity ($\\tau^2 = 0$) the observed variability is caused by sampling error only. The expectd value of the $\\chi^2$ is just the degrees of freedom ($df = k - 1$).\n\n- In case of $\\tau^2 \\neq 0$, the expected value is $k - 1 + \\lambda$ where $\\lambda$ is a non-centrality parameter.\n\n- In other terms, if the expected value of $Q$ exceed the expected value assuming no heterogeneity, we have evidence that $\\tau^2 \\neq 0$.\n\n## The Q Statistics\n\nLet's try a more practical approach. We simulate a lot of meta-analysis with and without heterogeneity and we check the Q statistics.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nget_Q <- function(yi, vi){\n  wi <- 1/vi\n  theta_ee <- weighted.mean(yi, wi)\n  sum(wi*(yi - theta_ee)^2)\n}\n\nk <- 30\nn <- 30\ntau2 <- 0.1\nnsim <- 1e4\n\nQs_tau2_0 <- rep(0, nsim)\nQs_tau2 <- rep(0, nsim)\nres2_tau2_0 <- vector(\"list\", nsim)\nres2_tau2 <- vector(\"list\", nsim)\n\nfor(i in 1:nsim){\n  dat_tau2_0 <- sim_studies(k = 30, theta = 0.5, tau2 = 0, n0 = n, n1 = n)\n  dat_tau2 <- sim_studies(k = 30, theta = 0.5, tau2 = tau2, n0 = n, n1 = n)\n  \n  theta_ee_tau2_0 <- weighted.mean(dat_tau2_0$yi, 1/dat_tau2_0$vi)\n  theta_ee <- weighted.mean(dat_tau2$yi, 1/dat_tau2$vi)\n  \n  res2_tau2_0[[i]] <- dat_tau2_0$yi - theta_ee_tau2_0\n  res2_tau2[[i]] <- dat_tau2$yi - theta_ee\n  \n  Qs_tau2_0[i] <- get_Q(dat_tau2_0$yi, dat_tau2_0$vi)\n  Qs_tau2[i] <- get_Q(dat_tau2$yi, dat_tau2$vi)\n}\n\ndf <- k - 1\n\npar(mfrow = c(2,2))\nhist(Qs_tau2_0, probability = TRUE, ylim = c(0, 0.08), xlim = c(0, 150),\n     xlab = \"Q\",\n     main = latex2exp::TeX(\"$\\\\tau^2 = 0$\"))\ncurve(dchisq(x, df), 0, 100, add = TRUE, col = \"firebrick\", lwd = 2)\n\nhist(unlist(res2_tau2_0), probability = TRUE, main = latex2exp::TeX(\"$\\\\tau^2 = 0$\"), ylim = c(0, 2),\n     xlab = latex2exp::TeX(\"$y_i - \\\\hat{\\\\mu}$\"))\ncurve(dnorm(x, 0, sqrt(1/n + 1/n)), add = TRUE, col = \"dodgerblue\", lwd = 2)\n\nhist(Qs_tau2, probability = TRUE, ylim = c(0, 0.08), xlim = c(0, 150),\n     xlab = \"Q\",\n     main = latex2exp::TeX(\"$\\\\tau^2 = 0.1$\"))\ncurve(dchisq(x, df), 0, 100, add = TRUE, col = \"firebrick\", lwd = 2)\n\nhist(unlist(res2_tau2), probability = TRUE, main = latex2exp::TeX(\"$\\\\tau^2 = 0.1$\"), ylim = c(0, 2),\n     xlab = latex2exp::TeX(\"$y_i - \\\\hat{\\\\mu}$\"))\ncurve(dnorm(x, 0, sqrt(1/n + 1/n)), add = TRUE, col = \"dodgerblue\", lwd = 2)\n```\n\n::: {.cell-output-display}\n![](03-meta-analysis_files/figure-revealjs/unnamed-chunk-22-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n## The Q Statistics\n\nLet's try a more practical approach. We simulate a lot of meta-analysis with and without heterogeneity and we check the Q statistics.\n\n. . .\n\n- clearly, in the presence of heterogeneity, the expected value of the Q statistics is higher (due to $\\lambda$) and also residuals are larger.\n\n. . .\n\n- we can calculate a p-value for deviation from the $\\tau^2 = 0$ case as evidence agaist the absence of heterogeneity\n\n## Estimating $\\tau^2$\n\nThe Q statistics is rarely used to directly represent the heterogeneity. The raw measure of heterogeneity is $\\tau^2$. The REML (restricted maximum likelihood) estimator is often used.\n\n$$\n\\hat \\tau^2 = \\frac{\\sum_{i = 1}^k w_i^2[(y_i - \\hat \\mu)^2 - \\sigma^2_i]}{\\sum_{i = 1}^k w_i^2} + \\frac{1}{\\sum_{i = 1}^k w_i}\n$$\n\nWhere $\\hat{\\mu}$ is the weighted average (i.e., maximum likelihood estimation, see @eq-re1 and @eq-re2).\n\n## $\\tau^2$ as heterogeneity measure\n\n- $\\tau^2$ is the direct measure of heterogeneity in meta-analysis\n- it is intepreted as a standard deviation (or variance) of the distribution of true effects\n- a phenomenon associated with an higher $\\tau^2$ is interpred as more variable \n\n## $\\tau^2$ as heterogeneity measure\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# we generate 1e5 values from a random-effect model with certain parameters and different tau2 values\n# and check the expected distrubution of effect sizes\n\nn <- 100\nk <- 1e5\ntau2 <- c(0, 0.1, 0.2, 0.5)\n\ndats <- lapply(tau2, function(x) sim_studies(k, 0.5, x, n, n))\nnames(dats) <- tau2\n\ndat <- bind_rows(dats, .id = \"tau2\") \ndat$tau2 <- factor(dat$tau2)\ndat$tau2 <- factor(dat$tau2, labels = latex2exp::TeX(sprintf(\"$\\\\tau^2 = %s$\", levels(dat$tau2))))\n\ndat |> \n  ggplot(aes(x = yi, y = after_stat(density))) +\n  geom_histogram(bins = 50) +\n  geom_vline(xintercept = 0.5, lwd = 0.5, color = \"firebrick\") +\n  facet_wrap(~tau2, labeller = label_parsed) +\n  ggtitle(latex2exp::TeX(\"Expected $y_i$ distribution, $d = 0.5$, $n_{1,2} = 100$\")) +\n  xlab(latex2exp::TeX(\"$y_i$\"))\n```\n\n::: {.cell-output-display}\n![](03-meta-analysis_files/figure-revealjs/unnamed-chunk-23-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n## $I^2$ [@Higgins2002-fh]\n\nWe have two sources of variability in a random-effects meta-analysis, the sampling variabilities $\\sigma_i^2$ and $\\tau^2$. We can use the $I^2$ to express the interplay between the two.\n$$\nI^2 = 100\\% \\times \\frac{\\hat{\\tau}^2}{\\hat{\\tau}^2 + \\tilde{v}}\n$${#eq-i2}\n\n$$\n\\tilde{v} = \\frac{(k-1) \\sum w_i}{(\\sum w_i)^2 - \\sum w_i^2},\n$$\n\nWhere $\\tilde{v}$ is the typical sampling variability. $I^2$ is intepreted as the proportion of total variability due to real heterogeneity (i.e., $\\tau^2$)\n\n## $I^2$ [@Higgins2002-fh]^[see [https://www.meta-analysis-workshops.com/download/common-mistakes1.pdf](https://www.meta-analysis-workshops.com/download/common-mistakes1.pdf)]\n\nNote that we can have the same $I^2$ in two completely different meta-analysis. An high $I^2$ does not represent high heterogeneity. Let's assume to have two meta-analysis with $k$ studies and small ($n = 30$) vs large ($n = 500$) sample sizes. \n\nLet's solve @eq-i2 for $\\tau^2$ (using `filor::tau2_from_I2()`) and we found that the same $I^2$ can be obtained with two completely different $\\tau^2$ values:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn_1 <- 30\nvi_1 <- 1/n_1 + 1/n_1\ntau2_1 <- filor::tau2_from_I2(0.8, vi_1)\ntau2_1\n#> [1] 0.2666667\n\nn_2 <- 500\nvi_2 <- 1/n_2 + 1/n_2\ntau2_2 <- filor::tau2_from_I2(0.8, vi_2)\ntau2_2\n#> [1] 0.016\n```\n:::\n\n\n\n\n## $I^2$ [@Higgins2002-fh]\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn_1 <- 30\nvi_1 <- 1/n_1 + 1/n_1\ntau2_1 <- filor::tau2_from_I2(0.8, vi_1)\ntau2_1\n#> [1] 0.2666667\n\nn_2 <- 500\nvi_2 <- 1/n_2 + 1/n_2\ntau2_2 <- filor::tau2_from_I2(0.8, vi_2)\ntau2_2\n#> [1] 0.016\n```\n:::\n\n\n\n\n. . .\n\nIn other terms, the $I^2$ can be considered a good index of heterogeneity only when the total variance ($\\tilde{v} + \\tau^2$) is the same.\n\n# Meta-analysis in R {.section}\n\n## Meta-analysis in R\n\nIn R there are several packages to conduct a meta-analysis. For me, the best package is `metafor` [@Viechtbauer2010-xz]. The package support all steps in meta-analysis providing also an amazing documentation:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n<iframe src=\"https://www.metafor-project.org/doku.php/metafor\" width=\"960\" height=\"400px\" data-external=\"1\"></iframe>\n:::\n\n\n\n\n## Equal-effects model in R\n\nDisclaimer: we are omitting the important part of collecting the information from published studies and calculating the (un)standardized effect sizes. Clerly this part is highly dependent on the actual dataset.\n\nThere are few useful resources:\n\n- Chapters 1-9 from Borestein et al. [-@Borenstein2009-mo]\n- The `metafor::escalc()` [function](https://wviechtb.github.io/metafor/reference/escalc.html) that calculate all effects size and report an useful documentation \n\n## Equal-effects model in R\n\nWe can simulate an EE dataset (i.e., $\\tau^2 = 0$) and then fit the model with `metafor`:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntheta <- 0.3\nk <- 30 # number of studies\nn <- round(runif(k, 10, 60)) # random sample sizes with a plausible range\ndat <- sim_studies(k = k, theta = theta, tau2 = 0, n0 = n, n1 = n)\ndat$n <- n # n1 and n2 are the same, put only one\nhead(dat)\n#>           yi         vi       sei  n\n#> 1  0.5006860 0.05474557 0.2339777 33\n#> 2  0.8160800 0.12421999 0.3524486 17\n#> 3  0.7038806 0.11651275 0.3413396 17\n#> 4  0.3023013 0.14427791 0.3798393 19\n#> 5 -0.1286906 0.14727102 0.3837591 18\n#> 6  0.5989998 0.03710777 0.1926338 49\n```\n:::\n\n\n\n\n## Equal-effects model in R\n\nWe can start by some summary statistics:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# unweighted summary statistics for the effect\nsummary(dat$yi)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#> -0.1602  0.1105  0.2959  0.3077  0.5019  0.8161\n\n# unweighted summary statistics for the variances\nsummary(dat$vi)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#> 0.03592 0.04446 0.06456 0.07630 0.09004 0.21512\n\n# sample sizes\nsummary(dat$n) # n0 and n1 are the same\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   14.00   19.25   32.00   32.70   43.75   60.00\n```\n:::\n\n\n\n\n## Equal-effects model in R\n\nThen we can use the `metafor::rma.uni()` function (or more simply `metafor::rma()`) providing the effect size, variances and `method = \"EE\"` to specify that we are fitting an equal-effects model.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit_ee <- rma(yi, vi, method = \"EE\", data = dat)\nsummary(fit_ee)\n#> \n#> Equal-Effects Model (k = 30)\n#> \n#>   logLik  deviance       AIC       BIC      AICc   \n#>  -0.7274   27.3050    3.4547    4.8559    3.5976   \n#> \n#> I^2 (total heterogeneity / total variability):   0.00%\n#> H^2 (total variability / sampling variability):  0.94\n#> \n#> Test for Heterogeneity:\n#> Q(df = 29) = 27.3050, p-val = 0.5553\n#> \n#> Model Results:\n#> \n#> estimate      se    zval    pval   ci.lb   ci.ub      \n#>   0.3046  0.0449  6.7781  <.0001  0.2166  0.3927  *** \n#> \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n\n\n\n## Equal-effects model in R\n\nWe can easily show that the results is the same as performing a simple weighted average using study-specific variances as weight:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nwi <- 1/dat$vi\n\nweighted.mean(dat$yi, wi)\n#> [1] 0.3046422\n\nsummary(fit_ee)\n#> \n#> Equal-Effects Model (k = 30)\n#> \n#>   logLik  deviance       AIC       BIC      AICc   \n#>  -0.7274   27.3050    3.4547    4.8559    3.5976   \n#> \n#> I^2 (total heterogeneity / total variability):   0.00%\n#> H^2 (total variability / sampling variability):  0.94\n#> \n#> Test for Heterogeneity:\n#> Q(df = 29) = 27.3050, p-val = 0.5553\n#> \n#> Model Results:\n#> \n#> estimate      se    zval    pval   ci.lb   ci.ub      \n#>   0.3046  0.0449  6.7781  <.0001  0.2166  0.3927  *** \n#> \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n\n\n\n\n## Random-effects model in R\n\nThe syntax for a random-effects model is the same, we just need to use `method = \"REML\"` (or another $\\tau^2$ estimation method)\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit_re <- rma(yi, vi, method = \"REML\", data = dat)\nsummary(fit_re)\n#> \n#> Random-Effects Model (k = 30; tau^2 estimator: REML)\n#> \n#>   logLik  deviance       AIC       BIC      AICc   \n#>  -1.2101    2.4203    6.4203    9.1549    6.8818   \n#> \n#> tau^2 (estimated amount of total heterogeneity): 0 (SE = 0.0148)\n#> tau (square root of estimated tau^2 value):      0\n#> I^2 (total heterogeneity / total variability):   0.00%\n#> H^2 (total variability / sampling variability):  1.00\n#> \n#> Test for Heterogeneity:\n#> Q(df = 29) = 27.3050, p-val = 0.5553\n#> \n#> Model Results:\n#> \n#> estimate      se    zval    pval   ci.lb   ci.ub      \n#>   0.3046  0.0449  6.7781  <.0001  0.2166  0.3927  *** \n#> \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n\n\n\n## Random-effects model in R\n\nWe can easily compare the models using the `compare_rma()` function:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfilor::compare_rma(fit_ee, fit_re)  |>\n  round(5)\n#>        fit_ee  fit_re\n#> b     0.30464 0.30464\n#> se    0.04495 0.04495\n#> zval  6.77808 6.77808\n#> pval  0.00000 0.00000\n#> ci.lb 0.21655 0.21655\n#> ci.ub 0.39273 0.39273\n#> I2    0.00000 0.00000\n#> tau2  0.00000 0.00000\n```\n:::\n\n\n\n\nWhat do you think? are there differences between the two models? What could be the reasons?\n\n## Random-effects model in R\n\nClearly, given that we simulated $\\tau^2 = 0$ the `RE` model results is very close (if not equal) to the `EE` model. We can simulate a `RE` model:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntau2 <- 0.2\ndat <- sim_studies(k = k, theta = theta, tau2 = tau2, n0 = n, n1 = n)\n\nfit_re <- rma(yi, vi, method = \"REML\", data = dat)\nsummary(fit_re)\n#> \n#> Random-Effects Model (k = 30; tau^2 estimator: REML)\n#> \n#>   logLik  deviance       AIC       BIC      AICc   \n#> -26.0957   52.1914   56.1914   58.9260   56.6529   \n#> \n#> tau^2 (estimated amount of total heterogeneity): 0.2855 (SE = 0.0937)\n#> tau (square root of estimated tau^2 value):      0.5343\n#> I^2 (total heterogeneity / total variability):   82.53%\n#> H^2 (total variability / sampling variability):  5.72\n#> \n#> Test for Heterogeneity:\n#> Q(df = 29) = 167.7055, p-val < .0001\n#> \n#> Model Results:\n#> \n#> estimate      se    zval    pval   ci.lb   ci.ub     \n#>   0.3436  0.1093  3.1430  0.0017  0.1293  0.5578  ** \n#> \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n\n\n\n## Forest Plot\n\nThe most common plot to represent meta-analysis results is the forest plot:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nforest(fit_re)\n```\n\n::: {.cell-output-display}\n![](03-meta-analysis_files/figure-revealjs/unnamed-chunk-34-1.svg){fig-align='center' width=768}\n:::\n:::\n\n\n\n\n## Forest Plot\n\nThe most common plot to represent meta-analysis results is the forest plot:\n\n- The `x` axis represent the **effect size**\n- The `y` axis represent the **studies**\n- The `size of the square` is the **weight** of that study ($w_i = 1/\\sigma_i^2$)\n- The `interval` is the **95% confidence interval**\n- The `diamond` is the **estimated effect and the 95% confidence interval**\n\n## Forest Plot\n\nIt is also common to plot studies ordering by the size of the effect, showing asymmetries or other patterns:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nforest(fit_re, order = \"yi\")\n```\n\n::: {.cell-output-display}\n![](03-meta-analysis_files/figure-revealjs/unnamed-chunk-35-1.svg){fig-align='center' width=768}\n:::\n:::\n\n\n\n\n# Multilab Studies  {.section}\n\n## Multilab Studies\n\nSo far we reasoned about collecting published studies and conducting a meta-analysis. Despite pooling evidence from multiple studies there are several limitations:\n\n. . .\n\n- publication bias\n\n. . .\n\n- small number of studies ($k$) thus low power and low estimation precision\n\n. . .\n\n- the estimated $\\tau^2$ could be high because the methdological heterogeneity is high --> each research group use a different methodology\n\n## Multilab Studies\n\nWith multilab studies we define a **new data collection** where different research group conduct an experiment with a **similar (or the exact same) methodology**. In this way we can:\n\n. . .\n\n- eliminate the **publication bias**\n\n. . .\n\n- **calibrate** the number of studies ($k$) and observations ($n$) according to our criteria (power, precision, etc.)\n\n## Multilab Studies and meta-analysis\n\nFrom a statistical point of view, the only difference is the source of the data (planned and collected vs collected from the literature). In fact, a multilab study can be analyzed also using standard meta-analysis tools.\n\n. . .\n\nWhen intepreting the results we could highlight some differences:\n\n. . .\n\n- $\\tau^2$ in standard meta-analysis is considered the true variability of the phenomenon. In multilab studies (assuming an exact replication approach) should be low or close to zero.\n\n. . .\n\n- In standard meta-analysis we could try to explain $\\tau^2$ using meta-regression (e.g., putting the average participant age as predictor) while in multilab studies we could plan to use the same age thus removing the age effect.\n\n## Extra - Planning a Multilab Study {.extra}\n\nLet's imagine to plan a multilab study with the same setup as the previous meta-analysis. We are not collecting data from the literature but we want to know the number of studies $k$ that we need to achieve e.g. a good statistical Power.\n\n. . .\n\nWe define power as the probability of correctly rejecting the null hypothesis of no effect when the effect is actually present\n\n## Extra - Planning a Multilab Study {.extra}\n\nWe can do a Power analysis using Monte Carlo simulations:\n\n1. simulate a meta-analysis given a set of parameters\n2. fit the meta-analysis model\n3. extract the p-value\n4. repeat the steps 1-3 a lot of times e.g. 10000\n5. count the number of significant (e.g., $\\leq \\alpha$) p-values\n\n## Extra - Planning a Multilab Study {.extra}\n\nWe can implement a simple simulation as:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n```r\nlibrary(metafor)\nlibrary(ggplot2)\n\n# set.seed(2023)\n\nk <- seq(10, 100, 10)\nn <- seq(10, 100, 20)\nes <- 0.3\ntau2 <- 0.1\nalpha <- 0.05\nnsim <- 1e3\n\nsim <- tidyr::expand_grid(k, n, es, tau2)\nsim$power <- 0\n\n# handle errors, return NA\nsrma <- purrr::possibly(rma, otherwise = NA)\n\nfor(i in 1:nrow(sim)){ # iterate for each condition\n  pval <- rep(0, nsim)\n  for(j in 1:nsim){ # iterate for each simulation\n    dat <- sim_studies(k = sim$k[i], \n                       theta = sim$es[i], \n                       tau2 = sim$tau2[i],\n                       n0 = sim$n[i],\n                       n1 = sim$n[i])\n    fit <- srma(yi, vi, data = dat, method = \"REML\")\n    pval[j] <- fit$pval\n  }\n  sim$power[i] <- mean(pval <= alpha) # calculate power\n  filor::pb(nrow(sim), i)\n}\n\nsaveRDS(sim, here(\"04-meta-analysis/objects/power-example.rds\"))\n```\n:::\n\n\n\n\n## Extra - Planning a Multilab Study {.extra}\n\nThen we can plot the results:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nsim <- readRDS(here(\"slides/03-meta-analysis/objects/power-example.rds\"))\n\ntitle <- \"$\\\\mu_\\\\theta = 0.3$, $\\\\tau^2 = 0.1$, $\\\\alpha = 0.05\"\n\nsim |> \n  ggplot(aes(x = k, y = power, group = n, color = factor(n))) +\n  geom_line(lwd = 1) +\n  xlab(\"Number of Studies (k)\") +\n  ylab(\"Power\") +\n  labs(color = \"Sample Size\") +\n  theme(legend.position = \"bottom\") +\n  ggtitle(latex2exp::TeX(title))\n```\n\n::: {.cell-output-display}\n![](03-meta-analysis_files/figure-revealjs/unnamed-chunk-37-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n## Extra - Planning a Multilab Study {.extra}\n\nThis is a flexible way to simulate and plan a multilab study:\n\n- We could prefer increasing $k$ (i.e., the research units) and limiting $n$ thus reducing the effort for each lab. The downside are difficulties in managing multiple labs, increased dropout rate, difficulty in reaching the planned $k$\n- We could prefer increasing $n$ and limiting $k$. Each lab need to put more resources but the overall project could be easier.\n- We could try to estimate a cost function according to several parameters and find the best trade-off as implemented in @Hedges2021-of\n\n# Publication Bias (PB) {.section}\n\n# What do you think about PB? What do you know? Causes? Remedies?  {.question .smaller}\n\n## Publication Bias (PB)\n\nThe PB is one of the **most problematic aspects** of meta-analysis. Essentially **the probability of publishing a paper** (~and thus including into the meta-analysis) [is not the same regardless the result]{.imp}. Clearly we could include also the gray literature to mitigate the problem.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](03-meta-analysis_files/figure-revealjs/unnamed-chunk-38-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n## Publication Bias Disclaimer!\n\n**We cannot (completely) solve the PB using statistical tools**. The PB is a problem related to the publishing process and publishing incentives (significant results are more catchy and easy to explain).\n\n. . .\n\n- **pre-registrations**, **multi-lab studies**, etc. can (almost) completely solve the problem filling the literature with unbiased studies (at least from the publishing point of view)\n\n. . .\n\n- there are **statistical tools to detect, estimate and correct** for the publication bias. As every statistical method, they are influenced by statistical assumptions, number of studies and sample size, heterogeneity, etc.\n\n## Publication Bias (PB) - The Big Picture^[Thanks to the Wolfgang Viechtbauer's course [https://www.wvbauer.com/doku.php/course_ma](https://www.wvbauer.com/doku.php/course_ma)]\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](img/big-picture.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## Publication Bias (PB) - Funnel Plot\n\nThe first tool is called **funnel plot**. This is a **visual tool** to check the presence of asymmetry that could be caused by publication bias. If meta-analysis assumptions are respected, and there is no publication bias:\n\n- effects should be normally distributed around the average effect\n- more precise studies should be closer to the average effect\n- less precise studies could be equally distributed around the average effect\n\nLet's simulate a lot of studies to show:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(2023)\nk <- 1e3\nn <- round(runif(k, 10, 200))\ndat <- sim_studies(k = k, theta = 0.5, tau2 = 0, n0 = n, n1 = n)\nfit <- rma(yi, vi, method = \"EE\", data = dat)\n```\n:::\n\n\n\n\n## Publication Bias (PB) - Funnel Plot\n\nLet's plot the distribution of the data:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhist(dat$yi)\n```\n\n::: {.cell-output-display}\n![](03-meta-analysis_files/figure-revealjs/unnamed-chunk-41-1.svg){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\nThe distribution is clearly normal and centered on the true simulated value. Now let's add the precision (in this case standard error thus $\\sqrt{\\sigma_i^2}$) on the y-axis.\n\n## Publication Bias (PB) - Funnel Plot\n\nNote that the `y` axis is reversed so high-precise studies ($\\sqrt{\\sigma_i^2}$ close to 0) are on top.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nplot(dat$yi, dat$sei, ylim = rev(range(dat$sei)),\n     xlab = latex2exp::TeX(\"$y_i$\"),\n     ylab = latex2exp::TeX(\"$\\\\sqrt{\\\\sigma_i^2}$\"),\n     pch = 19,\n     col = scales::alpha(\"firebrick\", 0.5))\nabline(v = fit$b[[1]], col = \"black\", lwd = 1.2)\n```\n\n::: {.cell-output-display}\n![](03-meta-analysis_files/figure-revealjs/unnamed-chunk-42-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n## Publication Bias (PB) - Funnel Plot\n\nThe plot assume the typical funnel shape and there are not missing spots on the at the bottom. The presence of missing spots is a potential index of publication bias.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](03-meta-analysis_files/figure-revealjs/unnamed-chunk-43-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n## Publication Bias (PB) - Funnel Plot\n\nThe plot assume the typical funnel shape and there are not missing spots on the at the bottom. The presence of missing spots is a potential index of publication bias.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](03-meta-analysis_files/figure-revealjs/unnamed-chunk-44-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n## Robustness to PB - Fail-safe N\n\nThe Fail-safe N [@Rosenthal1979-yx] idea is very simple. Given a meta-analysis with a significant result (i.e., $p \\leq \\alpha$). How many null studies (i.e., $\\hat \\theta = 0$) do I need to obtain $p > \\alpha$?\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmetafor::fsn(yi, vi, data = dat)\n#> \n#> Fail-safe N Calculation Using the Rosenthal Approach\n#> \n#> Observed Significance Level: <.0001\n#> Target Significance Level:   0.05\n#> \n#> Fail-safe N: 4396757\n```\n:::\n\n\n\n\n## Robustness to PB - Fail-safe N\n\nThere are several criticism to the Fail-safe N procedure:\n\n. . .\n\n- is not actually *detecting* the PB but suggesting the required PB size to remove the effect. A very large N suggest that even with PB, it is unlikely that the results could be completely changed by actually reporting null studies\n\n. . .\n\n- @Orwin1983-vu proposed a new method calculating the number of studies required to reduce the effect size to a given target\n\n. . .\n\n- @Rosenberg2005-ie proposed a method similar to Rosenthal [-@Rosenthal1979-yx] but combining the (weighted) effect sizes and not the p-values.\n\n\n## Detecting PB - Egger Regression\n\nA basic method to test the funnel plot asymmetry is using an the **Egger regression test**. Basically we calculate the relationship between $y_i$ and $\\sqrt{\\sigma^2_i}$. In the absence of asimmetry, the line slope should be not different from 0.\n\nWe can use the `metafor::regtest()` function:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\negger <- regtest(fit)\negger\n#> \n#> Regression Test for Funnel Plot Asymmetry\n#> \n#> Model:     fixed-effects meta-regression model\n#> Predictor: standard error\n#> \n#> Test for Funnel Plot Asymmetry: z = 1.0847, p = 0.2781\n#> Limit Estimate (as sei -> 0):   b = 0.4821 (CI: 0.4521, 0.5122)\n```\n:::\n\n\n\n\n## Publication Bias (PB) - Egger Regression\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](03-meta-analysis_files/figure-revealjs/unnamed-chunk-47-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n\n\nThis is a standard (meta) regression thus the number of studies, the precision of each study and heterogeneity influence the reliability (power, type-1 error rate, etc.) of the procedure.\n\n## Correcting PB - Trim and Fill\n\nThe Trim and Fill method [@Duval2000-ym] is used to impute the hypothetical missing studies according to the funnel plot and recomputing the meta-analysis effect. Shi and Lin [@Shi2019-pj] provide an updated overview of the method with some criticisms.\n\n. . .\n\nLet's simulate again a **publication** bias with $k = 100$ studies:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(2023)\nk <- 100 # we increased k to better show the effect\ntheta <- 0.5\ntau2 <- 0.1\n\ndat <- sim_pub_bias(selmodel = list(method = \"2step\", param = \"pval\", th = 0.05, side = \"<=\"), k = k, theta = theta, tau2 = tau2, nmin = 10, nmax = 200)\nfit <- rma(yi, vi, data = dat, method = \"REML\")\n```\n:::\n\n\n\n\n\n## Correcting PB - Trim and Fill\n\nNow we can use the `metafor::trimfill()` function:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntaf <- metafor::trimfill(fit)\ntaf\n#> \n#> Estimated number of missing studies on the left side: 29 (SE = 6.4513)\n#> \n#> Random-Effects Model (k = 129; tau^2 estimator: REML)\n#> \n#> tau^2 (estimated amount of total heterogeneity): 0.1382 (SE = 0.0203)\n#> tau (square root of estimated tau^2 value):      0.3718\n#> I^2 (total heterogeneity / total variability):   88.56%\n#> H^2 (total variability / sampling variability):  8.74\n#> \n#> Test for Heterogeneity:\n#> Q(df = 128) = 1073.8812, p-val < .0001\n#> \n#> Model Results:\n#> \n#> estimate      se     zval    pval   ci.lb   ci.ub      \n#>   0.4851  0.0357  13.5951  <.0001  0.4152  0.5551  *** \n#> \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n\n\n\n## Correcting PB - Trim and Fill\n\nThe trim-and-fill estimates that 29 are missing. The new effect size after including the studies is reduced and closer to the simulated value (but in this case still significant).\n\n## Correcting PB - Trim and Fill\n\nWe can also visualize the funnel plot highligting the points that are included by the method.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfunnel(taf)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nfunnel(taf)\negg <- regtest(fit)\negg_npb <- regtest(taf)\nse <- seq(0,1.8,length=100)\nlines(coef(egg$fit)[1] + coef(egg$fit)[2]*se, se, lwd=3, col = \"black\")\nlines(coef(egg_npb$fit)[1] + coef(egg_npb$fit)[2]*se, se, lwd=3, col = \"firebrick\")\nlegend(\"topleft\", legend = c(\"Original\", \"Corrected\"), fill = c(\"black\", \"firebrick\"))\n```\n\n::: {.cell-output-display}\n![](03-meta-analysis_files/figure-revealjs/unnamed-chunk-51-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n## Correcting PB - Selection Models (SM)\n\n. . .\n\n- SM assume a **relationship between the p-value and the probability of publishing**\n\n. . .\n\n- SM **estimate this relationship** from available studies and **correct the average effect**\n\n. . .\n\n:::{.cons}\n- The models are complicated (number of parameters) and need a large $k$\n:::\n\n. . .\n\n:::{.pros}\n- They provide a very elegant framework to formalize the publication bias supporting simulations and methods development\n:::\n\n## SM - Publication Bias Function\n\n- The publication bias can be formalized using a **weight function** that assign a probability to a certain study properties (e.g., p-value, sample size, z-score, etc.) representing the likelihood of that study being published.\n\n. . .\n\n- The general idea [e.g., @Citkowicz2017-ox] is to use a weighted probability density function (wPDF). In the presence of publication bias, the parameters of the wPDF will be different (i.e., adjusted) compared to unweighted PDF (i.e., assuming no publication bias)\n\n## SM - Publication Bias Function\n\nThe random-effect meta-analysis PDF can be written as [e.g., @Citkowicz2017-ox]:\n\n$$\nf\\left(y_i \\mid \\beta, \\tau^2 ; \\sigma_i^2\\right)=\\phi\\left(\\frac{y_i-\\Delta_i}{\\sqrt{\\sigma_i^2+\\tau^2}}\\right) / \\sqrt{\\sigma_i^2+\\tau^2},\n$$\n\nAnd adding the weight function:\n\n$$\nf\\left(Y_i \\mid \\beta, \\tau^2 ; \\sigma_i^2\\right)=\\frac{\\mathrm{w}\\left(p_i\\right) \\phi\\left(\\frac{y_i-\\Delta_i}{\\sqrt{\\sigma_i^2+\\tau^2}}\\right) / \\sqrt{\\sigma_i^2+\\tau^2}}{\\int_{-\\infty}^{\\infty} \\mathrm{w}\\left(p_i\\right) \\phi\\left(\\frac{Y_i-\\Delta_i}{\\sqrt{\\sigma_i^2+\\tau^2}}\\right) / \\sqrt{\\sigma_i^2+\\tau^2} d Y_i}\n$$\n\n## SM - Publication Bias Function {#sec-pub-bias-fun}\n\nFor example, Citkowicz and Vevea [-@Citkowicz2017-ox] proposed a model using a weight function based on the Beta distribution with two parameters $a$ and $b$^[[https://www.youtube.com/watch?v=ucmOCuyCk-c](https://www.youtube.com/watch?v=ucmOCuyCk-c)]\n$w(p_i) = p_i^{a - 1} \\times (1 - p_i)^{b - 1}$:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](03-meta-analysis_files/figure-revealjs/unnamed-chunk-52-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n## Selection Models\n\nIn R we can use the `metafor::selmodel()` function to implement several type of models. For example we can apply the Citkowicz and Vevea [-@Citkowicz2017-ox] model:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsel_beta <- selmodel(fit, type = \"beta\")\n```\n:::\n\n\n\n\n::: {.panel-tabset}\n\n### Results\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```\n#> \n#> Random-Effects Model (k = 100; tau^2 estimator: ML)\n#> \n#> tau^2 (estimated amount of total heterogeneity): 0.0857 (SE = 0.0255)\n#> tau (square root of estimated tau^2 value):      0.2928\n#> \n#> Test for Heterogeneity:\n#> LRT(df = 1) = 293.8104, p-val < .0001\n#> \n#> Model Results:\n#> \n#> estimate      se    zval    pval   ci.lb   ci.ub      \n#>   0.4847  0.1020  4.7521  <.0001  0.2848  0.6846  *** \n#> \n#> Test for Selection Model Parameters:\n#> LRT(df = 2) = 14.3271, p-val = 0.0008\n#> \n#> Selection Model Results:\n#> \n#>          estimate      se     zval    pval   ci.lb   ci.ub     \n#> delta.1    0.8174  0.0700  -2.6075  0.0091  0.6802  0.9547  ** \n#> delta.2    0.7017  0.2009  -1.4846  0.1376  0.3080  1.0955     \n#> \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n\n\n\n### Plot\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(sel_beta)\n```\n\n::: {.cell-output-display}\n![](03-meta-analysis_files/figure-revealjs/unnamed-chunk-55-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n:::\n\n## Selection Models\n\nLet's try the Beta selection model without publication bias:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(2023)\ndat <- sim_studies(30, 0.5, 0, 30, 30)\nfit <- rma(yi, vi, data = dat, method = \"ML\")\nsel_beta <- selmodel(fit, type = \"beta\")\nplot(sel_beta)\n```\n\n::: {.cell-output-display}\n![](03-meta-analysis_files/figure-revealjs/unnamed-chunk-56-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n## More on SM and Publication Bias\n\n- The SM documentation of `metafor::selmodel()` [https://wviechtb.github.io/metafor/reference/selmodel.html](https://www.youtube.com/watch?v=ucmOCuyCk-c)\n- Wolfgang Viechtbauer overview of PB [https://www.youtube.com/watch?v=ucmOCuyCk-c](https://www.youtube.com/watch?v=ucmOCuyCk-c)\n- @Harrer2021-go - Doing Meta-analysis in R - [Chapter 9](https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/pub-bias.html)\n- @McShane2016-bk for a nice introduction about publication bias and SM\n- Another good overview by @Jin2015-ik\n- See also @Guan2016-kn, @Maier2023-js and @Bartos2022-im for Bayesian approaches to PB\n\n## More on SM and Publication Bias\n\nAssessing, testing and developing sofisticated models for publication bias is surely important and interesting. But as Wolfgang Viechtbauer (the author of `metafor`) said:\n\n> hopefully there won't be need for these models in the future [@Viechtbauer2021-od]\n\n## More on meta-analysis\n\n- the `metafor` website contains a lot of materials, examples, tutorials about meta-analysis models\n- I have an entire workshop on meta-analysis using data simulation [https://stat-teaching.github.io/metasimulation/](https://stat-teaching.github.io/metasimulation/)\n- The book [Doing meta-analysis in R](https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/) is an amazing resource\n\n## References",
    "supporting": [
      "03-meta-analysis_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}