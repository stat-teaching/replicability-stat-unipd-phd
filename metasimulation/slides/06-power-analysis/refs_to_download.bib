@MISC{Borenstein2009-mo,
  title = {Introduction to {Meta-Analysis}},
  author = {Borenstein, Michael and Hedges, Larry V and Higgins, Julian P T and
  Rothstein, Hannah R},
  date = {2009},
  doi = {10.1002/9780470743386},
  url = {http://dx.doi.org/10.1002/9780470743386},
  keywords = {MA Unconscious WM}
}

@ARTICLE{Jackson2017-dv,
  title = {Power analysis for random-effects meta-analysis},
  author = {Jackson, Dan and Turner, Rebecca},
  journaltitle = {Research synthesis methods},
  publisher = {Wiley Online Library},
  volume = {8},
  issue = {3},
  pages = {290-302},
  date = {2017-09},
  doi = {10.1002/jrsm.1240},
  pmc = {PMC5590730},
  pmid = {28378395},
  issn = {1759-2879,1759-2887},
  abstract = {One of the reasons for the popularity of meta-analysis is the
  notion that these analyses will possess more power to detect effects than
  individual studies. This is inevitably the case under a fixed-effect model.
  However, the inclusion of the between-study variance in the random-effects
  model, and the need to estimate this parameter, can have unfortunate
  implications for this power. We develop methods for assessing the power of
  random-effects meta-analyses, and the average power of the individual studies
  that contribute to meta-analyses, so that these powers can be compared. In
  addition to deriving new analytical results and methods, we apply our methods
  to 1991 meta-analyses taken from the Cochrane Database of Systematic Reviews
  to retrospectively calculate their powers. We find that, in practice, 5 or
  more studies are needed to reasonably consistently achieve powers from
  random-effects meta-analyses that are greater than the studies that contribute
  to them. Not only is statistical inference under the random-effects model
  challenging when there are very few studies but also less worthwhile in such
  cases. The assumption that meta-analysis will result in an increase in power
  is challenged by our findings.},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.1240},
  keywords = {cochrane; empirical evaluation; power calculations; random-effects
  meta-analysis},
  language = {en}
}

@ARTICLE{Hedges2019-ry,
  title = {Statistical analyses for studying replication: Meta-analytic
  perspectives},
  author = {Hedges, Larry V and Schauer, Jacob M},
  journaltitle = {Psychological methods},
  publisher = {American Psychological Association (APA)},
  volume = {24},
  issue = {5},
  pages = {557-570},
  date = {2019-10},
  doi = {10.1037/met0000189},
  pmid = {30070547},
  issn = {1082-989X,1939-1463},
  abstract = {Formal empirical assessments of replication have recently become
  more prominent in several areas of science, including psychology. These
  assessments have used different statistical approaches to determine if a
  finding has been replicated. The purpose of this article is to provide several
  alternative conceptual frameworks that lead to different statistical analyses
  to test hypotheses about replication. All of these analyses are based on
  statistical methods used in meta-analysis. The differences among the methods
  described involve whether the burden of proof is placed on replication or
  nonreplication, whether replication is exact or allows for a small amount of
  "negligible heterogeneity," and whether the studies observed are assumed to be
  fixed (constituting the entire body of relevant evidence) or are a sample from
  a universe of possibly relevant studies. The statistical power of each of
  these tests is computed and shown to be low in many cases, raising issues of
  the interpretability of tests for replication. (PsycINFO Database Record (c)
  2019 APA, all rights reserved).},
  url = {http://dx.doi.org/10.1037/met0000189},
  file = {Hedges and Schauer 2019 - Statistical analyses for studying replication - Meta-analytic perspectives.pdf},
  keywords = {replication-methods},
  language = {en}
}

@ARTICLE{Valentine2010-aj,
  title = {How Many Studies Do You Need?: A Primer on Statistical Power for
  {Meta-Analysis}},
  author = {Valentine, Jeffrey C and Pigott, Therese D and Rothstein, Hannah R},
  journaltitle = {Journal of educational and behavioral statistics: a quarterly
  publication sponsored by the American Educational Research Association and the
  American Statistical Association},
  publisher = {American Educational Research Association},
  volume = {35},
  issue = {2},
  pages = {215-247},
  date = {2010-04-01},
  doi = {10.3102/1076998609346961},
  issn = {1076-9986},
  abstract = {In this article, the authors outline methods for using fixed and
  random effects power analysis in the context of meta-analysis. Like
  statistical power analysis for primary studies, power analysis for
  meta-analysis can be done either prospectively or retrospectively and requires
  assumptions about parameters that are unknown. The authors provide some
  suggestions for thinking about these parameters, in particular for the random
  effects variance component. The authors also show how the typically
  uninformative retrospective power analysis can be made more informative. The
  authors then discuss the value of confidence intervals, show how they could be
  used in addition to or instead of retrospective power analysis, and also
  demonstrate that confidence intervals can convey information more effectively
  in some situations than power analyses alone. Finally, the authors take up the
  question ?How many studies do you need to do a meta-analysis?? and show that,
  given the need for a conclusion, the answer is ?two studies,? because all
  other synthesis techniques are less transparent and/or are less likely to be
  valid. For systematic reviewers who choose not to conduct a quantitative
  synthesis, the authors provide suggestions for both highlighting the current
  limitations in the research base and for displaying the characteristics and
  results of studies that were found to meet inclusion criteria.},
  url = {https://doi.org/10.3102/1076998609346961},
  note = {doi: 10.3102/1076998609346961},
  keywords = {MA Unconscious WM}
}

