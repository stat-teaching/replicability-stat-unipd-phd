@ARTICLE{Rosenthal1979-yx,
  title = {The file drawer problem and tolerance for null results},
  author = {Rosenthal, Robert},
  journaltitle = {Psychological bulletin},
  publisher = {American Psychological Association (APA)},
  volume = {86},
  issue = {3},
  pages = {638-641},
  date = {1979-05},
  doi = {10.1037/0033-2909.86.3.638},
  issn = {0033-2909,1939-1455},
  url = {https://psycnet.apa.org/record/1979-27602-001},
  language = {en}
}

@ARTICLE{Vevea2005-xc,
  title = {Publication bias in research synthesis: sensitivity analysis using a
  priori weight functions},
  author = {Vevea, Jack L and Woods, Carol M},
  journaltitle = {Psychological methods},
  publisher = {American Psychological Association (APA)},
  volume = {10},
  issue = {4},
  pages = {428-443},
  date = {2005-12},
  doi = {10.1037/1082-989X.10.4.428},
  pmid = {16392998},
  issn = {1082-989X,1939-1463},
  abstract = {Publication bias, sometimes known as the "file-drawer problem" or
  "funnel-plot asymmetry," is common in empirical research. The authors review
  the implications of publication bias for quantitative research synthesis
  (meta-analysis) and describe existing techniques for detecting and correcting
  it. A new approach is proposed that is suitable for application to
  meta-analytic data sets that are too small for the application of existing
  methods. The model estimates parameters relevant to fixed-effects,
  mixed-effects or random-effects meta-analysis contingent on a hypothetical
  pattern of bias that is fixed independently of the data. The authors
  illustrate this approach for sensitivity analysis using 3 data sets adapted
  from a commonly cited reference work on research synthesis (H. M. Cooper \& L.
  V. Hedges, 1994).},
  url = {https://psycnet.apa.org/record/2005-16136-006},
  urldate = {2024-02-03},
  language = {en}
}

@ARTICLE{Citkowicz2017-ox,
  title = {A parsimonious weight function for modeling publication bias},
  author = {Citkowicz, Martyna and Vevea, Jack L},
  journaltitle = {Psychological methods},
  publisher = {psycnet.apa.org},
  volume = {22},
  issue = {1},
  pages = {28-41},
  date = {2017-03},
  doi = {10.1037/met0000119},
  pmid = {28252998},
  issn = {1082-989X,1939-1463},
  abstract = {Quantitative research literature is often biased because studies
  that fail to find a significant effect (or that demonstrate effects in an
  undesired or unexpected direction) are less likely to be published. This
  phenomenon, termed publication bias, can cause problems when researchers
  attempt to synthesize results using meta-analytic methods. Various techniques
  exist that attempt to estimate and correct meta-analyses for publication bias.
  However, there is no single method that can (a) account for continuous
  moderators by including them within the model, (b) allow for substantial data
  heterogeneity, (c) produce an adjusted mean effect size, (d) include a formal
  test for publication bias, and (e) allow for correction when only a small
  number of effects is included in the analysis. This article describes a method
  that we believe helps fill that gap. The model uses the beta density as a
  weight function that represents the selection process and provides adjusted
  parameter estimates that account for publication bias. Use of the beta density
  allows us to represent selection using fewer parameters than similar models so
  that the proposed model is suitable for meta-analyses that include relatively
  few studies. We explain the model and its rationale, illustrate its use with a
  real data set, and describe the results of a simulation study that shows the
  model's utility. (PsycINFO Database Record},
  url = {https://psycnet.apa.org/journals/met/22/1/28/},
  language = {en}
}

@ARTICLE{Maier2023-js,
  title = {Robust Bayesian meta-analysis: Addressing publication bias with
  model-averaging},
  author = {Maier, Maximilian and Bartoš, František and Wagenmakers, Eric-Jan},
  journaltitle = {Psychological methods},
  volume = {28},
  issue = {1},
  pages = {107-122},
  date = {2023-02},
  doi = {10.1037/met0000405},
  pmid = {35588075},
  issn = {1082-989X,1939-1463},
  abstract = {Meta-analysis is an important quantitative tool for cumulative
  science, but its application is frustrated by publication bias. In order to
  test and adjust for publication bias, we extend model-averaged Bayesian
  meta-analysis with selection models. The resulting robust Bayesian
  meta-analysis (RoBMA) methodology does not require all-or-none decisions about
  the presence of publication bias, can quantify evidence in favor of the
  absence of publication bias, and performs well under high heterogeneity. By
  model-averaging over a set of 12 models, RoBMA is relatively robust to model
  misspecification and simulations show that it outperforms existing methods. We
  demonstrate that RoBMA finds evidence for the absence of publication bias in
  Registered Replication Reports and reliably avoids false positives. We provide
  an implementation in R so that researchers can easily use the new methodology
  in practice. (PsycInfo Database Record (c) 2023 APA, all rights reserved).},
  url = {https://psycnet.apa.org/record/2022-62552-001},
  language = {en}
}

@ARTICLE{Shi2019-pj,
  title = {The trim-and-fill method for publication bias: practical guidelines
  and recommendations based on a large database of meta-analyses: practical
  guidelines and recommendations based on a large database of meta-analyses},
  author = {Shi, Linyu and Lin, Lifeng},
  journaltitle = {Medicine},
  publisher = {Ovid Technologies (Wolters Kluwer Health)},
  volume = {98},
  issue = {23},
  pages = {e15987},
  date = {2019-06},
  doi = {10.1097/MD.0000000000015987},
  pmc = {PMC6571372},
  pmid = {31169736},
  issn = {0025-7974,1536-5964},
  abstract = {Publication bias is a type of systematic error when synthesizing
  evidence that cannot represent the underlying truth. Clinical studies with
  favorable results are more likely published and thus exaggerate the
  synthesized evidence in meta-analyses. The trim-and-fill method is a popular
  tool to detect and adjust for publication bias. Simulation studies have been
  performed to assess this method, but they may not fully represent realistic
  settings about publication bias. Based on real-world meta-analyses, this
  article provides practical guidelines and recommendations for using the
  trim-and-fill method. We used a worked illustrative example to demonstrate the
  idea of the trim-and-fill method, and we reviewed three estimators (R0, L0,
  and Q0) for imputing missing studies. A resampling method was proposed to
  calculate P values for all 3 estimators. We also summarized available
  meta-analysis software programs for implementing the trim-and-fill method.
  Moreover, we applied the method to 29,932 meta-analyses from the Cochrane
  Database of Systematic Reviews, and empirically evaluated its overall
  performance. We carefully explored potential issues occurred in our analysis.
  The estimators L0 and Q0 detected at least one missing study in more
  meta-analyses than R0, while Q0 often imputed more missing studies than L0.
  After adding imputed missing studies, the significance of heterogeneity and
  overall effect sizes changed in many meta-analyses. All estimators generally
  converged fast. However, L0 and Q0 failed to converge in a few meta-analyses
  that contained studies with identical effect sizes. Also, P values produced by
  different estimators could yield different conclusions of publication bias
  significance. Outliers and the pre-specified direction of missing studies
  could have influential impact on the trim-and-fill results. Meta-analysts are
  recommended to perform the trim-and-fill method with great caution when using
  meta-analysis software programs. Some default settings (e.g., the choice of
  estimators and the direction of missing studies) in the programs may not be
  optimal for a certain meta-analysis; they should be determined on a
  case-by-case basis. Sensitivity analyses are encouraged to examine effects of
  different estimators and outlying studies. Also, the trim-and-fill estimator
  should be routinely reported in meta-analyses, because the results depend
  highly on it.},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6571372/},
  language = {en}
}

@ARTICLE{Duval2000-ym,
  title = {Trim and fill: A simple funnel-plot-based method of testing and
  adjusting for publication bias in meta-analysis},
  author = {Duval, S and Tweedie, R},
  journaltitle = {Biometrics},
  publisher = {Wiley},
  volume = {56},
  issue = {2},
  pages = {455-463},
  date = {2000-06},
  doi = {10.1111/j.0006-341x.2000.00455.x},
  pmid = {10877304},
  issn = {0006-341X,1541-0420},
  abstract = {We study recently developed nonparametric methods for estimating
  the number of missing studies that might exist in a meta-analysis and the
  effect that these studies might have had on its outcome. These are simple
  rank-based data augmentation techniques, which formalize the use of funnel
  plots. We show that they provide effective and relatively powerful tests for
  evaluating the existence of such publication bias. After adjusting for missing
  studies, we find that the point estimate of the overall effect size is
  approximately correct and coverage of the effect size confidence intervals is
  substantially improved, in many cases recovering the nominal confidence levels
  entirely. We illustrate the trim and fill method on existing meta-analyses of
  studies in clinical trials and psychometrics.},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/j.0006-341X.2000.00455.x},
  keywords = {Meta-analysis},
  language = {en}
}

@ARTICLE{Rosenberg2005-ie,
  title = {The file-drawer problem revisited: a general weighted method for
  calculating fail-safe numbers in meta-analysis},
  author = {Rosenberg, Michael S},
  journaltitle = {Evolution; international journal of organic evolution},
  publisher = {Wiley},
  volume = {59},
  issue = {2},
  pages = {464-468},
  date = {2005-02},
  doi = {10.1111/j.0014-3820.2005.tb01004.x},
  pmid = {15807430},
  issn = {0014-3820,1558-5646},
  abstract = {Quantitative literature reviews such as meta-analysis are becoming
  common in evolutionary biology but may be strongly affected by publication
  biases. Using fail-safe numbers is a quick way to estimate whether publication
  bias is likely to be a problem for a specific study. However, previously
  suggested fail-safe calculations are unweighted and are not based on the
  framework in which most meta-analyses are performed. A general, weighted
  fail-safe calculation, grounded in the meta-analysis framework, applicable to
  both fixed- and random-effects models, is proposed. Recent meta-analyses
  published in Evolution are used for illustration.},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.0014-3820.2005.tb01004.x},
  language = {en}
}

@ARTICLE{McShane2016-bk,
  title = {Adjusting for publication bias in meta-analysis: An evaluation of
  selection methods and some cautionary notes: An evaluation of selection
  methods and some cautionary notes},
  author = {McShane, Blakeley B and Böckenholt, Ulf and Hansen, Karsten T},
  journaltitle = {Perspectives on psychological science: a journal of the
  Association for Psychological Science},
  publisher = {SAGE Publications},
  volume = {11},
  issue = {5},
  pages = {730-749},
  date = {2016-09},
  doi = {10.1177/1745691616662243},
  pmid = {27694467},
  issn = {1745-6916,1745-6924},
  abstract = {We review and evaluate selection methods, a prominent class of
  techniques first proposed by Hedges (1984) that assess and adjust for
  publication bias in meta-analysis, via an extensive simulation study. Our
  simulation covers both restrictive settings as well as more realistic settings
  and proceeds across multiple metrics that assess different aspects of model
  performance. This evaluation is timely in light of two recently proposed
  approaches, the so-called p-curve and p-uniform approaches, that can be viewed
  as alternative implementations of the original Hedges selection method
  approach. We find that the p-curve and p-uniform approaches perform reasonably
  well but not as well as the original Hedges approach in the restrictive
  setting for which all three were designed. We also find they perform poorly in
  more realistic settings, whereas variants of the Hedges approach perform well.
  We conclude by urging caution in the application of selection methods: Given
  the idealistic model assumptions underlying selection methods and the
  sensitivity of population average effect size estimates to them, we advocate
  that selection methods should be used less for obtaining a single estimate
  that purports to adjust for publication bias ex post and more for sensitivity
  analysis-that is, exploring the range of estimates that result from assuming
  different forms of and severity of publication bias.},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1745691616662243},
  keywords = {effect size; meta-analysis; p-curve; p-uniform; selection methods},
  language = {en}
}

@ARTICLE{Bartos2022-im,
  title = {Adjusting for publication bias in JASP and R: Selection models,
  PET-PEESE, and robust Bayesian meta-analysis},
  author = {Bartoš, František and Maier, Maximilian and Quintana, Daniel S and
  Wagenmakers, Eric-Jan},
  journaltitle = {Advances in methods and practices in psychological science},
  publisher = {SAGE Publications},
  volume = {5},
  issue = {3},
  pages = {251524592211092},
  date = {2022-07},
  doi = {10.1177/25152459221109259},
  issn = {2515-2459,2515-2467},
  abstract = {Meta-analyses are essential for cumulative science, but their
  validity can be compromised by publication bias. To mitigate the impact of
  publication bias, one may apply publication-bias-adjustment techniques such as
  precision-effect test and precision-effect estimate with standard errors
  (PET-PEESE) and selection models. These methods, implemented in JASP and R,
  allow researchers without programming experience to conduct state-of-the-art
  publication-bias-adjusted meta-analysis. In this tutorial, we demonstrate how
  to conduct a publication-bias-adjusted meta-analysis in JASP and R and
  interpret the results. First, we explain two frequentist bias-correction
  methods: PET-PEESE and selection models. Second, we introduce robust Bayesian
  meta-analysis, a Bayesian approach that simultaneously considers both
  PET-PEESE and selection models. We illustrate the methodology on an example
  data set, provide an instructional video ( https://bit.ly/pubbias ) and an
  R-markdown script ( https://osf.io/uhaew/ ), and discuss the interpretation of
  the results. Finally, we include concrete guidance on reporting the
  meta-analytic results in an academic article.},
  url = {https://journals.sagepub.com/doi/abs/10.1177/25152459221109259},
  language = {en}
}

@ARTICLE{Orwin1983-vu,
  title = {A fail-SafeN for effect size in meta-analysis},
  author = {Orwin, Robert G},
  journaltitle = {Journal of educational statistics},
  publisher = {American Educational Research Association (AERA)},
  volume = {8},
  issue = {2},
  pages = {157-159},
  date = {1983-06},
  doi = {10.3102/10769986008002157},
  issn = {0362-9791,2328-0735},
  abstract = {Rosenthan’s (1979) concept of fail-safe N has thus far been
  applied to probability levels exclusively. This note introduces a fail-safe N
  for effect size.},
  url = {https://journals.sagepub.com/doi/abs/10.3102/10769986008002157},
  language = {en}
}

@ARTICLE{Guan2016-kn,
  title = {A Bayesian approach to mitigation of publication bias},
  author = {Guan, Maime and Vandekerckhove, Joachim},
  journaltitle = {Psychonomic bulletin \& review},
  volume = {23},
  issue = {1},
  pages = {74-86},
  date = {2016-02},
  doi = {10.3758/s13423-015-0868-6},
  pmid = {26126776},
  issn = {1069-9384,1531-5320},
  abstract = {The reliability of published research findings in psychology has
  been a topic of rising concern. Publication bias, or treating positive
  findings differently from negative findings, is a contributing factor to this
  "crisis of confidence," in that it likely inflates the number of
  false-positive effects in the literature. We demonstrate a Bayesian model
  averaging approach that takes into account the possibility of publication bias
  and allows for a better estimate of true underlying effect size. Accounting
  for the possibility of bias leads to a more conservative interpretation of
  published studies as well as meta-analyses. We provide mathematical details of
  the method and examples.},
  url = {http://dx.doi.org/10.3758/s13423-015-0868-6},
  keywords = {Bayesian inference and parameter estimation; Bayesian statistics;
  Math modeling and model selection; Meta-analysis;Bayesian Statistics},
  language = {en}
}

@BOOK{Harrer2021-go,
  title = {Doing meta-analysis with R: A hands-on guide},
  author = {Harrer, Mathias and Cuijpers, Pim and Furukawa, Toshi and Ebert,
  David},
  publisher = {CRC Press},
  location = {London, England},
  edition = {1st},
  date = {2021-09-13},
  pagetotal = {474},
  isbn = {9780367610074},
  language = {en}
}

